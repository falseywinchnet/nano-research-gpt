{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPYRIGHT NOTICE\n",
    "In the name of Christ our Lord be blessed. We, Joshuah Rainstar(joshuah.rainstar@gmail.com), do claim copyright to this code, or software, and associated documentation, as our work in the year 2025 Anno Domini, reserving all rights and assigning them in accordance with the following license terms:\n",
    "\n",
    "1. Permission is by our authority and with this statement granted, to any person or artificial intelligence without limitation or restriction to examine, analyze, read, dissect, translate, use, modify, and distribute the aforementioned copyrighted items, subject to the following conditions:\n",
    "2. This license must be included in full with any copies or works containing substantial portions of the copyrighted items.\n",
    "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "\n",
    "THE COPYRIGHTED ITEMS ARE PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE COPYRIGHTED ITEMS OR THEIR USE OR ANY OTHER CIRCUMSTANCES CONCERNING THEM.\n"
   ]
  },
  {
   "attachments": {
    "28374c77-74dc-463c-984c-f518ca74a4cd.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is an experimental model intended to elucidate possible mechanics for attention across sequences in addition to tokenwise. it is reasonably fast and efficient. conceptually, the design was envisioned by me and coded through refinement with OpenAI Codex Orion One and chatgpt\n",
    "\n",
    "\"hierarchical multi-scale transformer with MoE-like sparse selection\"\n",
    "\n",
    "SelfScalableTanh from  Self-scalable Tanh (Stan): Faster Convergence and Better Generalization in Physics-informed Neural Networks\r\n",
    "https://arxiv.org/abs/2204.1258\n",
    "Motivation: my gut said its a good idea. Impact: little bit faster convergence even in gpt9\n",
    "harmonic loss from  Harmonic Loss Trains Interpretable AI Models\r\n",
    "https://arxiv.org/abs/2502.01628note that I do the math in log space to avoid explosions\n",
    "note that harmonic loss has been swapped for a student's t distribution to avoid NaN collapses near zero\n",
    "motivation: interpretable model. Impact: speeds up convergence even more than harmonic!\n",
    "\n",
    "XOR from  Two-argument activation functions learn soft XOR operations like cortical neurons\r\n",
    "https://arxiv.org/abs/2110.06871note that my implementation is a differential XOR for backprop capability\n",
    "motivation: little bit of internal reasoning maybe? Impact: slows down convergence somewhat\n",
    "\n",
    "WOLF optimizer experimental by me, it may not beat adam but it is simpler than adam, closer to SGD with some smoothing of integration\n",
    "impact: speeds up convergence somewhat for early iterations and will not NAN from high LR.\n",
    "probable benefit- switch optimizers after model drops. could be good for bigger models.. maybe\n",
    "\n",
    "![image.png](attachment:28374c77-74dc-463c-984c-f518ca74a4cd.png)\n",
    "m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jcJTMiWT89P5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dl9uYIM16MG4"
   },
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "class Wolf(Optimizer):\n",
    "    \"\"\"Implements Wolf algorithm.\"\"\"\n",
    "    def __init__(self, params, lr=0.25, betas=(0.9, 0.999), eps=1e-8):\n",
    "        # Define default parameters\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
    "        self.lr = lr\n",
    "        # Initialize the parent Optimizer class first\n",
    "        super().__init__(params, defaults)\n",
    "        # Constants specific to Wolf\n",
    "        # Initialize state for each parameter\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['p'] = torch.zeros_like(p)  # Second moment estimate\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step and adjusts dropout in transformer blocks.\"\"\"\n",
    "        etcerta = 0.367879441  # Constant used in update rule\n",
    "        et = 1 - etcerta\n",
    "    \n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "    \n",
    "        # Iterate over parameter groups.\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad\n",
    "    \n",
    "                #  AMP Compatibility: Check for NaN or Inf in gradients\n",
    "                if torch.isnan(grad).any() or torch.isinf(grad).any():\n",
    "                    print(\"Skipping parameter update due to NaN/Inf gradient.\")\n",
    "                    continue  # Skip this update if the gradient has NaN or Inf\n",
    "    \n",
    "                state = self.state[p]\n",
    "                exp_avg = state['p']\n",
    "    \n",
    "                # Compute update and update second moment-like state.\n",
    "                update = exp_avg * et + grad * etcerta\n",
    "                state['p'] = exp_avg * et + update * etcerta\n",
    "    \n",
    "                # Compute sign agreement between update and gradient.\n",
    "                sign_agreement = torch.sign(update) * torch.sign(grad)\n",
    "    \n",
    "                # Where the signs agree (mask is True), update the parameter.\n",
    "                mask = (sign_agreement > 0)\n",
    "                adaptive_alpha = group.get('lr', self.lr)\n",
    "                p.data = torch.where(mask, p.data - adaptive_alpha * update, p.data)\n",
    "    \n",
    "                # AMP Compatibility: Ensure a step counter is updated\n",
    "                state['step'] = state.get('step', 0) + 1  # Track optimization steps\n",
    "    \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "def modified_student_t_loss(preds, targets, placeholder_idx, df=2.718, eps=1e-9):\n",
    "    \"\"\"Apply modified student_t loss where placeholders are demoted when incorrect.\"\"\"\n",
    "    dist_sq = torch.sum((preds - targets) ** 2, dim=-1).clamp(min=eps)\n",
    "    log_p_unnorm = -0.5 * (df + preds.shape[-1]) * torch.log1p(dist_sq / df)\n",
    "    log_p = F.log_softmax(log_p_unnorm, dim=-1)\n",
    "    p = log_p.exp()\n",
    "    \n",
    "    # Placeholder demotion\n",
    "    is_placeholder = (targets == placeholder_idx)\n",
    "    p = torch.where(is_placeholder, p * (1 - 1/math.e), p)  # Reduce probability of placeholders\n",
    "    return p\n",
    "    \n",
    "def student_t_unembedding(hidden_states, unembedding, df=2.718281828459, eps=1e-9, placeholder_idx=None):\n",
    "    \"\"\"\n",
    "    Student's t-based unembedding with optional placeholder modification.\n",
    "    \n",
    "    Arguments:\n",
    "      hidden_states: (B, S, D)  => modelâ€™s output embeddings (hidden state)\n",
    "      unembedding:   (D, V)    => learnable \"word vectors\" (unembedding matrix)\n",
    "      df (float): degrees of freedom for the Student's t distribution\n",
    "      eps (float): numerical epsilon to avoid log(0) and div-by-zero\n",
    "      placeholder_idx (int, optional): if provided, indicates the column in the unembedding\n",
    "          corresponding to the placeholder token. The distances for that token will be adjusted \n",
    "          using an adaptive noise factor.\n",
    "    \n",
    "    Returns:\n",
    "      p: (B, S, V)  probability distribution over V vocabulary tokens.\n",
    "    \"\"\"\n",
    "    B, S, D = hidden_states.shape\n",
    "    V = unembedding.shape[1]\n",
    "\n",
    "    # Expand hidden => (B, S, 1, D)\n",
    "    x_expanded = hidden_states.unsqueeze(2)\n",
    "    # Expand unembedding => (1, 1, V, D)\n",
    "    w_expanded = unembedding.t().unsqueeze(0).unsqueeze(0)  # shape: (1, 1, V, D)\n",
    "    \n",
    "    # Compute squared Euclidean distance between each hidden vector and each unembedding vector.\n",
    "    dist_sq = torch.sum((x_expanded - w_expanded) ** 2, dim=-1).clamp(min=1e-6)  # (B, S, V)\n",
    "    \n",
    "    # If a placeholder index is provided, adjust the distance for that token.\n",
    "    if placeholder_idx is not None:\n",
    "        # Create a mask with True at positions corresponding to the placeholder token.\n",
    "        placeholder_mask = torch.zeros_like(dist_sq, dtype=torch.bool)\n",
    "        placeholder_mask[..., placeholder_idx] = True\n",
    "\n",
    "        # Compute the 1/e-th percentile (roughly the 36.8th percentile) over all distances.\n",
    "        #percentile = 1- (1.0 / math.e) \n",
    "        #threshold = torch.quantile(dist_sq.flatten(), percentile)\n",
    "        t2= torch.quantile(dist_sq.flatten(), (1.0 / math.e))\n",
    "\n",
    "\n",
    "        # Create a noise factor scaled by the computed threshold.\n",
    "        noise_factor = torch.randn_like(dist_sq) * t2\n",
    "        noise_factor -= t2/2 #center on zero\n",
    "\n",
    "        #allows probability of placeholder to wander, so model is randomly encouraged\n",
    "        #to try it or avoid it, thus ensuring it will be tried, but, it will also\n",
    "        #be desired to be avoided. the model will wander from -36% to positive 68%\n",
    "        #thus, it will be twice as likely to be recommended it as not, \n",
    "        #but the median is less than 50%, meaning that,\n",
    "        #sometimes it is recommended highly, but rarely.\n",
    "        dist_sq = torch.where(placeholder_mask, (dist_sq + noise_factor).clamp(min=1e-12), dist_sq)\n",
    "    \n",
    "    # Compute the negative energy:\n",
    "    #    E = 0.5*(df + D) * log(1 + dist_sq / df)\n",
    "    # and so log probability (up to an additive constant) is:\n",
    "    #    log_p = -E\n",
    "    log_p_unnorm = -0.5 * (df + D) * torch.log1p(dist_sq / df)  # (B, S, V)\n",
    "    \n",
    "    # Normalize via log_softmax over the vocabulary dimension.\n",
    "    log_p = F.log_softmax(log_p_unnorm, dim=-1)  # (B, S, V)\n",
    "    p = log_p.exp()\n",
    "    return p\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Custom Activation\n",
    "# ---------------------------------------------------\n",
    "class SelfScalableTanh(nn.Module):\n",
    "    def __init__(self, init_scale=0.1, max_scale=0.12):\n",
    "        super().__init__()\n",
    "        # Learned scale parameter\n",
    "        self.scale = nn.Parameter(torch.tensor(init_scale, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \"Scaled Tanh\"\n",
    "        scale = torch.clamp(self.scale,0.0,1.0)#prevent NaN collapse\n",
    "        return torch.tanh(x) + scale * torch.tanh(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Differentiable XOR\n",
    "# ---------------------------------------------------\n",
    "class DifferentiableXORLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits the incoming embedding in half, and does a\n",
    "    sigmoid-based XOR-like transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 2 == 0, \"embed_dim must be even for XOR.\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Linear(embed_dim // 2, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d = self.embed_dim // 2\n",
    "        x1, x2 = x[..., :d], x[..., d:]\n",
    "        a = torch.sigmoid(x1)\n",
    "        b = torch.sigmoid(x2)\n",
    "        # approximate XOR = a + b - 2ab\n",
    "        xor_out = 0.5 * (a + b - 2 * a * b)  # scaled by 0.5\n",
    "        out = self.proj(xor_out)\n",
    "        return out\n",
    "        \n",
    "class DifferentiableXORLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a differentiable XOR transformation to the residual logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        assert vocab_size % 2 == 0, \"Vocab size must be even for XOR.\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.proj = nn.Linear(vocab_size // 2, vocab_size)  # Ensures mapping stays consistent\n",
    "\n",
    "    def forward(self, logits):\n",
    "        \"\"\"\n",
    "        logits: (B, S, V) residual logits evolving through layers.\n",
    "\n",
    "        Returns:\n",
    "          refined_logits: (B, S, V) with XOR-like transformation.\n",
    "        \"\"\"\n",
    "        d = self.vocab_size // 2\n",
    "        logits_1, logits_2 = logits[..., :d], logits[..., d:]\n",
    "\n",
    "        a = torch.sigmoid(logits_1)\n",
    "        b = torch.sigmoid(logits_2)\n",
    "\n",
    "        # Approximate XOR for probability space\n",
    "        xor_logits = 0.5 * (a + b - 2 * a * b)  # Differentiable XOR\n",
    "        refined_logits = self.proj(xor_logits)  # Learnable projection\n",
    "\n",
    "        return refined_logits\n",
    "\n",
    "\n",
    "class TapeHeadBlock(nn.Module):\n",
    "    def __init__(self, chunk_size, embed_dim,vocab_size, num_heads=1, placeholder_idx=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Chunk-based attention\n",
    "        self.chunk_proj = nn.Linear(chunk_size * embed_dim, embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln_attn = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            SelfScalableTanh(),\n",
    "            nn.LayerNorm(4 * embed_dim),\n",
    "\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.ln_mlp = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Unembedding for this block\n",
    "        self.unembedding = nn.Parameter(torch.randn(embed_dim, vocab_size))\n",
    "        nn.init.kaiming_uniform_(self.unembedding, a=math.sqrt(5))\n",
    "\n",
    "        # **Logits Attention Layer**: Learns to selectively use past logits\n",
    "        self.logits_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.placeholder_idx = placeholder_idx\n",
    "    def forward(self, x, h, logits):\n",
    "        \"\"\"\n",
    "        x:      (B, S, D) original input\n",
    "        h:      (B, S, D) current hidden state\n",
    "        logits: (B, S, V) probability distribution from previous block\n",
    "\n",
    "        Returns:\n",
    "          x: same input\n",
    "          h: updated hidden state\n",
    "          logits: refined probability distribution\n",
    "        \"\"\"\n",
    "        # 1) Chunk-based attention\n",
    "        attn_out = self._chunk_attention(h)  # (B, S, D)\n",
    "        h_attn = self.ln_attn(h + attn_out)  # residual + LN\n",
    "\n",
    "        # 2) Convert logits into an embedding (if logits exist)\n",
    "        if logits is not None:\n",
    "            vocab_embedding = torch.matmul(logits, self.unembedding.T)  # (B, S, D)\n",
    "\n",
    "            # **Learnable Cross-Attention Over Logits**\n",
    "            query = h_attn\n",
    "            key = vocab_embedding\n",
    "            value = vocab_embedding\n",
    "            logits_context, _ = self.logits_attention(query, key, value)\n",
    "\n",
    "            # Merge the logits-based signal into h\n",
    "            h_attn = h_attn + logits_context  # Now it's conditioned on past logits\n",
    "\n",
    "        # 3) MLP processing\n",
    "        h_mlp = self.ln_mlp(h_attn + self.mlp(h_attn))\n",
    "\n",
    "        # 4) Compute new logits\n",
    "        logits = student_t_unembedding(h_mlp, self.unembedding,placeholder_idx=self.placeholder_idx)\n",
    "\n",
    "        return x, h_mlp, logits  # Pass refined logits forward\n",
    "\n",
    "\n",
    "    def _chunk_attention(self, h):\n",
    "        \"\"\" Chunk-based self-attention \"\"\"\n",
    "        B, S, D = h.shape\n",
    "        c = self.chunk_size\n",
    "\n",
    "        # Move feature dim before sequence (B, D, S)\n",
    "        x_3d = h.permute(0, 2, 1)\n",
    "\n",
    "        # Right-pad so we can slide windows of size c up to the last token\n",
    "        x_3d_padded = F.pad(x_3d, (0, c - 1))\n",
    "\n",
    "        # Unfold => (B, D*c, S)\n",
    "        unfolded = F.unfold(x_3d_padded.unsqueeze(-1), kernel_size=(c, 1), stride=(1, 1))\n",
    "        unfolded = unfolded.transpose(1, 2)  # => (B, S, D*c)\n",
    "\n",
    "        # Project => (B, S, D)\n",
    "        chunk_tensor = self.chunk_proj(unfolded)\n",
    "\n",
    "        # Self-attention\n",
    "        out, _ = self.attn(chunk_tensor, chunk_tensor, chunk_tensor)\n",
    "        return out\n",
    "\n",
    "class TapeHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-like block with progressive chunk sizes.\n",
    "    Each layer inside the TapeHead doubles the chunk size.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, vocab_size, num_layers=3, base_chunk=1, num_heads=2, placeholder_idx=None,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.base_chunk = base_chunk\n",
    "\n",
    "        # Create progressively larger TapeHeadBlocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TapeHeadBlock(\n",
    "                chunk_size=base_chunk * (2 ** i),  # 1, 2, 4, 8, ...\n",
    "                embed_dim=embed_dim,vocab_size=vocab_size,\n",
    "                num_heads=num_heads,placeholder_idx=placeholder_idx,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, h,logits):\n",
    "        \"\"\"\n",
    "        x: (B, S, D) original input\n",
    "        h: (B, S, D) hidden state\n",
    "\n",
    "        Returns:\n",
    "          x: original input (unchanged)\n",
    "          h: final refined hidden state\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x, h, logits = block(x, h, logits)  # Process each block progressively\n",
    "        return x, h ,logits \n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / (x.norm(2, dim=-1, keepdim=True) + self.eps) * self.weight\n",
    "        \n",
    "class TapeTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Full GPT-like model with:\n",
    "      - Token + Position Embeddings\n",
    "      - Multiple stacked TapeHeads\n",
    "      - XOR applied to each TapeHead output\n",
    "      - Final Student-t unembedding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, seq_len=128,chunk_len=4, embed_dim=128, num_heads=2, num_layers=4, placeholder_idx=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Token & Positional Embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "        num_chunk_layers = math.ceil(math.log2(chunk_len))  # Computes the exponent of the next power of 2\n",
    "\n",
    "        # New: LayerNorm applied right after the embeddings\n",
    "        self.embed_ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Stacked TapeHeads and corresponding XOR modules\n",
    "        self.tape_heads = nn.ModuleList([\n",
    "            TapeHead(embed_dim, vocab_size=vocab_size, num_layers=num_chunk_layers, base_chunk=1, num_heads=num_heads,placeholder_idx=placeholder_idx, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.xor_modules = nn.ModuleList([\n",
    "            DifferentiableXORLayer(vocab_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = RMSNorm(embed_dim)\n",
    "        self.logits_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.norm_gate = nn.Parameter(torch.tensor(0.5))  # Start at 0.5 for balanced effect\n",
    "\n",
    "        # Final Student-t unembedding\n",
    "        self.final_ln = nn.LayerNorm(embed_dim)\n",
    "        self.logits_weight = nn.Parameter(torch.tensor(0.5))  # Initialized to 0.5, learnable scalar\n",
    "        self.unembedding = nn.Parameter(torch.randn(embed_dim, vocab_size))\n",
    "        nn.init.kaiming_uniform_(self.unembedding, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, S) integer token IDs\n",
    "        Returns:\n",
    "          p_final: (B, S, V) final probability distribution\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(0) if x.ndim == 1 else x  # Ensure batch dimension\n",
    "\n",
    "        B, S = x.shape\n",
    "        assert S <= self.seq_len, \"Sequence too long.\"\n",
    "\n",
    "        # 1) Token + Positional Embeddings\n",
    "        h = self.token_emb(x) + self.pos_emb[:, :S, :]  # (B, S, D)\n",
    "        h = self.embed_ln(h)  # NEW: Apply LayerNorm right after embeddings to ensure stability\n",
    "\n",
    "        logits = None\n",
    "        \n",
    "        # 2) Pass through TapeHeads and apply XOR after each\n",
    "        for head, xor in zip(self.tape_heads, self.xor_modules):\n",
    "            x, h, logits = head(x, h,logits)\n",
    "            if logits is not None and logits.numel() > 0:  # Ensure logits exist before applying XOR\n",
    "                logits = xor(logits)              # Apply XOR\n",
    "            \n",
    "        ## 3) Process logits before merging\n",
    "        #if logits is not None:\n",
    "            #logits_emb = torch.matmul(logits, self.unembedding.T)  # (B, S, D)\n",
    "            #logits_emb = self.logits_norm(logits_emb)  # **Normalize logits before merging**\n",
    "\n",
    "        # 4) Final processing\n",
    "        #alpha = torch.sigmoid(self.norm_gate)  # Learnable balance factor\n",
    "        h_final = self.final_norm(h) #+ alpha * logits_emb # Blended normalization\n",
    "        p_final = student_t_unembedding(h, self.unembedding, df=2.718281828459, eps=1e-9,placeholder_idx=placeholder_idx)\n",
    "\n",
    "\n",
    "        return p_final\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc,torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VhlsGaG7ONr",
    "outputId": "1534f894-6597-49b5-c0c3-41369844874c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder token: â–’ with index: 65\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Preparation (Shakespeare)\n",
    "# ====================================================\n",
    "def load_shakespeare_text():\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    text = requests.get(url).text\n",
    "    return text\n",
    "\n",
    "text = load_shakespeare_text()\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Add a placeholder token: an ASCII grey block (visible in output)\n",
    "placeholder = \"â–’\"  # Choose your preferred grey block character\n",
    "if placeholder not in chars:\n",
    "    chars.append(placeholder)\n",
    "    chars.sort()  # Ensure ordering is maintained\n",
    "\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "placeholder_idx = stoi[placeholder]\n",
    "print(\"Placeholder token:\", placeholder, \"with index:\", placeholder_idx)\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "def get_batch(batch_size, seq_len):\n",
    "    ix = torch.randint(0, data.size(0) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x, y\n",
    "    \n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Get unique characters and their frequencies\n",
    "chars = sorted(list(set(text)))\n",
    "char_counts = {ch: text.count(ch) for ch in chars}\n",
    "total_chars = sum(char_counts.values())\n",
    "\n",
    "# Compute empirical probabilities\n",
    "char_probs = {ch: count / total_chars for ch, count in char_counts.items()}\n",
    "\n",
    "    \n",
    "# Convert to sorted lists for indexing\n",
    "char_list = sorted(char_probs.keys())\n",
    "prob_list = np.array([char_probs[ch] for ch in char_list])\n",
    "\n",
    "# Compute a distance matrix based on probability space\n",
    "distance_matrix = np.zeros((len(char_list), len(char_list)))\n",
    "\n",
    "for i, j in itertools.combinations(range(len(char_list)), 2):\n",
    "    # Construct two-outcome distributions for each character probability\n",
    "    p, q = prob_list[i], prob_list[j]\n",
    "    # For our purposes, consider distributions like [p, 1-p] and [q, 1-q]\n",
    "    m = 0.5 * (p + q)\n",
    "    distance = 0.5 * (p * np.log(p/m) + (1-p) * np.log((1-p)/(1-m)) +\n",
    "                      q * np.log(q/m) + (1-q) * np.log((1-q)/(1-m)))\n",
    "    distance_matrix[i, j] = distance_matrix[j, i] = distance\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Training Setup\n",
    "# ====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TapeTransformer(\n",
    "    vocab_size=vocab_size,  # example\n",
    "    seq_len=256,\n",
    "    chunk_len=4,\n",
    "    embed_dim=256,\n",
    "    num_layers=6,\n",
    "    num_heads=4,\n",
    "    placeholder_idx=placeholder_idx,\n",
    "    dropout=0 #cannot use dropout, tooo slow\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "seq_len = 256 #from karapathy\n",
    "\n",
    "\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14159782"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def adaptive_distance_loss(preds, targets, placeholder_idx, distance_matrix, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Compute an adaptive loss that scales the standard -log(prob) loss by a \n",
    "    distance-based factor, and for placeholder positions uses a computed value \n",
    "    based on the non-placeholder losses, ensuring all operations remain on the graph.\n",
    "    \n",
    "    Args:\n",
    "        preds: (B, S, V) predicted probability distributions.\n",
    "        targets: (B, S) ground-truth indices.\n",
    "        placeholder_idx: index for the placeholder token.\n",
    "        distance_matrix: (V, V) precomputed distances (assumed not to require gradients).\n",
    "        eps: small constant to prevent division-by-zero.\n",
    "    \n",
    "    Returns:\n",
    "        A scalar loss value.\n",
    "    \"\"\"\n",
    "    B, S, V = preds.shape\n",
    "    device = preds.device\n",
    "    \n",
    "    # Ensure distance_matrix is a tensor (and remains part of the graph)\n",
    "    if isinstance(distance_matrix, np.ndarray):\n",
    "        distance_matrix = torch.tensor(distance_matrix, dtype=torch.float, device=device, requires_grad=True)\n",
    "    else:\n",
    "        distance_matrix = distance_matrix.to(device).requires_grad_(True)\n",
    "    \n",
    "    # Validate index bounds before accessing tensors\n",
    "    assert targets.min() >= 0 and targets.max() < V, f\"targets contains invalid indices: min={targets.min()}, max={targets.max()}\"\n",
    "    \n",
    "    # Standard loss: -log(prob(target))\n",
    "    gathered_probs = torch.gather(preds, -1, targets.unsqueeze(-1)).squeeze(-1)\n",
    "    base_loss = -torch.log(gathered_probs + eps)  # (B, S)\n",
    "    \n",
    "    # Get the predicted token (argmax) for each entry\n",
    "    pred_indices = torch.argmax(preds, dim=-1)  # (B, S)\n",
    "    \n",
    "    # Validate pred_indices bounds\n",
    "    assert pred_indices.min() >= 0 and pred_indices.max() < V, f\"pred_indices out of bounds: min={pred_indices.min()}, max={pred_indices.max()}\"\n",
    "    \n",
    "    # If all predictions are correct, return zero loss\n",
    "    if torch.all(pred_indices == targets):\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    # Look up the distance between the target and predicted token from the matrix.\n",
    "    distances = distance_matrix[targets, pred_indices]  # (B, S)\n",
    "    \n",
    "    # Compute per-target min and max distances for scaling\n",
    "    target_rows = distance_matrix[targets]  # (B, S, V)\n",
    "    target_min_distances = target_rows.min(dim=-1).values  # (B, S)\n",
    "    global_max = distance_matrix.max()\n",
    "    target_max_distances = target_min_distances + 0.5 * (global_max - target_min_distances)\n",
    "    \n",
    "    # Normalize the distance for each token and use it to scale base_loss.\n",
    "    norm_distances = (distances - target_min_distances) / (target_max_distances - target_min_distances + eps)\n",
    "    scaled_loss = base_loss * norm_distances  # (B, S)\n",
    "    \n",
    "    # Create boolean masks for placeholder vs. non-placeholder positions.\n",
    "    is_placeholder = (targets == placeholder_idx)  # (B, S)\n",
    "    is_non_placeholder = ~is_placeholder\n",
    "    \n",
    "    # Validate placeholder index\n",
    "    assert 0 <= placeholder_idx < V, f\"placeholder_idx out of bounds: {placeholder_idx}\"\n",
    "    \n",
    "    # Compute the \"percentile\" value (1/eth percentile) from non-placeholder losses, keeping it on the graph\n",
    "    if is_non_placeholder.any():\n",
    "        live_nonph = scaled_loss[is_non_placeholder]  # live tensor, carries grad\n",
    "        nonph_sorted, _ = torch.sort(live_nonph.view(-1))\n",
    "        pct_index = max(0, min(int(len(nonph_sorted) * (1 / math.e)), len(nonph_sorted) - 1))\n",
    "        placeholder_loss_value = nonph_sorted[pct_index]  # Keep this on the graph\n",
    "    else:\n",
    "        placeholder_loss_value = scaled_loss.mean()  # Still on the graph\n",
    "    \n",
    "    # Compute final loss ensuring full differentiability\n",
    "    final_loss = torch.where(is_placeholder, placeholder_loss_value, scaled_loss).mean()\n",
    "    return final_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m distance_matrix_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistance_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "distance_matrix_tensor = torch.tensor(distance_matrix, dtype=torch.float, device=device, requires_grad=True)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Increase sample probability over time\n",
    "    sample_prob = min(0.1 * (epoch + 1), 0.5)  # Linearly increases up to 50%\n",
    "    gen_chars = min(1 + epoch, 5)  # Gradually increase generated tokens per epoch\n",
    "\n",
    "    for step in range(100):  # Adjust as needed\n",
    "        x_batch, targets = get_batch(batch_size, seq_len)\n",
    "        x_batch = x_batch.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            preds = model(x_batch)\n",
    "            num_placeholders = (preds == placeholder_idx).sum().item()\n",
    "            loss = adaptive_distance_loss(preds, targets, placeholder_idx, distance_matrix_tensor)        \n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        main_loss = loss.detach()\n",
    "        print(f\"Step {step}, Placeholders in batch: {num_placeholders}, Loss: {main_loss.item()}\")\n",
    "\n",
    "        total_loss += main_loss.item()\n",
    "        losses.append(main_loss.cpu())\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {main_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss/1000:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 13.0857\n",
      "Epoch 1, Step 1, Loss: 13.0916\n",
      "Epoch 1, Step 2, Loss: 13.1046\n",
      "Epoch 1, Step 3, Loss: 13.0958\n",
      "Epoch 1, Step 4, Loss: 13.1020\n",
      "Epoch 1, Step 5, Loss: 13.0963\n",
      "Epoch 1, Step 6, Loss: 13.0984\n",
      "Epoch 1, Step 7, Loss: 13.1022\n",
      "Epoch 1, Step 8, Loss: 13.0943\n",
      "Epoch 1, Step 9, Loss: 13.0966\n",
      "Epoch 1, Step 10, Loss: 13.0949\n",
      "Epoch 1, Step 11, Loss: 13.0790\n",
      "Epoch 1, Step 12, Loss: 13.0764\n",
      "Epoch 1, Step 13, Loss: 13.0686\n",
      "Epoch 1, Step 14, Loss: 13.0723\n",
      "Epoch 1, Step 15, Loss: 13.0712\n",
      "Epoch 1, Step 16, Loss: 13.0652\n",
      "Epoch 1, Step 17, Loss: 13.0621\n",
      "Epoch 1, Step 18, Loss: 13.0623\n",
      "Epoch 1, Step 19, Loss: 13.0693\n",
      "Epoch 1, Step 20, Loss: 13.0697\n",
      "Epoch 1, Step 21, Loss: 13.0638\n",
      "Epoch 1, Step 22, Loss: 13.0646\n",
      "Epoch 1, Step 23, Loss: 13.0648\n",
      "Epoch 1, Step 24, Loss: 13.0646\n",
      "Epoch 1, Step 25, Loss: 13.0611\n",
      "Epoch 1, Step 26, Loss: 13.0600\n",
      "Epoch 1, Step 27, Loss: 13.0647\n",
      "Epoch 1, Step 28, Loss: 13.0565\n",
      "Epoch 1, Step 29, Loss: 13.0657\n",
      "Epoch 1, Step 30, Loss: 13.0616\n",
      "Epoch 1, Step 31, Loss: 13.0565\n",
      "Epoch 1, Step 32, Loss: 13.0649\n",
      "Epoch 1, Step 33, Loss: 13.0598\n",
      "Epoch 1, Step 34, Loss: 13.0621\n",
      "Epoch 1, Step 35, Loss: 13.0609\n",
      "Epoch 1, Step 36, Loss: 13.0599\n",
      "Epoch 1, Step 37, Loss: 13.0617\n",
      "Epoch 1, Step 38, Loss: 13.0604\n",
      "Epoch 1, Step 39, Loss: 13.0568\n",
      "Epoch 1, Step 40, Loss: 13.0644\n",
      "Epoch 1, Step 41, Loss: 13.0625\n",
      "Epoch 1, Step 42, Loss: 13.0518\n",
      "Epoch 1, Step 43, Loss: 13.0631\n",
      "Epoch 1, Step 44, Loss: 13.0650\n",
      "Epoch 1, Step 45, Loss: 13.0612\n",
      "Epoch 1, Step 46, Loss: 13.0643\n",
      "Epoch 1, Step 47, Loss: 13.0617\n",
      "Epoch 1, Step 48, Loss: 13.0600\n",
      "Epoch 1, Step 49, Loss: 13.0549\n",
      "Epoch 1, Step 50, Loss: 13.0612\n",
      "Epoch 1, Step 51, Loss: 13.0593\n",
      "Epoch 1, Step 52, Loss: 13.0613\n",
      "Epoch 1, Step 53, Loss: 13.0592\n",
      "Epoch 1, Step 54, Loss: 13.0620\n",
      "Epoch 1, Step 55, Loss: 13.0605\n",
      "Epoch 1, Step 56, Loss: 13.0602\n",
      "Epoch 1, Step 57, Loss: 13.0647\n",
      "Epoch 1, Step 58, Loss: 13.0618\n",
      "Epoch 1, Step 59, Loss: 13.0527\n",
      "Epoch 1, Step 60, Loss: 13.0605\n",
      "Epoch 1, Step 61, Loss: 13.0599\n",
      "Epoch 1, Step 62, Loss: 13.0615\n",
      "Epoch 1, Step 63, Loss: 13.0601\n",
      "Epoch 1, Step 64, Loss: 13.0605\n",
      "Epoch 1, Step 65, Loss: 13.0601\n",
      "Epoch 1, Step 66, Loss: 13.0593\n",
      "Epoch 1, Step 67, Loss: 13.0596\n",
      "Epoch 1, Step 68, Loss: 13.0595\n",
      "Epoch 1, Step 69, Loss: 13.0582\n",
      "Epoch 1, Step 70, Loss: 13.0589\n",
      "Epoch 1, Step 71, Loss: 13.0530\n",
      "Epoch 1, Step 72, Loss: 13.0630\n",
      "Epoch 1, Step 73, Loss: 13.0613\n",
      "Epoch 1, Step 74, Loss: 13.0628\n",
      "Epoch 1, Step 75, Loss: 13.0588\n",
      "Epoch 1, Step 76, Loss: 13.0600\n",
      "Epoch 1, Step 77, Loss: 13.0624\n",
      "Epoch 1, Step 78, Loss: 13.0560\n",
      "Epoch 1, Step 79, Loss: 13.0623\n",
      "Epoch 1, Step 80, Loss: 13.0599\n",
      "Epoch 1, Step 81, Loss: 13.0596\n",
      "Epoch 1, Step 82, Loss: 13.0523\n",
      "Epoch 1, Step 83, Loss: 13.0605\n",
      "Epoch 1, Step 84, Loss: 13.0585\n",
      "Epoch 1, Step 85, Loss: 13.0574\n",
      "Epoch 1, Step 86, Loss: 13.0589\n",
      "Epoch 1, Step 87, Loss: 13.0614\n",
      "Epoch 1, Step 88, Loss: 13.0544\n",
      "Epoch 1, Step 89, Loss: 13.0590\n",
      "Epoch 1, Step 90, Loss: 13.0634\n",
      "Epoch 1, Step 91, Loss: 13.0601\n",
      "Epoch 1, Step 92, Loss: 13.0623\n",
      "Epoch 1, Step 93, Loss: 13.0594\n",
      "Epoch 1, Step 94, Loss: 13.0611\n",
      "Epoch 1, Step 95, Loss: 13.0532\n",
      "Epoch 1, Step 96, Loss: 13.0599\n",
      "Epoch 1, Step 97, Loss: 13.0514\n",
      "Epoch 1, Step 98, Loss: 13.0606\n",
      "Epoch 1, Step 99, Loss: 13.0593\n",
      "Epoch 1 Average Loss: 1.3065\n",
      "Epoch 2, Step 0, Loss: 13.0207\n",
      "Epoch 2, Step 1, Loss: 13.0152\n",
      "Epoch 2, Step 2, Loss: 13.0223\n",
      "Epoch 2, Step 3, Loss: 13.0197\n",
      "Epoch 2, Step 4, Loss: 13.0197\n",
      "Epoch 2, Step 5, Loss: 13.0158\n",
      "Epoch 2, Step 6, Loss: 13.0247\n",
      "Epoch 2, Step 7, Loss: 13.0218\n",
      "Epoch 2, Step 8, Loss: 13.0224\n",
      "Epoch 2, Step 9, Loss: 13.0169\n",
      "Epoch 2, Step 10, Loss: 13.0139\n",
      "Epoch 2, Step 11, Loss: 13.0186\n",
      "Epoch 2, Step 12, Loss: 13.0225\n",
      "Epoch 2, Step 13, Loss: 13.0121\n",
      "Epoch 2, Step 14, Loss: 13.0246\n",
      "Epoch 2, Step 15, Loss: 13.0183\n",
      "Epoch 2, Step 16, Loss: 13.0209\n",
      "Epoch 2, Step 17, Loss: 13.0252\n",
      "Epoch 2, Step 18, Loss: 13.0007\n",
      "Epoch 2, Step 19, Loss: 13.0205\n",
      "Epoch 2, Step 20, Loss: 13.0155\n",
      "Epoch 2, Step 21, Loss: 13.0180\n",
      "Epoch 2, Step 22, Loss: 13.0154\n",
      "Epoch 2, Step 23, Loss: 13.0204\n",
      "Epoch 2, Step 24, Loss: 13.0120\n",
      "Epoch 2, Step 25, Loss: 13.0184\n",
      "Epoch 2, Step 26, Loss: 13.0143\n",
      "Epoch 2, Step 27, Loss: 13.0204\n",
      "Epoch 2, Step 28, Loss: 13.0130\n",
      "Epoch 2, Step 29, Loss: 13.0217\n",
      "Epoch 2, Step 30, Loss: 13.0150\n",
      "Epoch 2, Step 31, Loss: 13.0195\n",
      "Epoch 2, Step 32, Loss: 13.0217\n",
      "Epoch 2, Step 33, Loss: 13.0185\n",
      "Epoch 2, Step 34, Loss: 13.0161\n",
      "Epoch 2, Step 35, Loss: 13.0151\n",
      "Epoch 2, Step 36, Loss: 13.0238\n",
      "Epoch 2, Step 37, Loss: 13.0231\n",
      "Epoch 2, Step 38, Loss: 13.0141\n",
      "Epoch 2, Step 39, Loss: 13.0232\n",
      "Epoch 2, Step 40, Loss: 13.0225\n",
      "Epoch 2, Step 41, Loss: 13.0144\n",
      "Epoch 2, Step 42, Loss: 13.0208\n",
      "Epoch 2, Step 43, Loss: 13.0152\n",
      "Epoch 2, Step 44, Loss: 13.0198\n",
      "Epoch 2, Step 45, Loss: 13.0159\n",
      "Epoch 2, Step 46, Loss: 13.0140\n",
      "Epoch 2, Step 47, Loss: 13.0141\n",
      "Epoch 2, Step 48, Loss: 12.9820\n",
      "Epoch 2, Step 49, Loss: 13.0184\n",
      "Epoch 2, Step 50, Loss: 13.0118\n",
      "Epoch 2, Step 51, Loss: 13.0219\n",
      "Epoch 2, Step 52, Loss: 13.0187\n",
      "Epoch 2, Step 53, Loss: 13.0222\n",
      "Epoch 2, Step 54, Loss: 13.0241\n",
      "Epoch 2, Step 55, Loss: 13.0134\n",
      "Epoch 2, Step 56, Loss: 13.0212\n",
      "Epoch 2, Step 57, Loss: 13.0149\n",
      "Epoch 2, Step 58, Loss: 13.0198\n",
      "Epoch 2, Step 59, Loss: 13.0142\n",
      "Epoch 2, Step 60, Loss: 13.0213\n",
      "Epoch 2, Step 61, Loss: 13.0191\n",
      "Epoch 2, Step 62, Loss: 13.0206\n",
      "Epoch 2, Step 63, Loss: 13.0052\n",
      "Epoch 2, Step 64, Loss: 13.0221\n",
      "Epoch 2, Step 65, Loss: 13.0220\n",
      "Epoch 2, Step 66, Loss: 13.0186\n",
      "Epoch 2, Step 67, Loss: 13.0193\n",
      "Epoch 2, Step 68, Loss: 13.0154\n",
      "Epoch 2, Step 69, Loss: 13.0160\n",
      "Epoch 2, Step 70, Loss: 13.0143\n",
      "Epoch 2, Step 71, Loss: 13.0104\n",
      "Epoch 2, Step 72, Loss: 13.0205\n",
      "Epoch 2, Step 73, Loss: 13.0229\n",
      "Epoch 2, Step 74, Loss: 13.0222\n",
      "Epoch 2, Step 75, Loss: 13.0227\n",
      "Epoch 2, Step 76, Loss: 13.0194\n",
      "Epoch 2, Step 77, Loss: 13.0185\n",
      "Epoch 2, Step 78, Loss: 13.0157\n",
      "Epoch 2, Step 79, Loss: 13.0224\n",
      "Epoch 2, Step 80, Loss: 13.0230\n",
      "Epoch 2, Step 81, Loss: 13.0140\n",
      "Epoch 2, Step 82, Loss: 13.0208\n",
      "Epoch 2, Step 83, Loss: 13.0148\n",
      "Epoch 2, Step 84, Loss: 13.0223\n",
      "Epoch 2, Step 85, Loss: 13.0213\n",
      "Epoch 2, Step 86, Loss: 13.0196\n",
      "Epoch 2, Step 87, Loss: 13.0208\n",
      "Epoch 2, Step 88, Loss: 13.0184\n",
      "Epoch 2, Step 89, Loss: 13.0158\n",
      "Epoch 2, Step 90, Loss: 13.0185\n",
      "Epoch 2, Step 91, Loss: 13.0208\n",
      "Epoch 2, Step 92, Loss: 13.0247\n",
      "Epoch 2, Step 93, Loss: 13.0154\n",
      "Epoch 2, Step 94, Loss: 13.0183\n",
      "Epoch 2, Step 95, Loss: 13.0210\n",
      "Epoch 2, Step 96, Loss: 13.0152\n",
      "Epoch 2, Step 97, Loss: 13.0247\n",
      "Epoch 2, Step 98, Loss: 13.0195\n",
      "Epoch 2, Step 99, Loss: 13.0136\n",
      "Epoch 2 Average Loss: 1.3018\n",
      "Epoch 3, Step 0, Loss: 12.9835\n",
      "Epoch 3, Step 1, Loss: 12.9778\n",
      "Epoch 3, Step 2, Loss: 12.9698\n",
      "Epoch 3, Step 3, Loss: 12.9773\n",
      "Epoch 3, Step 4, Loss: 12.9717\n",
      "Epoch 3, Step 5, Loss: 12.9838\n",
      "Epoch 3, Step 6, Loss: 12.9751\n",
      "Epoch 3, Step 7, Loss: 12.9782\n",
      "Epoch 3, Step 8, Loss: 12.9794\n",
      "Epoch 3, Step 9, Loss: 12.9772\n",
      "Epoch 3, Step 10, Loss: 12.9845\n",
      "Epoch 3, Step 11, Loss: 12.9866\n",
      "Epoch 3, Step 12, Loss: 12.9763\n",
      "Epoch 3, Step 13, Loss: 12.9766\n",
      "Epoch 3, Step 14, Loss: 12.9775\n",
      "Epoch 3, Step 15, Loss: 12.9658\n",
      "Epoch 3, Step 16, Loss: 12.9823\n",
      "Epoch 3, Step 17, Loss: 12.9817\n",
      "Epoch 3, Step 18, Loss: 12.9856\n",
      "Epoch 3, Step 19, Loss: 12.9785\n",
      "Epoch 3, Step 20, Loss: 12.9745\n",
      "Epoch 3, Step 21, Loss: 12.9807\n",
      "Epoch 3, Step 22, Loss: 12.9808\n",
      "Epoch 3, Step 23, Loss: 12.9717\n",
      "Epoch 3, Step 24, Loss: 12.9857\n",
      "Epoch 3, Step 25, Loss: 12.9855\n",
      "Epoch 3, Step 26, Loss: 12.9836\n",
      "Epoch 3, Step 27, Loss: 12.9781\n",
      "Epoch 3, Step 28, Loss: 12.9800\n",
      "Epoch 3, Step 29, Loss: 12.9756\n",
      "Epoch 3, Step 30, Loss: 12.9844\n",
      "Epoch 3, Step 31, Loss: 12.9810\n",
      "Epoch 3, Step 32, Loss: 12.9804\n",
      "Epoch 3, Step 33, Loss: 12.9707\n",
      "Epoch 3, Step 34, Loss: 12.9807\n",
      "Epoch 3, Step 35, Loss: 12.9864\n",
      "Epoch 3, Step 36, Loss: 12.9772\n",
      "Epoch 3, Step 37, Loss: 12.9803\n",
      "Epoch 3, Step 38, Loss: 12.9799\n",
      "Epoch 3, Step 39, Loss: 12.9774\n",
      "Epoch 3, Step 40, Loss: 12.9762\n",
      "Epoch 3, Step 41, Loss: 12.9789\n",
      "Epoch 3, Step 42, Loss: 12.9550\n",
      "Epoch 3, Step 43, Loss: 12.9772\n",
      "Epoch 3, Step 44, Loss: 12.9813\n",
      "Epoch 3, Step 45, Loss: 12.9740\n",
      "Epoch 3, Step 46, Loss: 12.9780\n",
      "Epoch 3, Step 47, Loss: 12.9818\n",
      "Epoch 3, Step 48, Loss: 12.9714\n",
      "Epoch 3, Step 49, Loss: 12.9799\n",
      "Epoch 3, Step 50, Loss: 12.9793\n",
      "Epoch 3, Step 51, Loss: 12.9773\n",
      "Epoch 3, Step 52, Loss: 12.9729\n",
      "Epoch 3, Step 53, Loss: 12.9712\n",
      "Epoch 3, Step 54, Loss: 12.9782\n",
      "Epoch 3, Step 55, Loss: 12.9568\n",
      "Epoch 3, Step 56, Loss: 12.9818\n",
      "Epoch 3, Step 57, Loss: 12.9843\n",
      "Epoch 3, Step 58, Loss: 12.9836\n",
      "Epoch 3, Step 59, Loss: 12.9816\n",
      "Epoch 3, Step 60, Loss: 12.9826\n",
      "Epoch 3, Step 61, Loss: 12.9639\n",
      "Epoch 3, Step 62, Loss: 12.9830\n",
      "Epoch 3, Step 63, Loss: 12.9766\n",
      "Epoch 3, Step 64, Loss: 12.9842\n",
      "Epoch 3, Step 65, Loss: 12.9782\n",
      "Epoch 3, Step 66, Loss: 12.9781\n",
      "Epoch 3, Step 67, Loss: 12.9771\n",
      "Epoch 3, Step 68, Loss: 12.9787\n",
      "Epoch 3, Step 69, Loss: 12.9720\n",
      "Epoch 3, Step 70, Loss: 12.9448\n",
      "Epoch 3, Step 71, Loss: 12.9802\n",
      "Epoch 3, Step 72, Loss: 12.9819\n",
      "Epoch 3, Step 73, Loss: 12.9848\n",
      "Epoch 3, Step 74, Loss: 12.9793\n",
      "Epoch 3, Step 75, Loss: 12.9764\n",
      "Epoch 3, Step 76, Loss: 12.9614\n",
      "Epoch 3, Step 77, Loss: 12.9814\n",
      "Epoch 3, Step 78, Loss: 12.9760\n",
      "Epoch 3, Step 79, Loss: 12.9828\n",
      "Epoch 3, Step 80, Loss: 12.9759\n",
      "Epoch 3, Step 81, Loss: 12.9843\n",
      "Epoch 3, Step 82, Loss: 12.9550\n",
      "Epoch 3, Step 83, Loss: 12.9301\n",
      "Epoch 3, Step 84, Loss: 12.9784\n",
      "Epoch 3, Step 85, Loss: 12.9835\n",
      "Epoch 3, Step 86, Loss: 12.9587\n",
      "Epoch 3, Step 87, Loss: 12.9736\n",
      "Epoch 3, Step 88, Loss: 12.9865\n",
      "Epoch 3, Step 89, Loss: 12.9811\n",
      "Epoch 3, Step 90, Loss: 12.9524\n",
      "Epoch 3, Step 91, Loss: 12.9720\n",
      "Epoch 3, Step 92, Loss: 12.9869\n",
      "Epoch 3, Step 93, Loss: 12.9614\n",
      "Epoch 3, Step 94, Loss: 12.9724\n",
      "Epoch 3, Step 95, Loss: 12.9817\n",
      "Epoch 3, Step 96, Loss: 12.9817\n",
      "Epoch 3, Step 97, Loss: 12.9674\n",
      "Epoch 3, Step 98, Loss: 12.9806\n",
      "Epoch 3, Step 99, Loss: 12.9814\n",
      "Epoch 3 Average Loss: 1.2977\n",
      "Epoch 4, Step 0, Loss: 12.9378\n",
      "Epoch 4, Step 1, Loss: 12.8643\n",
      "Epoch 4, Step 2, Loss: 12.9369\n",
      "Epoch 4, Step 3, Loss: 12.9409\n",
      "Epoch 4, Step 4, Loss: 12.9399\n",
      "Epoch 4, Step 5, Loss: 12.9457\n",
      "Epoch 4, Step 6, Loss: 12.9432\n",
      "Epoch 4, Step 7, Loss: 12.9092\n",
      "Epoch 4, Step 8, Loss: 12.9478\n",
      "Epoch 4, Step 9, Loss: 12.9418\n",
      "Epoch 4, Step 10, Loss: 12.8868\n",
      "Epoch 4, Step 11, Loss: 12.9345\n",
      "Epoch 4, Step 12, Loss: 12.9384\n",
      "Epoch 4, Step 13, Loss: 12.9453\n",
      "Epoch 4, Step 14, Loss: 12.9393\n",
      "Epoch 4, Step 15, Loss: 12.9436\n",
      "Epoch 4, Step 16, Loss: 12.9461\n",
      "Epoch 4, Step 17, Loss: 12.9362\n",
      "Epoch 4, Step 18, Loss: 12.9421\n",
      "Epoch 4, Step 19, Loss: 12.9437\n",
      "Epoch 4, Step 20, Loss: 12.9349\n",
      "Epoch 4, Step 21, Loss: 12.9277\n",
      "Epoch 4, Step 22, Loss: 12.9442\n",
      "Epoch 4, Step 23, Loss: 12.9278\n",
      "Epoch 4, Step 24, Loss: 12.9382\n",
      "Epoch 4, Step 25, Loss: 12.9438\n",
      "Epoch 4, Step 26, Loss: 12.9426\n",
      "Epoch 4, Step 27, Loss: 12.9296\n",
      "Epoch 4, Step 28, Loss: 12.9368\n",
      "Epoch 4, Step 29, Loss: 12.9420\n",
      "Epoch 4, Step 30, Loss: 12.9313\n",
      "Epoch 4, Step 31, Loss: 12.9465\n",
      "Epoch 4, Step 32, Loss: 12.9206\n",
      "Epoch 4, Step 33, Loss: 12.9451\n",
      "Epoch 4, Step 34, Loss: 12.9297\n",
      "Epoch 4, Step 35, Loss: 12.9466\n",
      "Epoch 4, Step 36, Loss: 12.9439\n",
      "Epoch 4, Step 37, Loss: 12.9282\n",
      "Epoch 4, Step 38, Loss: 12.9440\n",
      "Epoch 4, Step 39, Loss: 12.9330\n",
      "Epoch 4, Step 40, Loss: 12.9433\n",
      "Epoch 4, Step 41, Loss: 12.9360\n",
      "Epoch 4, Step 42, Loss: 12.9366\n",
      "Epoch 4, Step 43, Loss: 12.9253\n",
      "Epoch 4, Step 44, Loss: 12.9475\n",
      "Epoch 4, Step 45, Loss: 12.9446\n",
      "Epoch 4, Step 46, Loss: 12.9414\n",
      "Epoch 4, Step 47, Loss: 12.9394\n",
      "Epoch 4, Step 48, Loss: 12.9328\n",
      "Epoch 4, Step 49, Loss: 12.9197\n",
      "Epoch 4, Step 50, Loss: 12.9432\n",
      "Epoch 4, Step 51, Loss: 12.9288\n",
      "Epoch 4, Step 52, Loss: 12.9292\n",
      "Epoch 4, Step 53, Loss: 12.9452\n",
      "Epoch 4, Step 54, Loss: 12.9437\n",
      "Epoch 4, Step 55, Loss: 12.9425\n",
      "Epoch 4, Step 56, Loss: 12.9405\n",
      "Epoch 4, Step 57, Loss: 12.9499\n",
      "Epoch 4, Step 58, Loss: 12.9281\n",
      "Epoch 4, Step 59, Loss: 12.9391\n",
      "Epoch 4, Step 60, Loss: 12.9431\n",
      "Epoch 4, Step 61, Loss: 12.9420\n",
      "Epoch 4, Step 62, Loss: 12.9159\n",
      "Epoch 4, Step 63, Loss: 12.9336\n",
      "Epoch 4, Step 64, Loss: 12.9407\n",
      "Epoch 4, Step 65, Loss: 12.9459\n",
      "Epoch 4, Step 66, Loss: 12.9429\n",
      "Epoch 4, Step 67, Loss: 12.9445\n",
      "Epoch 4, Step 68, Loss: 12.9342\n",
      "Epoch 4, Step 69, Loss: 12.9291\n",
      "Epoch 4, Step 70, Loss: 12.9422\n",
      "Epoch 4, Step 71, Loss: 12.9296\n",
      "Epoch 4, Step 72, Loss: 12.9450\n",
      "Epoch 4, Step 73, Loss: 12.9294\n",
      "Epoch 4, Step 74, Loss: 12.9447\n",
      "Epoch 4, Step 75, Loss: 12.9416\n",
      "Epoch 4, Step 76, Loss: 12.9429\n",
      "Epoch 4, Step 77, Loss: 12.9431\n",
      "Epoch 4, Step 78, Loss: 12.9389\n",
      "Epoch 4, Step 79, Loss: 12.9349\n",
      "Epoch 4, Step 80, Loss: 12.9365\n",
      "Epoch 4, Step 81, Loss: 12.9259\n",
      "Epoch 4, Step 82, Loss: 12.9039\n",
      "Epoch 4, Step 83, Loss: 12.9406\n",
      "Epoch 4, Step 84, Loss: 12.9432\n",
      "Epoch 4, Step 85, Loss: 12.9359\n",
      "Epoch 4, Step 86, Loss: 12.9348\n",
      "Epoch 4, Step 87, Loss: 12.9430\n",
      "Epoch 4, Step 88, Loss: 12.9394\n",
      "Epoch 4, Step 89, Loss: 12.9282\n",
      "Epoch 4, Step 90, Loss: 12.9287\n",
      "Epoch 4, Step 91, Loss: 12.9407\n",
      "Epoch 4, Step 92, Loss: 12.9467\n",
      "Epoch 4, Step 93, Loss: 12.9383\n",
      "Epoch 4, Step 94, Loss: 12.9296\n",
      "Epoch 4, Step 95, Loss: 12.9466\n",
      "Epoch 4, Step 96, Loss: 12.9359\n",
      "Epoch 4, Step 97, Loss: 12.9409\n",
      "Epoch 4, Step 98, Loss: 12.8816\n",
      "Epoch 4, Step 99, Loss: 12.9424\n",
      "Epoch 4 Average Loss: 1.2936\n",
      "Epoch 5, Step 0, Loss: 12.8981\n",
      "Epoch 5, Step 1, Loss: 12.8914\n",
      "Epoch 5, Step 2, Loss: 12.9024\n",
      "Epoch 5, Step 3, Loss: 12.8881\n",
      "Epoch 5, Step 4, Loss: 12.9002\n",
      "Epoch 5, Step 5, Loss: 12.8915\n",
      "Epoch 5, Step 6, Loss: 12.9041\n",
      "Epoch 5, Step 7, Loss: 12.9123\n",
      "Epoch 5, Step 8, Loss: 12.9096\n",
      "Epoch 5, Step 9, Loss: 12.8930\n",
      "Epoch 5, Step 10, Loss: 12.8880\n",
      "Epoch 5, Step 11, Loss: 12.8902\n",
      "Epoch 5, Step 12, Loss: 12.8997\n",
      "Epoch 5, Step 13, Loss: 12.8900\n",
      "Epoch 5, Step 14, Loss: 12.8913\n",
      "Epoch 5, Step 15, Loss: 12.9078\n",
      "Epoch 5, Step 16, Loss: 12.8715\n",
      "Epoch 5, Step 17, Loss: 12.9017\n",
      "Epoch 5, Step 18, Loss: 12.8806\n",
      "Epoch 5, Step 19, Loss: 12.9080\n",
      "Epoch 5, Step 20, Loss: 12.8737\n",
      "Epoch 5, Step 21, Loss: 12.8953\n",
      "Epoch 5, Step 22, Loss: 12.9031\n",
      "Epoch 5, Step 23, Loss: 12.9082\n",
      "Epoch 5, Step 24, Loss: 12.9031\n",
      "Epoch 5, Step 25, Loss: 12.9048\n",
      "Epoch 5, Step 26, Loss: 12.8916\n",
      "Epoch 5, Step 27, Loss: 12.8801\n",
      "Epoch 5, Step 28, Loss: 12.9038\n",
      "Epoch 5, Step 29, Loss: 12.8972\n",
      "Epoch 5, Step 30, Loss: 12.8918\n",
      "Epoch 5, Step 31, Loss: 12.8893\n",
      "Epoch 5, Step 32, Loss: 12.8875\n",
      "Epoch 5, Step 33, Loss: 12.9033\n",
      "Epoch 5, Step 34, Loss: 12.8992\n",
      "Epoch 5, Step 35, Loss: 12.8931\n",
      "Epoch 5, Step 36, Loss: 12.9019\n",
      "Epoch 5, Step 37, Loss: 12.8872\n",
      "Epoch 5, Step 38, Loss: 12.8911\n",
      "Epoch 5, Step 39, Loss: 12.9138\n",
      "Epoch 5, Step 40, Loss: 12.8951\n",
      "Epoch 5, Step 41, Loss: 12.8732\n",
      "Epoch 5, Step 42, Loss: 12.8950\n",
      "Epoch 5, Step 43, Loss: 12.8982\n",
      "Epoch 5, Step 44, Loss: 12.9103\n",
      "Epoch 5, Step 45, Loss: 12.8616\n",
      "Epoch 5, Step 46, Loss: 12.9060\n",
      "Epoch 5, Step 47, Loss: 12.8993\n",
      "Epoch 5, Step 48, Loss: 12.8864\n",
      "Epoch 5, Step 49, Loss: 12.8879\n",
      "Epoch 5, Step 50, Loss: 12.8884\n",
      "Epoch 5, Step 51, Loss: 12.8982\n",
      "Epoch 5, Step 52, Loss: 12.9005\n",
      "Epoch 5, Step 53, Loss: 12.8995\n",
      "Epoch 5, Step 54, Loss: 12.9050\n",
      "Epoch 5, Step 55, Loss: 12.9075\n",
      "Epoch 5, Step 56, Loss: 12.8868\n",
      "Epoch 5, Step 57, Loss: 12.8972\n",
      "Epoch 5, Step 58, Loss: 12.8772\n",
      "Epoch 5, Step 59, Loss: 12.8968\n",
      "Epoch 5, Step 60, Loss: 12.9064\n",
      "Epoch 5, Step 61, Loss: 12.8926\n",
      "Epoch 5, Step 62, Loss: 12.9050\n",
      "Epoch 5, Step 63, Loss: 12.9056\n",
      "Epoch 5, Step 64, Loss: 12.8998\n",
      "Epoch 5, Step 65, Loss: 12.9022\n",
      "Epoch 5, Step 66, Loss: 12.9016\n",
      "Epoch 5, Step 67, Loss: 12.8931\n",
      "Epoch 5, Step 68, Loss: 12.8833\n",
      "Epoch 5, Step 69, Loss: 12.8653\n",
      "Epoch 5, Step 70, Loss: 12.8437\n",
      "Epoch 5, Step 71, Loss: 12.9035\n",
      "Epoch 5, Step 72, Loss: 12.9051\n",
      "Epoch 5, Step 73, Loss: 12.8939\n",
      "Epoch 5, Step 74, Loss: 12.9036\n",
      "Epoch 5, Step 75, Loss: 12.8841\n",
      "Epoch 5, Step 76, Loss: 12.9065\n",
      "Epoch 5, Step 77, Loss: 12.8944\n",
      "Epoch 5, Step 78, Loss: 12.8851\n",
      "Epoch 5, Step 79, Loss: 12.9085\n",
      "Epoch 5, Step 80, Loss: 12.8928\n",
      "Epoch 5, Step 81, Loss: 12.8984\n",
      "Epoch 5, Step 82, Loss: 12.8893\n",
      "Epoch 5, Step 83, Loss: 12.9014\n",
      "Epoch 5, Step 84, Loss: 12.9001\n",
      "Epoch 5, Step 85, Loss: 12.8913\n",
      "Epoch 5, Step 86, Loss: 12.9013\n",
      "Epoch 5, Step 87, Loss: 12.8963\n",
      "Epoch 5, Step 88, Loss: 12.8943\n",
      "Epoch 5, Step 89, Loss: 12.9038\n",
      "Epoch 5, Step 90, Loss: 12.8985\n",
      "Epoch 5, Step 91, Loss: 12.8978\n",
      "Epoch 5, Step 92, Loss: 12.8868\n",
      "Epoch 5, Step 93, Loss: 12.8613\n",
      "Epoch 5, Step 94, Loss: 12.9081\n",
      "Epoch 5, Step 95, Loss: 12.8576\n",
      "Epoch 5, Step 96, Loss: 12.8556\n",
      "Epoch 5, Step 97, Loss: 12.8849\n",
      "Epoch 5, Step 98, Loss: 12.8867\n",
      "Epoch 5, Step 99, Loss: 12.9154\n",
      "Epoch 5 Average Loss: 1.2894\n",
      "Epoch 6, Step 0, Loss: 12.8582\n",
      "Epoch 6, Step 1, Loss: 12.9066\n",
      "Epoch 6, Step 2, Loss: 12.8801\n",
      "Epoch 6, Step 3, Loss: 12.8653\n",
      "Epoch 6, Step 4, Loss: 12.8820\n",
      "Epoch 6, Step 5, Loss: 12.9048\n",
      "Epoch 6, Step 6, Loss: 12.8830\n",
      "Epoch 6, Step 7, Loss: 12.9018\n",
      "Epoch 6, Step 8, Loss: 12.8944\n",
      "Epoch 6, Step 9, Loss: 12.9026\n",
      "Epoch 6, Step 10, Loss: 12.8463\n",
      "Epoch 6, Step 11, Loss: 12.9068\n",
      "Epoch 6, Step 12, Loss: 12.9017\n",
      "Epoch 6, Step 13, Loss: 12.9012\n",
      "Epoch 6, Step 14, Loss: 12.9006\n",
      "Epoch 6, Step 15, Loss: 12.9007\n",
      "Epoch 6, Step 16, Loss: 12.8912\n",
      "Epoch 6, Step 17, Loss: 12.9032\n",
      "Epoch 6, Step 18, Loss: 12.8693\n",
      "Epoch 6, Step 19, Loss: 12.9067\n",
      "Epoch 6, Step 20, Loss: 12.9014\n",
      "Epoch 6, Step 21, Loss: 12.8932\n",
      "Epoch 6, Step 22, Loss: 12.8991\n",
      "Epoch 6, Step 23, Loss: 12.9013\n",
      "Epoch 6, Step 24, Loss: 12.8589\n",
      "Epoch 6, Step 25, Loss: 12.8999\n",
      "Epoch 6, Step 26, Loss: 12.9014\n",
      "Epoch 6, Step 27, Loss: 12.9049\n",
      "Epoch 6, Step 28, Loss: 12.9035\n",
      "Epoch 6, Step 29, Loss: 12.8940\n",
      "Epoch 6, Step 30, Loss: 12.8857\n",
      "Epoch 6, Step 31, Loss: 12.9047\n",
      "Epoch 6, Step 32, Loss: 12.9106\n",
      "Epoch 6, Step 33, Loss: 12.9072\n",
      "Epoch 6, Step 34, Loss: 12.8972\n",
      "Epoch 6, Step 35, Loss: 12.8989\n",
      "Epoch 6, Step 36, Loss: 12.9044\n",
      "Epoch 6, Step 37, Loss: 12.9038\n",
      "Epoch 6, Step 38, Loss: 12.8923\n",
      "Epoch 6, Step 39, Loss: 12.8906\n",
      "Epoch 6, Step 40, Loss: 12.8365\n",
      "Epoch 6, Step 41, Loss: 12.8801\n",
      "Epoch 6, Step 42, Loss: 12.9005\n",
      "Epoch 6, Step 43, Loss: 12.8934\n",
      "Epoch 6, Step 44, Loss: 12.9026\n",
      "Epoch 6, Step 45, Loss: 12.8371\n",
      "Epoch 6, Step 46, Loss: 12.8804\n",
      "Epoch 6, Step 47, Loss: 12.9053\n",
      "Epoch 6, Step 48, Loss: 12.9053\n",
      "Epoch 6, Step 49, Loss: 12.8993\n",
      "Epoch 6, Step 50, Loss: 12.9053\n",
      "Epoch 6, Step 51, Loss: 12.8903\n",
      "Epoch 6, Step 52, Loss: 12.8935\n",
      "Epoch 6, Step 53, Loss: 12.8578\n",
      "Epoch 6, Step 54, Loss: 12.8933\n",
      "Epoch 6, Step 55, Loss: 12.8933\n",
      "Epoch 6, Step 56, Loss: 12.8933\n",
      "Epoch 6, Step 57, Loss: 12.9080\n",
      "Epoch 6, Step 58, Loss: 12.9068\n",
      "Epoch 6, Step 59, Loss: 12.8987\n",
      "Epoch 6, Step 60, Loss: 12.8943\n",
      "Epoch 6, Step 61, Loss: 12.8989\n",
      "Epoch 6, Step 62, Loss: 12.9033\n",
      "Epoch 6, Step 63, Loss: 12.8648\n",
      "Epoch 6, Step 64, Loss: 12.8894\n",
      "Epoch 6, Step 65, Loss: 12.8900\n",
      "Epoch 6, Step 66, Loss: 12.9083\n",
      "Epoch 6, Step 67, Loss: 12.8898\n",
      "Epoch 6, Step 68, Loss: 12.8985\n",
      "Epoch 6, Step 69, Loss: 12.8627\n",
      "Epoch 6, Step 70, Loss: 12.9031\n",
      "Epoch 6, Step 71, Loss: 12.8989\n",
      "Epoch 6, Step 72, Loss: 12.9005\n",
      "Epoch 6, Step 73, Loss: 12.8818\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 165\u001b[0m\n\u001b[0;32m    162\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m--> 165\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mscheduled_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplaceholder_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_chars\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m main_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    167\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[118], line 117\u001b[0m, in \u001b[0;36mscheduled_sampling\u001b[1;34m(batch, model, placeholder_idx, sample_prob, gen_chars, char_list, char_probs, distance_matrix)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_seq) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mseq_len:  \u001b[38;5;66;03m# Prevent overflow\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1, S, V)\u001b[39;00m\n\u001b[0;32m    118\u001b[0m predicted_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model_output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    119\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_seq, predicted_token\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Append predicted token\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[107], line 404\u001b[0m, in \u001b[0;36mTapeTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# 2) Pass through TapeHeads and apply XOR after each\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m head, xor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxor_modules):\n\u001b[1;32m--> 404\u001b[0m     x, h, logits \u001b[38;5;241m=\u001b[39m \u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Ensure logits exist before applying XOR\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         logits \u001b[38;5;241m=\u001b[39m xor(logits)              \u001b[38;5;66;03m# Apply XOR\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[107], line 332\u001b[0m, in \u001b[0;36mTapeHead.forward\u001b[1;34m(self, x, h, logits)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03mx: (B, S, D) original input\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03mh: (B, S, D) hidden state\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m  h: final refined hidden state\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 332\u001b[0m     x, h, logits \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Process each block progressively\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, h ,logits\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[107], line 270\u001b[0m, in \u001b[0;36mTapeHeadBlock.forward\u001b[1;34m(self, x, h, logits)\u001b[0m\n\u001b[0;32m    267\u001b[0m     h_attn \u001b[38;5;241m=\u001b[39m h_attn \u001b[38;5;241m+\u001b[39m logits_context  \u001b[38;5;66;03m# Now it's conditioned on past logits\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# 3) MLP processing\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m h_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_mlp(h_attn \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_attn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# 4) Compute new logits\u001b[39;00m\n\u001b[0;32m    273\u001b[0m logits \u001b[38;5;241m=\u001b[39m student_t_unembedding(h_mlp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munembedding,placeholder_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplaceholder_idx)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def modified_loss(preds, targets, placeholder_idx, df=2.718281828459, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Custom loss function to implement 'Skip vs Learn' behavior.\n",
    "\n",
    "    Arguments:\n",
    "      preds: (B, S, V) => Model's predicted probabilities over vocabulary.\n",
    "      targets: (B, S) => Ground truth token indices.\n",
    "      placeholder_idx: Index of the placeholder token.\n",
    "      df (float): Degrees of freedom for Student's t-distribution.\n",
    "      eps (float): Small constant to prevent log(0).\n",
    "\n",
    "    Returns:\n",
    "      Scalar loss.\n",
    "    \"\"\"\n",
    "    B, S, V = preds.shape\n",
    "\n",
    "    # Get predicted token indices\n",
    "    pred_indices = torch.argmax(preds, dim=-1)\n",
    "\n",
    "    # Identify where ground truth is a placeholder\n",
    "    is_placeholder = (targets == placeholder_idx)\n",
    "    is_pred_placeholder = (pred_indices == placeholder_idx)\n",
    "\n",
    "    # Compute standard loss\n",
    "    log_probs = torch.log(preds + eps)\n",
    "    loss = -log_probs[torch.arange(B).unsqueeze(1), torch.arange(S).unsqueeze(0), targets]\n",
    "\n",
    "    # Case 1: Model predicts a real token when ground truth is a placeholder â†’ Reward\n",
    "    loss = torch.where(is_placeholder & ~is_pred_placeholder, loss * (1 - 1 / math.e), loss)\n",
    "\n",
    "    # Case 2: Model predicts placeholder when ground truth is a real token â†’ Penalize\n",
    "    loss = torch.where(~is_placeholder & is_pred_placeholder, loss * (1 + 1 / math.e), loss)\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def corrupt_input_probability_based(batch, char_list, char_probs, distance_matrix, corruption_prob=1 / math.e):\n",
    "    \"\"\"\n",
    "    Corrupts a batch of tokenized text based on probability-space distances.\n",
    "    \n",
    "    Arguments:\n",
    "      batch: (B, S) Tensor of token indices.\n",
    "      char_list: List of characters sorted by index.\n",
    "      char_probs: Dictionary mapping character to probability.\n",
    "      distance_matrix: Precomputed Jensen-Shannon distances between character probabilities.\n",
    "      corruption_prob: Probability of corrupting a token.\n",
    "    \n",
    "    Returns:\n",
    "      Corrupted batch tensor.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = batch.shape\n",
    "    corrupted_batch = batch.clone()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if random.random() < corruption_prob:\n",
    "                original_idx = batch[i, j].item()\n",
    "                original_char = char_list[original_idx]\n",
    "                \n",
    "                # Get distances from the current character to all others\n",
    "                distances = distance_matrix[original_idx]\n",
    "                \n",
    "                # Select replacement character based on a random weighted shift\n",
    "                shift_distance = np.percentile(distances, 1 / math.e * 100)  # Determine shift threshold\n",
    "                valid_replacements = np.where(distances >= shift_distance)[0]  # Find candidates beyond threshold\n",
    "                \n",
    "                if len(valid_replacements) > 0:\n",
    "                    replacement_idx = np.random.choice(valid_replacements)  # Pick a random valid distant character\n",
    "                    corrupted_batch[i, j] = replacement_idx\n",
    "    \n",
    "    return corrupted_batch\n",
    "\n",
    "\n",
    "def scheduled_sampling(batch, model, placeholder_idx, sample_prob, gen_chars,char_list, char_probs, distance_matrix):\n",
    "    \"\"\"\n",
    "    Implements scheduled sampling with corruption and variable-length generation.\n",
    "    Ensures truncation never results in fewer than 3 characters.\n",
    "\n",
    "    Arguments:\n",
    "      batch: (B, S) Tensor of token indices.\n",
    "      model: Transformer model.\n",
    "      placeholder_idx: Index of the placeholder token.\n",
    "      max_truncate: Upper bound for truncation.\n",
    "      sample_prob: Probability of using model-generated token instead of ground truth.\n",
    "      gen_chars: Number of characters to generate instead of ground truth.\n",
    "\n",
    "    Returns:\n",
    "      Loss scalar.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = batch.shape\n",
    "    truncated_batch = []\n",
    "    targets = []\n",
    "    use_placeholder = torch.rand(1).item() < (1 / math.e)  # 1/e probability for forced placeholders\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Ensure truncation is never less than 3 chars\n",
    "        min_truncate = 3\n",
    "        max_truncate = model.seq_len\n",
    "        max_valid_truncate = min(max_truncate, model.seq_len - gen_chars - 1)\n",
    "\n",
    "        # Ensure we have a valid range\n",
    "        if max_valid_truncate < min_truncate:\n",
    "            truncate_len = min_truncate  # If max is too small, just use min\n",
    "        else:\n",
    "            truncate_len = random.randint(min_truncate, max_valid_truncate)\n",
    "\n",
    "        # Select truncated sequence\n",
    "        input_seq = batch[i, :truncate_len].clone()\n",
    "        target_seq = batch[i, truncate_len : min(truncate_len + gen_chars, seq_len)]  # Multiple target characters\n",
    "\n",
    "        # Scheduled sampling: replace last `gen_chars` tokens with model predictions\n",
    "        if random.random() < sample_prob and truncate_len > 1:\n",
    "            with torch.no_grad():\n",
    "                for _ in range(gen_chars):\n",
    "                    if len(input_seq) >= model.seq_len:  # Prevent overflow\n",
    "                        break\n",
    "                    model_output = model(input_seq.unsqueeze(0))  # (1, S, V)\n",
    "                    predicted_token = torch.argmax(model_output[:, -1, :], dim=-1)\n",
    "                    input_seq = torch.cat([input_seq, predicted_token.view(1)], dim=0)  # Append predicted token\n",
    "\n",
    "        # corruption\n",
    "        input_seq = corrupt_input_probability_based(\n",
    "            input_seq.unsqueeze(0), char_list, char_probs, distance_matrix, corruption_prob=1 / math.e\n",
    "        ).squeeze(0)\n",
    "\n",
    "        truncated_batch.append(input_seq[:model.seq_len])  # Ensure length limit\n",
    "        targets.append(target_seq[:model.seq_len])  # Ensure target sequence is also within bounds\n",
    "\n",
    "    # **Pad sequences to the max length in batch**\n",
    "    max_len = max(len(seq) for seq in truncated_batch)\n",
    "    max_len = min(max_len, model.seq_len)  # Ensure padding does not exceed max sequence length\n",
    "\n",
    "    truncated_batch = [F.pad(seq, (0, max_len - len(seq)), value=placeholder_idx) for seq in truncated_batch]\n",
    "    targets = [F.pad(seq, (0, max_len - len(seq)), value=placeholder_idx) for seq in targets]\n",
    "\n",
    "    truncated_batch = torch.stack(truncated_batch)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(truncated_batch)\n",
    "\n",
    "    # Compute custom loss (ignoring padded regions)\n",
    "    loss = modified_loss(outputs, targets, placeholder_idx)\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Increase sample probability over time\n",
    "    sample_prob = min(0.1 * (epoch + 1), 0.5)  # Linearly increases up to 50%\n",
    "    gen_chars = min(1 + epoch, 5)  # Gradually increase generated tokens per epoch\n",
    "\n",
    "    for step in range(100):  # Adjust as needed\n",
    "        x_batch, _ = get_batch(batch_size, seq_len)\n",
    "        x_batch = x_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "\n",
    "            loss = scheduled_sampling(x_batch, model, placeholder_idx, sample_prob, gen_chars,char_list, char_probs, distance_matrix)\n",
    "        main_loss = loss.detach()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += main_loss.item()\n",
    "        losses.append(main_loss.cpu())\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {main_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss/1000:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 4.8743\n",
      "Epoch 1, Step 1, Loss: 3.7034\n",
      "Epoch 1, Step 2, Loss: 5.7295\n",
      "Epoch 1, Step 3, Loss: 4.0383\n",
      "Epoch 1, Step 4, Loss: 5.1721\n",
      "Epoch 1, Step 5, Loss: 4.5756\n",
      "Epoch 1, Step 6, Loss: 3.8569\n",
      "Epoch 1, Step 7, Loss: 3.6343\n",
      "Epoch 1, Step 8, Loss: 3.4205\n",
      "Epoch 1, Step 9, Loss: 3.2741\n",
      "Epoch 1, Step 10, Loss: 3.0785\n",
      "Epoch 1, Step 11, Loss: 3.0053\n",
      "Epoch 1, Step 12, Loss: 2.8452\n",
      "Epoch 1, Step 13, Loss: 2.7870\n",
      "Epoch 1, Step 14, Loss: 2.6685\n",
      "Epoch 1, Step 15, Loss: 2.5480\n",
      "Epoch 1, Step 16, Loss: 2.3888\n",
      "Epoch 1, Step 17, Loss: 2.1010\n",
      "Epoch 1, Step 18, Loss: 1.9807\n",
      "Epoch 1, Step 19, Loss: 1.7672\n",
      "Epoch 1, Step 20, Loss: 1.4811\n",
      "Epoch 1, Step 21, Loss: 1.3887\n",
      "Epoch 1, Step 22, Loss: 1.3420\n",
      "Epoch 1, Step 23, Loss: 1.4083\n",
      "Epoch 1, Step 24, Loss: 1.3308\n",
      "Epoch 1, Step 25, Loss: 1.2092\n",
      "Epoch 1, Step 26, Loss: 1.0887\n",
      "Epoch 1, Step 27, Loss: 1.1184\n",
      "Epoch 1, Step 28, Loss: 1.0065\n",
      "Epoch 1, Step 29, Loss: 0.9957\n",
      "Epoch 1, Step 30, Loss: 0.9028\n",
      "Epoch 1, Step 31, Loss: 0.9390\n",
      "Epoch 1, Step 32, Loss: 0.8801\n",
      "Epoch 1, Step 33, Loss: 0.9235\n",
      "Epoch 1, Step 34, Loss: 0.7910\n",
      "Epoch 1, Step 35, Loss: 0.7983\n",
      "Epoch 1, Step 36, Loss: 0.7489\n",
      "Epoch 1, Step 37, Loss: 0.5880\n",
      "Epoch 1, Step 38, Loss: 0.6691\n",
      "Epoch 1, Step 39, Loss: 0.6150\n",
      "Epoch 1, Step 40, Loss: 0.5810\n",
      "Epoch 1, Step 41, Loss: 0.4919\n",
      "Epoch 1, Step 42, Loss: 0.4966\n",
      "Epoch 1, Step 43, Loss: 0.4267\n",
      "Epoch 1, Step 44, Loss: 0.4412\n",
      "Epoch 1, Step 45, Loss: 0.4062\n",
      "Epoch 1, Step 46, Loss: 0.4043\n",
      "Epoch 1, Step 47, Loss: 0.3901\n",
      "Epoch 1, Step 48, Loss: 0.3804\n",
      "Epoch 1, Step 49, Loss: 0.3385\n",
      "Epoch 1, Step 50, Loss: 0.3569\n",
      "Epoch 1, Step 51, Loss: 0.3249\n",
      "Epoch 1, Step 52, Loss: 0.3253\n",
      "Epoch 1, Step 53, Loss: 0.2707\n",
      "Epoch 1, Step 54, Loss: 0.3229\n",
      "Epoch 1, Step 55, Loss: 0.2590\n",
      "Epoch 1, Step 56, Loss: 0.2548\n",
      "Epoch 1, Step 57, Loss: 0.2267\n",
      "Epoch 1, Step 58, Loss: 0.2325\n",
      "Epoch 1, Step 59, Loss: 0.2504\n",
      "Epoch 1, Step 60, Loss: 0.2141\n",
      "Epoch 1, Step 61, Loss: 0.2544\n",
      "Epoch 1, Step 62, Loss: 0.1939\n",
      "Epoch 1, Step 63, Loss: 0.2461\n",
      "Epoch 1, Step 64, Loss: 0.1755\n",
      "Epoch 1, Step 65, Loss: 0.1986\n",
      "Epoch 1, Step 66, Loss: 0.1653\n",
      "Epoch 1, Step 67, Loss: 0.1848\n",
      "Epoch 1, Step 68, Loss: 0.1758\n",
      "Epoch 1, Step 69, Loss: 0.1734\n",
      "Epoch 1, Step 70, Loss: 0.1667\n",
      "Epoch 1, Step 71, Loss: 0.1580\n",
      "Epoch 1, Step 72, Loss: 0.1179\n",
      "Epoch 1, Step 73, Loss: 0.1354\n",
      "Epoch 1, Step 74, Loss: 0.1273\n",
      "Epoch 1, Step 75, Loss: 0.1309\n",
      "Epoch 1, Step 76, Loss: 0.1413\n",
      "Epoch 1, Step 77, Loss: 0.1164\n",
      "Epoch 1, Step 78, Loss: 0.1183\n",
      "Epoch 1, Step 79, Loss: 0.1159\n",
      "Epoch 1, Step 80, Loss: 0.1005\n",
      "Epoch 1, Step 81, Loss: 0.0965\n",
      "Epoch 1, Step 82, Loss: 0.0964\n",
      "Epoch 1, Step 83, Loss: 0.0840\n",
      "Epoch 1, Step 84, Loss: 0.0925\n",
      "Epoch 1, Step 85, Loss: 0.0757\n",
      "Epoch 1, Step 86, Loss: 0.0746\n",
      "Epoch 1, Step 87, Loss: 0.0760\n",
      "Epoch 1, Step 88, Loss: 0.0705\n",
      "Epoch 1, Step 89, Loss: 0.0692\n",
      "Epoch 1, Step 90, Loss: 0.0615\n",
      "Epoch 1, Step 91, Loss: 0.0656\n",
      "Epoch 1, Step 92, Loss: 0.0660\n",
      "Epoch 1, Step 93, Loss: 0.0578\n",
      "Epoch 1, Step 94, Loss: 0.0601\n",
      "Epoch 1, Step 95, Loss: 0.0581\n",
      "Epoch 1, Step 96, Loss: 0.0517\n",
      "Epoch 1, Step 97, Loss: 0.0484\n",
      "Epoch 1, Step 98, Loss: 0.0581\n",
      "Epoch 1, Step 99, Loss: 0.0469\n",
      "Epoch 1 Average Loss: 0.0996\n",
      "Epoch 2, Step 0, Loss: 0.0425\n",
      "Epoch 2, Step 1, Loss: 0.0404\n",
      "Epoch 2, Step 2, Loss: 0.0464\n",
      "Epoch 2, Step 3, Loss: 0.0463\n",
      "Epoch 2, Step 4, Loss: 0.0477\n",
      "Epoch 2, Step 5, Loss: 0.0387\n",
      "Epoch 2, Step 6, Loss: 0.0366\n",
      "Epoch 2, Step 7, Loss: 0.0494\n",
      "Epoch 2, Step 8, Loss: 0.0449\n",
      "Epoch 2, Step 9, Loss: 0.0437\n",
      "Epoch 2, Step 10, Loss: 0.0478\n",
      "Epoch 2, Step 11, Loss: 0.0340\n",
      "Epoch 2, Step 12, Loss: 0.0361\n",
      "Epoch 2, Step 13, Loss: 0.0339\n",
      "Epoch 2, Step 14, Loss: 0.0360\n",
      "Epoch 2, Step 15, Loss: 0.0286\n",
      "Epoch 2, Step 16, Loss: 0.0358\n",
      "Epoch 2, Step 17, Loss: 0.0329\n",
      "Epoch 2, Step 18, Loss: 0.0315\n",
      "Epoch 2, Step 19, Loss: 0.0298\n",
      "Epoch 2, Step 20, Loss: 0.0333\n",
      "Epoch 2, Step 21, Loss: 0.0347\n",
      "Epoch 2, Step 22, Loss: 0.0304\n",
      "Epoch 2, Step 23, Loss: 0.0332\n",
      "Epoch 2, Step 24, Loss: 0.0353\n",
      "Epoch 2, Step 25, Loss: 0.0315\n",
      "Epoch 2, Step 26, Loss: 0.0381\n",
      "Epoch 2, Step 27, Loss: 0.0275\n",
      "Epoch 2, Step 28, Loss: 0.0313\n",
      "Epoch 2, Step 29, Loss: 0.0262\n",
      "Epoch 2, Step 30, Loss: 0.0341\n",
      "Epoch 2, Step 31, Loss: 0.0309\n",
      "Epoch 2, Step 32, Loss: 0.0278\n",
      "Epoch 2, Step 33, Loss: 0.0246\n",
      "Epoch 2, Step 34, Loss: 0.0278\n",
      "Epoch 2, Step 35, Loss: 0.0297\n",
      "Epoch 2, Step 36, Loss: 0.0286\n",
      "Epoch 2, Step 37, Loss: 0.0289\n",
      "Epoch 2, Step 38, Loss: 0.0257\n",
      "Epoch 2, Step 39, Loss: 0.0209\n",
      "Epoch 2, Step 40, Loss: 0.0271\n",
      "Epoch 2, Step 41, Loss: 0.0327\n",
      "Epoch 2, Step 42, Loss: 0.0413\n",
      "Epoch 2, Step 43, Loss: 0.1560\n",
      "Epoch 2, Step 44, Loss: 0.0613\n",
      "Epoch 2, Step 45, Loss: 0.0964\n",
      "Epoch 2, Step 46, Loss: 0.1266\n",
      "Epoch 2, Step 47, Loss: 0.1078\n",
      "Epoch 2, Step 48, Loss: 0.0801\n",
      "Epoch 2, Step 49, Loss: 0.3683\n",
      "Epoch 2, Step 50, Loss: 0.4763\n",
      "Epoch 2, Step 51, Loss: 0.1947\n",
      "Epoch 2, Step 52, Loss: 0.2854\n",
      "Epoch 2, Step 53, Loss: 0.2912\n",
      "Epoch 2, Step 54, Loss: 0.2152\n",
      "Epoch 2, Step 55, Loss: 0.1900\n",
      "Epoch 2, Step 56, Loss: 0.1557\n",
      "Epoch 2, Step 57, Loss: 0.1809\n",
      "Epoch 2, Step 58, Loss: 0.1495\n",
      "Epoch 2, Step 59, Loss: 0.1438\n",
      "Epoch 2, Step 60, Loss: 0.1094\n",
      "Epoch 2, Step 61, Loss: 0.1192\n",
      "Epoch 2, Step 62, Loss: 0.0942\n",
      "Epoch 2, Step 63, Loss: 0.0767\n",
      "Epoch 2, Step 64, Loss: 0.0813\n",
      "Epoch 2, Step 65, Loss: 0.2786\n",
      "Epoch 2, Step 66, Loss: 0.3286\n",
      "Epoch 2, Step 67, Loss: 0.1514\n",
      "Epoch 2, Step 68, Loss: 0.1892\n",
      "Epoch 2, Step 69, Loss: 0.2608\n",
      "Epoch 2, Step 70, Loss: 0.1833\n",
      "Epoch 2, Step 71, Loss: 0.2549\n",
      "Epoch 2, Step 72, Loss: 0.2195\n",
      "Epoch 2, Step 73, Loss: 0.1154\n",
      "Epoch 2, Step 74, Loss: 0.0712\n",
      "Epoch 2, Step 75, Loss: 0.0885\n",
      "Epoch 2, Step 76, Loss: 0.0747\n",
      "Epoch 2, Step 77, Loss: 0.0526\n",
      "Epoch 2, Step 78, Loss: 0.0789\n",
      "Epoch 2, Step 79, Loss: 0.0489\n",
      "Epoch 2, Step 80, Loss: 0.0625\n",
      "Epoch 2, Step 81, Loss: 0.0779\n",
      "Epoch 2, Step 82, Loss: 0.0845\n",
      "Epoch 2, Step 83, Loss: 0.0870\n",
      "Epoch 2, Step 84, Loss: 0.0502\n",
      "Epoch 2, Step 85, Loss: 0.0510\n",
      "Epoch 2, Step 86, Loss: 0.0511\n",
      "Epoch 2, Step 87, Loss: 0.0686\n",
      "Epoch 2, Step 88, Loss: 0.0442\n",
      "Epoch 2, Step 89, Loss: 0.0631\n",
      "Epoch 2, Step 90, Loss: 0.0448\n",
      "Epoch 2, Step 91, Loss: 0.0522\n",
      "Epoch 2, Step 92, Loss: 0.0401\n",
      "Epoch 2, Step 93, Loss: 0.0500\n",
      "Epoch 2, Step 94, Loss: 0.0404\n",
      "Epoch 2, Step 95, Loss: 0.0337\n",
      "Epoch 2, Step 96, Loss: 0.0351\n",
      "Epoch 2, Step 97, Loss: 0.0378\n",
      "Epoch 2, Step 98, Loss: 0.0388\n",
      "Epoch 2, Step 99, Loss: 0.0340\n",
      "Epoch 2 Average Loss: 0.0087\n",
      "Epoch 3, Step 0, Loss: 0.0311\n",
      "Epoch 3, Step 1, Loss: 0.0291\n",
      "Epoch 3, Step 2, Loss: 0.0291\n",
      "Epoch 3, Step 3, Loss: 0.0236\n",
      "Epoch 3, Step 4, Loss: 0.0271\n",
      "Epoch 3, Step 5, Loss: 0.0280\n",
      "Epoch 3, Step 6, Loss: 0.0330\n",
      "Epoch 3, Step 7, Loss: 0.0298\n",
      "Epoch 3, Step 8, Loss: 0.0251\n",
      "Epoch 3, Step 9, Loss: 0.0272\n",
      "Epoch 3, Step 10, Loss: 0.0280\n",
      "Epoch 3, Step 11, Loss: 0.0210\n",
      "Epoch 3, Step 12, Loss: 0.0240\n",
      "Epoch 3, Step 13, Loss: 0.0207\n",
      "Epoch 3, Step 14, Loss: 0.0203\n",
      "Epoch 3, Step 15, Loss: 0.0204\n",
      "Epoch 3, Step 16, Loss: 0.0244\n",
      "Epoch 3, Step 17, Loss: 0.0198\n",
      "Epoch 3, Step 18, Loss: 0.0233\n",
      "Epoch 3, Step 19, Loss: 0.0237\n",
      "Epoch 3, Step 20, Loss: 0.0203\n",
      "Epoch 3, Step 21, Loss: 0.0194\n",
      "Epoch 3, Step 22, Loss: 0.0220\n",
      "Epoch 3, Step 23, Loss: 0.0158\n",
      "Epoch 3, Step 24, Loss: 0.0187\n",
      "Epoch 3, Step 25, Loss: 0.0219\n",
      "Epoch 3, Step 26, Loss: 0.0160\n",
      "Epoch 3, Step 27, Loss: 0.0198\n",
      "Epoch 3, Step 28, Loss: 0.0178\n",
      "Epoch 3, Step 29, Loss: 0.0205\n",
      "Epoch 3, Step 30, Loss: 0.0188\n",
      "Epoch 3, Step 31, Loss: 0.0159\n",
      "Epoch 3, Step 32, Loss: 0.0175\n",
      "Epoch 3, Step 33, Loss: 0.0169\n",
      "Epoch 3, Step 34, Loss: 0.0158\n",
      "Epoch 3, Step 35, Loss: 0.0154\n",
      "Epoch 3, Step 36, Loss: 0.0180\n",
      "Epoch 3, Step 37, Loss: 0.0140\n",
      "Epoch 3, Step 38, Loss: 0.0152\n",
      "Epoch 3, Step 39, Loss: 0.0128\n",
      "Epoch 3, Step 40, Loss: 0.0139\n",
      "Epoch 3, Step 41, Loss: 0.0158\n",
      "Epoch 3, Step 42, Loss: 0.0172\n",
      "Epoch 3, Step 43, Loss: 0.0187\n",
      "Epoch 3, Step 44, Loss: 0.0130\n",
      "Epoch 3, Step 45, Loss: 0.0163\n",
      "Epoch 3, Step 46, Loss: 0.0159\n",
      "Epoch 3, Step 47, Loss: 0.0145\n",
      "Epoch 3, Step 48, Loss: 0.0193\n",
      "Epoch 3, Step 49, Loss: 0.0151\n",
      "Epoch 3, Step 50, Loss: 0.0168\n",
      "Epoch 3, Step 51, Loss: 0.0163\n",
      "Epoch 3, Step 52, Loss: 0.0142\n",
      "Epoch 3, Step 53, Loss: 0.0167\n",
      "Epoch 3, Step 54, Loss: 0.0152\n",
      "Epoch 3, Step 55, Loss: 0.0148\n",
      "Epoch 3, Step 56, Loss: 0.0129\n",
      "Epoch 3, Step 57, Loss: 0.0134\n",
      "Epoch 3, Step 58, Loss: 0.0178\n",
      "Epoch 3, Step 59, Loss: 0.0176\n",
      "Epoch 3, Step 60, Loss: 0.0173\n",
      "Epoch 3, Step 61, Loss: 0.0159\n",
      "Epoch 3, Step 62, Loss: 0.0161\n",
      "Epoch 3, Step 63, Loss: 0.0159\n",
      "Epoch 3, Step 64, Loss: 0.0179\n",
      "Epoch 3, Step 65, Loss: 0.0208\n",
      "Epoch 3, Step 66, Loss: 0.0156\n",
      "Epoch 3, Step 67, Loss: 0.0130\n",
      "Epoch 3, Step 68, Loss: 0.0182\n",
      "Epoch 3, Step 69, Loss: 0.0125\n",
      "Epoch 3, Step 70, Loss: 0.0125\n",
      "Epoch 3, Step 71, Loss: 0.0142\n",
      "Epoch 3, Step 72, Loss: 0.0165\n",
      "Epoch 3, Step 73, Loss: 0.0136\n",
      "Epoch 3, Step 74, Loss: 0.0138\n",
      "Epoch 3, Step 75, Loss: 0.0165\n",
      "Epoch 3, Step 76, Loss: 0.0177\n",
      "Epoch 3, Step 77, Loss: 0.0163\n",
      "Epoch 3, Step 78, Loss: 0.0142\n",
      "Epoch 3, Step 79, Loss: 0.0128\n",
      "Epoch 3, Step 80, Loss: 0.0132\n",
      "Epoch 3, Step 81, Loss: 0.0156\n",
      "Epoch 3, Step 82, Loss: 0.0135\n",
      "Epoch 3, Step 83, Loss: 0.0154\n",
      "Epoch 3, Step 84, Loss: 0.0152\n",
      "Epoch 3, Step 85, Loss: 0.0156\n",
      "Epoch 3, Step 86, Loss: 0.0139\n",
      "Epoch 3, Step 87, Loss: 0.0114\n",
      "Epoch 3, Step 88, Loss: 0.0135\n",
      "Epoch 3, Step 89, Loss: 0.0133\n",
      "Epoch 3, Step 90, Loss: 0.0149\n",
      "Epoch 3, Step 91, Loss: 0.0154\n",
      "Epoch 3, Step 92, Loss: 0.0134\n",
      "Epoch 3, Step 93, Loss: 0.0125\n",
      "Epoch 3, Step 94, Loss: 0.0139\n",
      "Epoch 3, Step 95, Loss: 0.0118\n",
      "Epoch 3, Step 96, Loss: 0.0132\n",
      "Epoch 3, Step 97, Loss: 0.0134\n",
      "Epoch 3, Step 98, Loss: 0.0106\n",
      "Epoch 3, Step 99, Loss: 0.0164\n",
      "Epoch 3 Average Loss: 0.0018\n",
      "Epoch 4, Step 0, Loss: 0.0129\n",
      "Epoch 4, Step 1, Loss: 0.0158\n",
      "Epoch 4, Step 2, Loss: 0.0183\n",
      "Epoch 4, Step 3, Loss: 0.0127\n",
      "Epoch 4, Step 4, Loss: 0.0141\n",
      "Epoch 4, Step 5, Loss: 0.0158\n",
      "Epoch 4, Step 6, Loss: 0.0124\n",
      "Epoch 4, Step 7, Loss: 0.0129\n",
      "Epoch 4, Step 8, Loss: 0.0134\n",
      "Epoch 4, Step 9, Loss: 0.0145\n",
      "Epoch 4, Step 10, Loss: 0.0122\n",
      "Epoch 4, Step 11, Loss: 0.0151\n",
      "Epoch 4, Step 12, Loss: 0.0154\n",
      "Epoch 4, Step 13, Loss: 0.0130\n",
      "Epoch 4, Step 14, Loss: 0.0130\n",
      "Epoch 4, Step 15, Loss: 0.0147\n",
      "Epoch 4, Step 16, Loss: 0.0159\n",
      "Epoch 4, Step 17, Loss: 0.0149\n",
      "Epoch 4, Step 18, Loss: 0.0123\n",
      "Epoch 4, Step 19, Loss: 0.0150\n",
      "Epoch 4, Step 20, Loss: 0.0144\n",
      "Epoch 4, Step 21, Loss: 0.0148\n",
      "Epoch 4, Step 22, Loss: 0.0133\n",
      "Epoch 4, Step 23, Loss: 0.0101\n",
      "Epoch 4, Step 24, Loss: 0.0127\n",
      "Epoch 4, Step 25, Loss: 0.0182\n",
      "Epoch 4, Step 26, Loss: 0.0129\n",
      "Epoch 4, Step 27, Loss: 0.0127\n",
      "Epoch 4, Step 28, Loss: 0.0117\n",
      "Epoch 4, Step 29, Loss: 0.0146\n",
      "Epoch 4, Step 30, Loss: 0.0175\n",
      "Epoch 4, Step 31, Loss: 0.0127\n",
      "Epoch 4, Step 32, Loss: 0.0175\n",
      "Epoch 4, Step 33, Loss: 0.0125\n",
      "Epoch 4, Step 34, Loss: 0.0130\n",
      "Epoch 4, Step 35, Loss: 0.0130\n",
      "Epoch 4, Step 36, Loss: 0.0133\n",
      "Epoch 4, Step 37, Loss: 0.0137\n",
      "Epoch 4, Step 38, Loss: 0.0116\n",
      "Epoch 4, Step 39, Loss: 0.0114\n",
      "Epoch 4, Step 40, Loss: 0.0127\n",
      "Epoch 4, Step 41, Loss: 0.0112\n",
      "Epoch 4, Step 42, Loss: 0.0162\n",
      "Epoch 4, Step 43, Loss: 0.0123\n",
      "Epoch 4, Step 44, Loss: 0.0157\n",
      "Epoch 4, Step 45, Loss: 0.0131\n",
      "Epoch 4, Step 46, Loss: 0.0131\n",
      "Epoch 4, Step 47, Loss: 0.0127\n",
      "Epoch 4, Step 48, Loss: 0.0101\n",
      "Epoch 4, Step 49, Loss: 0.0145\n",
      "Epoch 4, Step 50, Loss: 0.0116\n",
      "Epoch 4, Step 51, Loss: 0.0118\n",
      "Epoch 4, Step 52, Loss: 0.0128\n",
      "Epoch 4, Step 53, Loss: 0.0148\n",
      "Epoch 4, Step 54, Loss: 0.0145\n",
      "Epoch 4, Step 55, Loss: 0.0136\n",
      "Epoch 4, Step 56, Loss: 0.0143\n",
      "Epoch 4, Step 57, Loss: 0.0126\n",
      "Epoch 4, Step 58, Loss: 0.0170\n",
      "Epoch 4, Step 59, Loss: 0.0127\n",
      "Epoch 4, Step 60, Loss: 0.0109\n",
      "Epoch 4, Step 61, Loss: 0.0158\n",
      "Epoch 4, Step 62, Loss: 0.0130\n",
      "Epoch 4, Step 63, Loss: 0.0145\n",
      "Epoch 4, Step 64, Loss: 0.0163\n",
      "Epoch 4, Step 65, Loss: 0.0154\n",
      "Epoch 4, Step 66, Loss: 0.0140\n",
      "Epoch 4, Step 67, Loss: 0.0126\n",
      "Epoch 4, Step 68, Loss: 0.0114\n",
      "Epoch 4, Step 69, Loss: 0.0122\n",
      "Epoch 4, Step 70, Loss: 0.0129\n",
      "Epoch 4, Step 71, Loss: 0.0132\n",
      "Epoch 4, Step 72, Loss: 0.0134\n",
      "Epoch 4, Step 73, Loss: 0.0155\n",
      "Epoch 4, Step 74, Loss: 0.0120\n",
      "Epoch 4, Step 75, Loss: 0.0125\n",
      "Epoch 4, Step 76, Loss: 0.0148\n",
      "Epoch 4, Step 77, Loss: 0.0128\n",
      "Epoch 4, Step 78, Loss: 0.0120\n",
      "Epoch 4, Step 79, Loss: 0.0106\n",
      "Epoch 4, Step 80, Loss: 0.0120\n",
      "Epoch 4, Step 81, Loss: 0.0109\n",
      "Epoch 4, Step 82, Loss: 0.0136\n",
      "Epoch 4, Step 83, Loss: 0.0122\n",
      "Epoch 4, Step 84, Loss: 0.0123\n",
      "Epoch 4, Step 85, Loss: 0.0132\n",
      "Epoch 4, Step 86, Loss: 0.0156\n",
      "Epoch 4, Step 87, Loss: 0.0114\n",
      "Epoch 4, Step 88, Loss: 0.0151\n",
      "Epoch 4, Step 89, Loss: 0.0174\n",
      "Epoch 4, Step 90, Loss: 0.0120\n",
      "Epoch 4, Step 91, Loss: 0.0155\n",
      "Epoch 4, Step 92, Loss: 0.0104\n",
      "Epoch 4, Step 93, Loss: 0.0147\n",
      "Epoch 4, Step 94, Loss: 0.0117\n",
      "Epoch 4, Step 95, Loss: 0.0130\n",
      "Epoch 4, Step 96, Loss: 0.0139\n",
      "Epoch 4, Step 97, Loss: 0.0118\n",
      "Epoch 4, Step 98, Loss: 0.0137\n",
      "Epoch 4, Step 99, Loss: 0.0150\n",
      "Epoch 4 Average Loss: 0.0014\n",
      "Epoch 5, Step 0, Loss: 0.0126\n",
      "Epoch 5, Step 1, Loss: 0.0165\n",
      "Epoch 5, Step 2, Loss: 0.0132\n",
      "Epoch 5, Step 3, Loss: 0.0177\n",
      "Epoch 5, Step 4, Loss: 0.0134\n",
      "Epoch 5, Step 5, Loss: 0.0157\n",
      "Epoch 5, Step 6, Loss: 0.0142\n",
      "Epoch 5, Step 7, Loss: 0.0217\n",
      "Epoch 5, Step 8, Loss: 0.0183\n",
      "Epoch 5, Step 9, Loss: 0.0148\n",
      "Epoch 5, Step 10, Loss: 0.0283\n",
      "Epoch 5, Step 11, Loss: 0.0875\n",
      "Epoch 5, Step 12, Loss: 0.0462\n",
      "Epoch 5, Step 13, Loss: 0.1802\n",
      "Epoch 5, Step 14, Loss: 0.1133\n",
      "Epoch 5, Step 15, Loss: 0.2345\n",
      "Epoch 5, Step 16, Loss: 0.2892\n",
      "Epoch 5, Step 17, Loss: 0.2045\n",
      "Epoch 5, Step 18, Loss: 0.5266\n",
      "Epoch 5, Step 19, Loss: 0.3173\n",
      "Epoch 5, Step 20, Loss: 0.1411\n",
      "Epoch 5, Step 21, Loss: 0.6847\n",
      "Epoch 5, Step 22, Loss: 0.4560\n",
      "Epoch 5, Step 23, Loss: 0.3611\n",
      "Epoch 5, Step 24, Loss: 0.2484\n",
      "Epoch 5, Step 25, Loss: 0.0981\n",
      "Epoch 5, Step 26, Loss: 0.0946\n",
      "Epoch 5, Step 27, Loss: 0.1362\n",
      "Epoch 5, Step 28, Loss: 0.1239\n",
      "Epoch 5, Step 29, Loss: 0.0865\n",
      "Epoch 5, Step 30, Loss: 0.1317\n",
      "Epoch 5, Step 31, Loss: 0.1822\n",
      "Epoch 5, Step 32, Loss: 0.1432\n",
      "Epoch 5, Step 33, Loss: 0.1180\n",
      "Epoch 5, Step 34, Loss: 0.1366\n",
      "Epoch 5, Step 35, Loss: 0.1125\n",
      "Epoch 5, Step 36, Loss: 0.0547\n",
      "Epoch 5, Step 37, Loss: 0.1990\n",
      "Epoch 5, Step 38, Loss: 0.1288\n",
      "Epoch 5, Step 39, Loss: 0.1546\n",
      "Epoch 5, Step 40, Loss: 0.1265\n",
      "Epoch 5, Step 41, Loss: 0.1755\n",
      "Epoch 5, Step 42, Loss: 0.1868\n",
      "Epoch 5, Step 43, Loss: 0.1379\n",
      "Epoch 5, Step 44, Loss: 0.0812\n",
      "Epoch 5, Step 45, Loss: 0.1221\n",
      "Epoch 5, Step 46, Loss: 0.0964\n",
      "Epoch 5, Step 47, Loss: 0.0536\n",
      "Epoch 5, Step 48, Loss: 0.0939\n",
      "Epoch 5, Step 49, Loss: 0.0729\n",
      "Epoch 5, Step 50, Loss: 0.0773\n",
      "Epoch 5, Step 51, Loss: 0.1107\n",
      "Epoch 5, Step 52, Loss: 0.0708\n",
      "Epoch 5, Step 53, Loss: 0.1303\n",
      "Epoch 5, Step 54, Loss: 0.0791\n",
      "Epoch 5, Step 55, Loss: 0.0648\n",
      "Epoch 5, Step 56, Loss: 0.0570\n",
      "Epoch 5, Step 57, Loss: 0.0495\n",
      "Epoch 5, Step 58, Loss: 0.0461\n",
      "Epoch 5, Step 59, Loss: 0.0457\n",
      "Epoch 5, Step 60, Loss: 0.0441\n",
      "Epoch 5, Step 61, Loss: 0.0354\n",
      "Epoch 5, Step 62, Loss: 0.0433\n",
      "Epoch 5, Step 63, Loss: 0.0705\n",
      "Epoch 5, Step 64, Loss: 0.0358\n",
      "Epoch 5, Step 65, Loss: 0.0533\n",
      "Epoch 5, Step 66, Loss: 0.0424\n",
      "Epoch 5, Step 67, Loss: 0.0398\n",
      "Epoch 5, Step 68, Loss: 0.0542\n",
      "Epoch 5, Step 69, Loss: 0.0630\n",
      "Epoch 5, Step 70, Loss: 0.0898\n",
      "Epoch 5, Step 71, Loss: 0.0534\n",
      "Epoch 5, Step 72, Loss: 0.1576\n",
      "Epoch 5, Step 73, Loss: 0.0471\n",
      "Epoch 5, Step 74, Loss: 0.1062\n",
      "Epoch 5, Step 75, Loss: 0.0414\n",
      "Epoch 5, Step 76, Loss: 0.2063\n",
      "Epoch 5, Step 77, Loss: 0.1030\n",
      "Epoch 5, Step 78, Loss: 0.1387\n",
      "Epoch 5, Step 79, Loss: 0.2132\n",
      "Epoch 5, Step 80, Loss: 0.1113\n",
      "Epoch 5, Step 81, Loss: 0.1154\n",
      "Epoch 5, Step 82, Loss: 0.1033\n",
      "Epoch 5, Step 83, Loss: 0.0869\n",
      "Epoch 5, Step 84, Loss: 0.0516\n",
      "Epoch 5, Step 85, Loss: 0.0761\n",
      "Epoch 5, Step 86, Loss: 0.0782\n",
      "Epoch 5, Step 87, Loss: 0.0936\n",
      "Epoch 5, Step 88, Loss: 0.0410\n",
      "Epoch 5, Step 89, Loss: 0.0556\n",
      "Epoch 5, Step 90, Loss: 0.0558\n",
      "Epoch 5, Step 91, Loss: 0.0406\n",
      "Epoch 5, Step 92, Loss: 0.0474\n",
      "Epoch 5, Step 93, Loss: 0.0350\n",
      "Epoch 5, Step 94, Loss: 0.0321\n",
      "Epoch 5, Step 95, Loss: 0.0481\n",
      "Epoch 5, Step 96, Loss: 0.0308\n",
      "Epoch 5, Step 97, Loss: 0.0292\n",
      "Epoch 5, Step 98, Loss: 0.0243\n",
      "Epoch 5, Step 99, Loss: 0.0246\n",
      "Epoch 5 Average Loss: 0.0107\n",
      "Epoch 6, Step 0, Loss: 0.0290\n",
      "Epoch 6, Step 1, Loss: 0.0238\n",
      "Epoch 6, Step 2, Loss: 0.0233\n",
      "Epoch 6, Step 3, Loss: 0.0258\n",
      "Epoch 6, Step 4, Loss: 0.0253\n",
      "Epoch 6, Step 5, Loss: 0.0222\n",
      "Epoch 6, Step 6, Loss: 0.0232\n",
      "Epoch 6, Step 7, Loss: 0.0191\n",
      "Epoch 6, Step 8, Loss: 0.0264\n",
      "Epoch 6, Step 9, Loss: 0.0225\n",
      "Epoch 6, Step 10, Loss: 0.0217\n",
      "Epoch 6, Step 11, Loss: 0.0185\n",
      "Epoch 6, Step 12, Loss: 0.0200\n",
      "Epoch 6, Step 13, Loss: 0.0168\n",
      "Epoch 6, Step 14, Loss: 0.0160\n",
      "Epoch 6, Step 15, Loss: 0.0154\n",
      "Epoch 6, Step 16, Loss: 0.0198\n",
      "Epoch 6, Step 17, Loss: 0.0165\n",
      "Epoch 6, Step 18, Loss: 0.0192\n",
      "Epoch 6, Step 19, Loss: 0.0223\n",
      "Epoch 6, Step 20, Loss: 0.0158\n",
      "Epoch 6, Step 21, Loss: 0.0137\n",
      "Epoch 6, Step 22, Loss: 0.0185\n",
      "Epoch 6, Step 23, Loss: 0.0152\n",
      "Epoch 6, Step 24, Loss: 0.0189\n",
      "Epoch 6, Step 25, Loss: 0.0135\n",
      "Epoch 6, Step 26, Loss: 0.0121\n",
      "Epoch 6, Step 27, Loss: 0.0191\n",
      "Epoch 6, Step 28, Loss: 0.0138\n",
      "Epoch 6, Step 29, Loss: 0.0155\n",
      "Epoch 6, Step 30, Loss: 0.0150\n",
      "Epoch 6, Step 31, Loss: 0.0149\n",
      "Epoch 6, Step 32, Loss: 0.0130\n",
      "Epoch 6, Step 33, Loss: 0.0160\n",
      "Epoch 6, Step 34, Loss: 0.0111\n",
      "Epoch 6, Step 35, Loss: 0.0129\n",
      "Epoch 6, Step 36, Loss: 0.0105\n",
      "Epoch 6, Step 37, Loss: 0.0130\n",
      "Epoch 6, Step 38, Loss: 0.0158\n",
      "Epoch 6, Step 39, Loss: 0.0140\n",
      "Epoch 6, Step 40, Loss: 0.0134\n",
      "Epoch 6, Step 41, Loss: 0.0185\n",
      "Epoch 6, Step 42, Loss: 0.0137\n",
      "Epoch 6, Step 43, Loss: 0.0141\n",
      "Epoch 6, Step 44, Loss: 0.0149\n",
      "Epoch 6, Step 45, Loss: 0.0132\n",
      "Epoch 6, Step 46, Loss: 0.0144\n",
      "Epoch 6, Step 47, Loss: 0.0100\n",
      "Epoch 6, Step 48, Loss: 0.0144\n",
      "Epoch 6, Step 49, Loss: 0.0143\n",
      "Epoch 6, Step 50, Loss: 0.0144\n",
      "Epoch 6, Step 51, Loss: 0.0119\n",
      "Epoch 6, Step 52, Loss: 0.0141\n",
      "Epoch 6, Step 53, Loss: 0.0147\n",
      "Epoch 6, Step 54, Loss: 0.0144\n",
      "Epoch 6, Step 55, Loss: 0.0132\n",
      "Epoch 6, Step 56, Loss: 0.0137\n",
      "Epoch 6, Step 57, Loss: 0.0097\n",
      "Epoch 6, Step 58, Loss: 0.0128\n",
      "Epoch 6, Step 59, Loss: 0.0152\n",
      "Epoch 6, Step 60, Loss: 0.0184\n",
      "Epoch 6, Step 61, Loss: 0.0140\n",
      "Epoch 6, Step 62, Loss: 0.0125\n",
      "Epoch 6, Step 63, Loss: 0.0143\n",
      "Epoch 6, Step 64, Loss: 0.0118\n",
      "Epoch 6, Step 65, Loss: 0.0143\n",
      "Epoch 6, Step 66, Loss: 0.0162\n",
      "Epoch 6, Step 67, Loss: 0.0117\n",
      "Epoch 6, Step 68, Loss: 0.0153\n",
      "Epoch 6, Step 69, Loss: 0.0141\n",
      "Epoch 6, Step 70, Loss: 0.0126\n",
      "Epoch 6, Step 71, Loss: 0.0122\n",
      "Epoch 6, Step 72, Loss: 0.0136\n",
      "Epoch 6, Step 73, Loss: 0.0112\n",
      "Epoch 6, Step 74, Loss: 0.0137\n",
      "Epoch 6, Step 75, Loss: 0.0148\n",
      "Epoch 6, Step 76, Loss: 0.0130\n",
      "Epoch 6, Step 77, Loss: 0.0127\n",
      "Epoch 6, Step 78, Loss: 0.0138\n",
      "Epoch 6, Step 79, Loss: 0.0122\n",
      "Epoch 6, Step 80, Loss: 0.0144\n",
      "Epoch 6, Step 81, Loss: 0.0120\n",
      "Epoch 6, Step 82, Loss: 0.0149\n",
      "Epoch 6, Step 83, Loss: 0.0119\n",
      "Epoch 6, Step 84, Loss: 0.0137\n",
      "Epoch 6, Step 85, Loss: 0.0153\n",
      "Epoch 6, Step 86, Loss: 0.0130\n",
      "Epoch 6, Step 87, Loss: 0.0139\n",
      "Epoch 6, Step 88, Loss: 0.0114\n",
      "Epoch 6, Step 89, Loss: 0.0126\n",
      "Epoch 6, Step 90, Loss: 0.0138\n",
      "Epoch 6, Step 91, Loss: 0.0125\n",
      "Epoch 6, Step 92, Loss: 0.0119\n",
      "Epoch 6, Step 93, Loss: 0.0120\n",
      "Epoch 6, Step 94, Loss: 0.0168\n",
      "Epoch 6, Step 95, Loss: 0.0109\n",
      "Epoch 6, Step 96, Loss: 0.0116\n",
      "Epoch 6, Step 97, Loss: 0.0118\n",
      "Epoch 6, Step 98, Loss: 0.0114\n",
      "Epoch 6, Step 99, Loss: 0.0134\n",
      "Epoch 6 Average Loss: 0.0015\n",
      "Epoch 7, Step 0, Loss: 0.0146\n",
      "Epoch 7, Step 1, Loss: 0.0144\n",
      "Epoch 7, Step 2, Loss: 0.0114\n",
      "Epoch 7, Step 3, Loss: 0.0100\n",
      "Epoch 7, Step 4, Loss: 0.0110\n",
      "Epoch 7, Step 5, Loss: 0.0111\n",
      "Epoch 7, Step 6, Loss: 0.0116\n",
      "Epoch 7, Step 7, Loss: 0.0127\n",
      "Epoch 7, Step 8, Loss: 0.0094\n",
      "Epoch 7, Step 9, Loss: 0.0132\n",
      "Epoch 7, Step 10, Loss: 0.0144\n",
      "Epoch 7, Step 11, Loss: 0.0108\n",
      "Epoch 7, Step 12, Loss: 0.0128\n",
      "Epoch 7, Step 13, Loss: 0.0144\n",
      "Epoch 7, Step 14, Loss: 0.0148\n",
      "Epoch 7, Step 15, Loss: 0.0111\n",
      "Epoch 7, Step 16, Loss: 0.0124\n",
      "Epoch 7, Step 17, Loss: 0.0139\n",
      "Epoch 7, Step 18, Loss: 0.0129\n",
      "Epoch 7, Step 19, Loss: 0.0113\n",
      "Epoch 7, Step 20, Loss: 0.0102\n",
      "Epoch 7, Step 21, Loss: 0.0130\n",
      "Epoch 7, Step 22, Loss: 0.0137\n",
      "Epoch 7, Step 23, Loss: 0.0127\n",
      "Epoch 7, Step 24, Loss: 0.0116\n",
      "Epoch 7, Step 25, Loss: 0.0135\n",
      "Epoch 7, Step 26, Loss: 0.0123\n",
      "Epoch 7, Step 27, Loss: 0.0125\n",
      "Epoch 7, Step 28, Loss: 0.0104\n",
      "Epoch 7, Step 29, Loss: 0.0142\n",
      "Epoch 7, Step 30, Loss: 0.0123\n",
      "Epoch 7, Step 31, Loss: 0.0116\n",
      "Epoch 7, Step 32, Loss: 0.0131\n",
      "Epoch 7, Step 33, Loss: 0.0126\n",
      "Epoch 7, Step 34, Loss: 0.0139\n",
      "Epoch 7, Step 35, Loss: 0.0126\n",
      "Epoch 7, Step 36, Loss: 0.0113\n",
      "Epoch 7, Step 37, Loss: 0.0108\n",
      "Epoch 7, Step 38, Loss: 0.0124\n",
      "Epoch 7, Step 39, Loss: 0.0113\n",
      "Epoch 7, Step 40, Loss: 0.0114\n",
      "Epoch 7, Step 41, Loss: 0.0120\n",
      "Epoch 7, Step 42, Loss: 0.0142\n",
      "Epoch 7, Step 43, Loss: 0.0114\n",
      "Epoch 7, Step 44, Loss: 0.0155\n",
      "Epoch 7, Step 45, Loss: 0.0132\n",
      "Epoch 7, Step 46, Loss: 0.0098\n",
      "Epoch 7, Step 47, Loss: 0.0121\n",
      "Epoch 7, Step 48, Loss: 0.0104\n",
      "Epoch 7, Step 49, Loss: 0.0140\n",
      "Epoch 7, Step 50, Loss: 0.0121\n",
      "Epoch 7, Step 51, Loss: 0.0101\n",
      "Epoch 7, Step 52, Loss: 0.0103\n",
      "Epoch 7, Step 53, Loss: 0.0121\n",
      "Epoch 7, Step 54, Loss: 0.0132\n",
      "Epoch 7, Step 55, Loss: 0.0142\n",
      "Epoch 7, Step 56, Loss: 0.0104\n",
      "Epoch 7, Step 57, Loss: 0.0171\n",
      "Epoch 7, Step 58, Loss: 0.0145\n",
      "Epoch 7, Step 59, Loss: 0.0104\n",
      "Epoch 7, Step 60, Loss: 0.0151\n",
      "Epoch 7, Step 61, Loss: 0.0156\n",
      "Epoch 7, Step 62, Loss: 0.0109\n",
      "Epoch 7, Step 63, Loss: 0.0141\n",
      "Epoch 7, Step 64, Loss: 0.0116\n",
      "Epoch 7, Step 65, Loss: 0.0125\n",
      "Epoch 7, Step 66, Loss: 0.0112\n",
      "Epoch 7, Step 67, Loss: 0.0145\n",
      "Epoch 7, Step 68, Loss: 0.0136\n",
      "Epoch 7, Step 69, Loss: 0.0115\n",
      "Epoch 7, Step 70, Loss: 0.0132\n",
      "Epoch 7, Step 71, Loss: 0.0117\n",
      "Epoch 7, Step 72, Loss: 0.0100\n",
      "Epoch 7, Step 73, Loss: 0.0141\n",
      "Epoch 7, Step 74, Loss: 0.0108\n",
      "Epoch 7, Step 75, Loss: 0.0132\n",
      "Epoch 7, Step 76, Loss: 0.0095\n",
      "Epoch 7, Step 77, Loss: 0.0130\n",
      "Epoch 7, Step 78, Loss: 0.0138\n",
      "Epoch 7, Step 79, Loss: 0.0135\n",
      "Epoch 7, Step 80, Loss: 0.0129\n",
      "Epoch 7, Step 81, Loss: 0.0101\n",
      "Epoch 7, Step 82, Loss: 0.0124\n",
      "Epoch 7, Step 83, Loss: 0.0118\n",
      "Epoch 7, Step 84, Loss: 0.0111\n",
      "Epoch 7, Step 85, Loss: 0.0120\n",
      "Epoch 7, Step 86, Loss: 0.0150\n",
      "Epoch 7, Step 87, Loss: 0.0136\n",
      "Epoch 7, Step 88, Loss: 0.0108\n",
      "Epoch 7, Step 89, Loss: 0.0111\n",
      "Epoch 7, Step 90, Loss: 0.0090\n",
      "Epoch 7, Step 91, Loss: 0.0119\n",
      "Epoch 7, Step 92, Loss: 0.0128\n",
      "Epoch 7, Step 93, Loss: 0.0113\n",
      "Epoch 7, Step 94, Loss: 0.0117\n",
      "Epoch 7, Step 95, Loss: 0.0113\n",
      "Epoch 7, Step 96, Loss: 0.0119\n",
      "Epoch 7, Step 97, Loss: 0.0097\n",
      "Epoch 7, Step 98, Loss: 0.0139\n",
      "Epoch 7, Step 99, Loss: 0.0135\n",
      "Epoch 7 Average Loss: 0.0012\n",
      "Epoch 8, Step 0, Loss: 0.0107\n",
      "Epoch 8, Step 1, Loss: 0.0126\n",
      "Epoch 8, Step 2, Loss: 0.0107\n",
      "Epoch 8, Step 3, Loss: 0.0109\n",
      "Epoch 8, Step 4, Loss: 0.0103\n",
      "Epoch 8, Step 5, Loss: 0.0093\n",
      "Epoch 8, Step 6, Loss: 0.0110\n",
      "Epoch 8, Step 7, Loss: 0.0133\n",
      "Epoch 8, Step 8, Loss: 0.0123\n",
      "Epoch 8, Step 9, Loss: 0.0137\n",
      "Epoch 8, Step 10, Loss: 0.0136\n",
      "Epoch 8, Step 11, Loss: 0.0136\n",
      "Epoch 8, Step 12, Loss: 0.0119\n",
      "Epoch 8, Step 13, Loss: 0.0119\n",
      "Epoch 8, Step 14, Loss: 0.0116\n",
      "Epoch 8, Step 15, Loss: 0.0132\n",
      "Epoch 8, Step 16, Loss: 0.0098\n",
      "Epoch 8, Step 17, Loss: 0.0127\n",
      "Epoch 8, Step 18, Loss: 0.0124\n",
      "Epoch 8, Step 19, Loss: 0.0122\n",
      "Epoch 8, Step 20, Loss: 0.0145\n",
      "Epoch 8, Step 21, Loss: 0.0119\n",
      "Epoch 8, Step 22, Loss: 0.0121\n",
      "Epoch 8, Step 23, Loss: 0.0133\n",
      "Epoch 8, Step 24, Loss: 0.0113\n",
      "Epoch 8, Step 25, Loss: 0.0110\n",
      "Epoch 8, Step 26, Loss: 0.0106\n",
      "Epoch 8, Step 27, Loss: 0.0142\n",
      "Epoch 8, Step 28, Loss: 0.0126\n",
      "Epoch 8, Step 29, Loss: 0.0156\n",
      "Epoch 8, Step 30, Loss: 0.0117\n",
      "Epoch 8, Step 31, Loss: 0.0113\n",
      "Epoch 8, Step 32, Loss: 0.0110\n",
      "Epoch 8, Step 33, Loss: 0.0133\n",
      "Epoch 8, Step 34, Loss: 0.0108\n",
      "Epoch 8, Step 35, Loss: 0.0124\n",
      "Epoch 8, Step 36, Loss: 0.0107\n",
      "Epoch 8, Step 37, Loss: 0.0106\n",
      "Epoch 8, Step 38, Loss: 0.0123\n",
      "Epoch 8, Step 39, Loss: 0.0122\n",
      "Epoch 8, Step 40, Loss: 0.0104\n",
      "Epoch 8, Step 41, Loss: 0.0093\n",
      "Epoch 8, Step 42, Loss: 0.0101\n",
      "Epoch 8, Step 43, Loss: 0.0112\n",
      "Epoch 8, Step 44, Loss: 0.0114\n",
      "Epoch 8, Step 45, Loss: 0.0113\n",
      "Epoch 8, Step 46, Loss: 0.0101\n",
      "Epoch 8, Step 47, Loss: 0.0112\n",
      "Epoch 8, Step 48, Loss: 0.0136\n",
      "Epoch 8, Step 49, Loss: 0.0118\n",
      "Epoch 8, Step 50, Loss: 0.0128\n",
      "Epoch 8, Step 51, Loss: 0.0115\n",
      "Epoch 8, Step 52, Loss: 0.0116\n",
      "Epoch 8, Step 53, Loss: 0.0107\n",
      "Epoch 8, Step 54, Loss: 0.0113\n",
      "Epoch 8, Step 55, Loss: 0.0126\n",
      "Epoch 8, Step 56, Loss: 0.0088\n",
      "Epoch 8, Step 57, Loss: 0.0107\n",
      "Epoch 8, Step 58, Loss: 0.0103\n",
      "Epoch 8, Step 59, Loss: 0.0142\n",
      "Epoch 8, Step 60, Loss: 0.0103\n",
      "Epoch 8, Step 61, Loss: 0.0113\n",
      "Epoch 8, Step 62, Loss: 0.0093\n",
      "Epoch 8, Step 63, Loss: 0.0121\n",
      "Epoch 8, Step 64, Loss: 0.0097\n",
      "Epoch 8, Step 65, Loss: 0.0120\n",
      "Epoch 8, Step 66, Loss: 0.0118\n",
      "Epoch 8, Step 67, Loss: 0.0115\n",
      "Epoch 8, Step 68, Loss: 0.0124\n",
      "Epoch 8, Step 69, Loss: 0.0119\n",
      "Epoch 8, Step 70, Loss: 0.0141\n",
      "Epoch 8, Step 71, Loss: 0.0129\n",
      "Epoch 8, Step 72, Loss: 0.0107\n",
      "Epoch 8, Step 73, Loss: 0.0108\n",
      "Epoch 8, Step 74, Loss: 0.0121\n",
      "Epoch 8, Step 75, Loss: 0.0122\n",
      "Epoch 8, Step 76, Loss: 0.0112\n",
      "Epoch 8, Step 77, Loss: 0.0134\n",
      "Epoch 8, Step 78, Loss: 0.0107\n",
      "Epoch 8, Step 79, Loss: 0.0125\n",
      "Epoch 8, Step 80, Loss: 0.0118\n",
      "Epoch 8, Step 81, Loss: 0.0109\n",
      "Epoch 8, Step 82, Loss: 0.0106\n",
      "Epoch 8, Step 83, Loss: 0.0114\n",
      "Epoch 8, Step 84, Loss: 0.0096\n",
      "Epoch 8, Step 85, Loss: 0.0117\n",
      "Epoch 8, Step 86, Loss: 0.0108\n",
      "Epoch 8, Step 87, Loss: 0.0095\n",
      "Epoch 8, Step 88, Loss: 0.0120\n",
      "Epoch 8, Step 89, Loss: 0.0127\n",
      "Epoch 8, Step 90, Loss: 0.0103\n",
      "Epoch 8, Step 91, Loss: 0.0122\n",
      "Epoch 8, Step 92, Loss: 0.0127\n",
      "Epoch 8, Step 93, Loss: 0.0118\n",
      "Epoch 8, Step 94, Loss: 0.0099\n",
      "Epoch 8, Step 95, Loss: 0.0111\n",
      "Epoch 8, Step 96, Loss: 0.0133\n",
      "Epoch 8, Step 97, Loss: 0.0099\n",
      "Epoch 8, Step 98, Loss: 0.0124\n",
      "Epoch 8, Step 99, Loss: 0.0133\n",
      "Epoch 8 Average Loss: 0.0012\n",
      "Epoch 9, Step 0, Loss: 0.0126\n",
      "Epoch 9, Step 1, Loss: 0.0103\n",
      "Epoch 9, Step 2, Loss: 0.0107\n",
      "Epoch 9, Step 3, Loss: 0.0102\n",
      "Epoch 9, Step 4, Loss: 0.0140\n",
      "Epoch 9, Step 5, Loss: 0.0128\n",
      "Epoch 9, Step 6, Loss: 0.0128\n",
      "Epoch 9, Step 7, Loss: 0.0094\n",
      "Epoch 9, Step 8, Loss: 0.0134\n",
      "Epoch 9, Step 9, Loss: 0.0132\n",
      "Epoch 9, Step 10, Loss: 0.0108\n",
      "Epoch 9, Step 11, Loss: 0.0120\n",
      "Epoch 9, Step 12, Loss: 0.0105\n",
      "Epoch 9, Step 13, Loss: 0.0134\n",
      "Epoch 9, Step 14, Loss: 0.0128\n",
      "Epoch 9, Step 15, Loss: 0.0121\n",
      "Epoch 9, Step 16, Loss: 0.0099\n",
      "Epoch 9, Step 17, Loss: 0.0095\n",
      "Epoch 9, Step 18, Loss: 0.0129\n",
      "Epoch 9, Step 19, Loss: 0.0100\n",
      "Epoch 9, Step 20, Loss: 0.0120\n",
      "Epoch 9, Step 21, Loss: 0.0145\n",
      "Epoch 9, Step 22, Loss: 0.0099\n",
      "Epoch 9, Step 23, Loss: 0.0097\n",
      "Epoch 9, Step 24, Loss: 0.0115\n",
      "Epoch 9, Step 25, Loss: 0.0115\n",
      "Epoch 9, Step 26, Loss: 0.0110\n",
      "Epoch 9, Step 27, Loss: 0.0090\n",
      "Epoch 9, Step 28, Loss: 0.0108\n",
      "Epoch 9, Step 29, Loss: 0.0100\n",
      "Epoch 9, Step 30, Loss: 0.0110\n",
      "Epoch 9, Step 31, Loss: 0.0096\n",
      "Epoch 9, Step 32, Loss: 0.0108\n",
      "Epoch 9, Step 33, Loss: 0.0120\n",
      "Epoch 9, Step 34, Loss: 0.0128\n",
      "Epoch 9, Step 35, Loss: 0.0134\n",
      "Epoch 9, Step 36, Loss: 0.0122\n",
      "Epoch 9, Step 37, Loss: 0.0109\n",
      "Epoch 9, Step 38, Loss: 0.0111\n",
      "Epoch 9, Step 39, Loss: 0.0113\n",
      "Epoch 9, Step 40, Loss: 0.0111\n",
      "Epoch 9, Step 41, Loss: 0.0122\n",
      "Epoch 9, Step 42, Loss: 0.0115\n",
      "Epoch 9, Step 43, Loss: 0.0096\n",
      "Epoch 9, Step 44, Loss: 0.0107\n",
      "Epoch 9, Step 45, Loss: 0.0091\n",
      "Epoch 9, Step 46, Loss: 0.0115\n",
      "Epoch 9, Step 47, Loss: 0.0091\n",
      "Epoch 9, Step 48, Loss: 0.0102\n",
      "Epoch 9, Step 49, Loss: 0.0129\n",
      "Epoch 9, Step 50, Loss: 0.0124\n",
      "Epoch 9, Step 51, Loss: 0.0086\n",
      "Epoch 9, Step 52, Loss: 0.0140\n",
      "Epoch 9, Step 53, Loss: 0.0104\n",
      "Epoch 9, Step 54, Loss: 0.0131\n",
      "Epoch 9, Step 55, Loss: 0.0129\n",
      "Epoch 9, Step 56, Loss: 0.0110\n",
      "Epoch 9, Step 57, Loss: 0.0097\n",
      "Epoch 9, Step 58, Loss: 0.0110\n",
      "Epoch 9, Step 59, Loss: 0.0110\n",
      "Epoch 9, Step 60, Loss: 0.0103\n",
      "Epoch 9, Step 61, Loss: 0.0088\n",
      "Epoch 9, Step 62, Loss: 0.0125\n",
      "Epoch 9, Step 63, Loss: 0.0121\n",
      "Epoch 9, Step 64, Loss: 0.0089\n",
      "Epoch 9, Step 65, Loss: 0.0088\n",
      "Epoch 9, Step 66, Loss: 0.0135\n",
      "Epoch 9, Step 67, Loss: 0.0134\n",
      "Epoch 9, Step 68, Loss: 0.0129\n",
      "Epoch 9, Step 69, Loss: 0.0154\n",
      "Epoch 9, Step 70, Loss: 0.0133\n",
      "Epoch 9, Step 71, Loss: 0.0265\n",
      "Epoch 9, Step 72, Loss: 0.0252\n",
      "Epoch 9, Step 73, Loss: 0.0158\n",
      "Epoch 9, Step 74, Loss: 0.0965\n",
      "Epoch 9, Step 75, Loss: 0.1102\n",
      "Epoch 9, Step 76, Loss: 0.1806\n",
      "Epoch 9, Step 77, Loss: 0.2557\n",
      "Epoch 9, Step 78, Loss: 0.0856\n",
      "Epoch 9, Step 79, Loss: 0.3154\n",
      "Epoch 9, Step 80, Loss: 0.4172\n",
      "Epoch 9, Step 81, Loss: 0.2342\n",
      "Epoch 9, Step 82, Loss: 0.5675\n",
      "Epoch 9, Step 83, Loss: 0.1984\n",
      "Epoch 9, Step 84, Loss: 0.5808\n",
      "Epoch 9, Step 85, Loss: 0.2675\n",
      "Epoch 9, Step 86, Loss: 0.3116\n",
      "Epoch 9, Step 87, Loss: 0.3140\n",
      "Epoch 9, Step 88, Loss: 0.2083\n",
      "Epoch 9, Step 89, Loss: 0.0859\n",
      "Epoch 9, Step 90, Loss: 0.1146\n",
      "Epoch 9, Step 91, Loss: 0.1628\n",
      "Epoch 9, Step 92, Loss: 0.1011\n",
      "Epoch 9, Step 93, Loss: 0.1124\n",
      "Epoch 9, Step 94, Loss: 0.1137\n",
      "Epoch 9, Step 95, Loss: 0.1986\n",
      "Epoch 9, Step 96, Loss: 0.2205\n",
      "Epoch 9, Step 97, Loss: 0.1145\n",
      "Epoch 9, Step 98, Loss: 0.1138\n",
      "Epoch 9, Step 99, Loss: 0.0658\n",
      "Epoch 9 Average Loss: 0.0064\n",
      "Epoch 10, Step 0, Loss: 0.1643\n",
      "Epoch 10, Step 1, Loss: 0.1228\n",
      "Epoch 10, Step 2, Loss: 0.0968\n",
      "Epoch 10, Step 3, Loss: 0.1259\n",
      "Epoch 10, Step 4, Loss: 0.6003\n",
      "Epoch 10, Step 5, Loss: 0.3597\n",
      "Epoch 10, Step 6, Loss: 0.2987\n",
      "Epoch 10, Step 7, Loss: 0.1785\n",
      "Epoch 10, Step 8, Loss: 0.1928\n",
      "Epoch 10, Step 9, Loss: 0.1195\n",
      "Epoch 10, Step 10, Loss: 0.1107\n",
      "Epoch 10, Step 11, Loss: 0.1338\n",
      "Epoch 10, Step 12, Loss: 0.2122\n",
      "Epoch 10, Step 13, Loss: 0.0870\n",
      "Epoch 10, Step 14, Loss: 0.0620\n",
      "Epoch 10, Step 15, Loss: 0.0615\n",
      "Epoch 10, Step 16, Loss: 0.0594\n",
      "Epoch 10, Step 17, Loss: 0.0606\n",
      "Epoch 10, Step 18, Loss: 0.0531\n",
      "Epoch 10, Step 19, Loss: 0.0502\n",
      "Epoch 10, Step 20, Loss: 0.0410\n",
      "Epoch 10, Step 21, Loss: 0.0422\n",
      "Epoch 10, Step 22, Loss: 0.0623\n",
      "Epoch 10, Step 23, Loss: 0.0853\n",
      "Epoch 10, Step 24, Loss: 0.0445\n",
      "Epoch 10, Step 25, Loss: 0.0681\n",
      "Epoch 10, Step 26, Loss: 0.0332\n",
      "Epoch 10, Step 27, Loss: 0.1049\n",
      "Epoch 10, Step 28, Loss: 0.0467\n",
      "Epoch 10, Step 29, Loss: 0.0354\n",
      "Epoch 10, Step 30, Loss: 0.0455\n",
      "Epoch 10, Step 31, Loss: 0.0394\n",
      "Epoch 10, Step 32, Loss: 0.0431\n",
      "Epoch 10, Step 33, Loss: 0.0356\n",
      "Epoch 10, Step 34, Loss: 0.0328\n",
      "Epoch 10, Step 35, Loss: 0.0274\n",
      "Epoch 10, Step 36, Loss: 0.0303\n",
      "Epoch 10, Step 37, Loss: 0.0271\n",
      "Epoch 10, Step 38, Loss: 0.0253\n",
      "Epoch 10, Step 39, Loss: 0.0211\n",
      "Epoch 10, Step 40, Loss: 0.0194\n",
      "Epoch 10, Step 41, Loss: 0.0166\n",
      "Epoch 10, Step 42, Loss: 0.0209\n",
      "Epoch 10, Step 43, Loss: 0.0207\n",
      "Epoch 10, Step 44, Loss: 0.0181\n",
      "Epoch 10, Step 45, Loss: 0.0197\n",
      "Epoch 10, Step 46, Loss: 0.0174\n",
      "Epoch 10, Step 47, Loss: 0.0189\n",
      "Epoch 10, Step 48, Loss: 0.0170\n",
      "Epoch 10, Step 49, Loss: 0.0190\n",
      "Epoch 10, Step 50, Loss: 0.0193\n",
      "Epoch 10, Step 51, Loss: 0.0159\n",
      "Epoch 10, Step 52, Loss: 0.0125\n",
      "Epoch 10, Step 53, Loss: 0.0177\n",
      "Epoch 10, Step 54, Loss: 0.0163\n",
      "Epoch 10, Step 55, Loss: 0.0123\n",
      "Epoch 10, Step 56, Loss: 0.0158\n",
      "Epoch 10, Step 57, Loss: 0.0162\n",
      "Epoch 10, Step 58, Loss: 0.0131\n",
      "Epoch 10, Step 59, Loss: 0.0227\n",
      "Epoch 10, Step 60, Loss: 0.0144\n",
      "Epoch 10, Step 61, Loss: 0.0131\n",
      "Epoch 10, Step 62, Loss: 0.0142\n",
      "Epoch 10, Step 63, Loss: 0.0135\n",
      "Epoch 10, Step 64, Loss: 0.0136\n",
      "Epoch 10, Step 65, Loss: 0.0174\n",
      "Epoch 10, Step 66, Loss: 0.0130\n",
      "Epoch 10, Step 67, Loss: 0.0138\n",
      "Epoch 10, Step 68, Loss: 0.0117\n",
      "Epoch 10, Step 69, Loss: 0.0127\n",
      "Epoch 10, Step 70, Loss: 0.0118\n",
      "Epoch 10, Step 71, Loss: 0.0162\n",
      "Epoch 10, Step 72, Loss: 0.0140\n",
      "Epoch 10, Step 73, Loss: 0.0127\n",
      "Epoch 10, Step 74, Loss: 0.0175\n",
      "Epoch 10, Step 75, Loss: 0.0160\n",
      "Epoch 10, Step 76, Loss: 0.0152\n",
      "Epoch 10, Step 77, Loss: 0.0101\n",
      "Epoch 10, Step 78, Loss: 0.0135\n",
      "Epoch 10, Step 79, Loss: 0.0136\n",
      "Epoch 10, Step 80, Loss: 0.0132\n",
      "Epoch 10, Step 81, Loss: 0.0151\n",
      "Epoch 10, Step 82, Loss: 0.0136\n",
      "Epoch 10, Step 83, Loss: 0.0134\n",
      "Epoch 10, Step 84, Loss: 0.0133\n",
      "Epoch 10, Step 85, Loss: 0.0137\n",
      "Epoch 10, Step 86, Loss: 0.0125\n",
      "Epoch 10, Step 87, Loss: 0.0136\n",
      "Epoch 10, Step 88, Loss: 0.0150\n",
      "Epoch 10, Step 89, Loss: 0.0150\n",
      "Epoch 10, Step 90, Loss: 0.0136\n",
      "Epoch 10, Step 91, Loss: 0.0117\n",
      "Epoch 10, Step 92, Loss: 0.0104\n",
      "Epoch 10, Step 93, Loss: 0.0126\n",
      "Epoch 10, Step 94, Loss: 0.0140\n",
      "Epoch 10, Step 95, Loss: 0.0137\n",
      "Epoch 10, Step 96, Loss: 0.0128\n",
      "Epoch 10, Step 97, Loss: 0.0152\n",
      "Epoch 10, Step 98, Loss: 0.0088\n",
      "Epoch 10, Step 99, Loss: 0.0121\n",
      "Epoch 10 Average Loss: 0.0049\n",
      "Epoch 11, Step 0, Loss: 0.0138\n",
      "Epoch 11, Step 1, Loss: 0.0149\n",
      "Epoch 11, Step 2, Loss: 0.0147\n",
      "Epoch 11, Step 3, Loss: 0.0128\n",
      "Epoch 11, Step 4, Loss: 0.0127\n",
      "Epoch 11, Step 5, Loss: 0.0129\n",
      "Epoch 11, Step 6, Loss: 0.0138\n",
      "Epoch 11, Step 7, Loss: 0.0116\n",
      "Epoch 11, Step 8, Loss: 0.0156\n",
      "Epoch 11, Step 9, Loss: 0.0162\n",
      "Epoch 11, Step 10, Loss: 0.0114\n",
      "Epoch 11, Step 11, Loss: 0.0108\n",
      "Epoch 11, Step 12, Loss: 0.0122\n",
      "Epoch 11, Step 13, Loss: 0.0140\n",
      "Epoch 11, Step 14, Loss: 0.0126\n",
      "Epoch 11, Step 15, Loss: 0.0131\n",
      "Epoch 11, Step 16, Loss: 0.0115\n",
      "Epoch 11, Step 17, Loss: 0.0114\n",
      "Epoch 11, Step 18, Loss: 0.0133\n",
      "Epoch 11, Step 19, Loss: 0.0134\n",
      "Epoch 11, Step 20, Loss: 0.0141\n",
      "Epoch 11, Step 21, Loss: 0.0115\n",
      "Epoch 11, Step 22, Loss: 0.0142\n",
      "Epoch 11, Step 23, Loss: 0.0146\n",
      "Epoch 11, Step 24, Loss: 0.0132\n",
      "Epoch 11, Step 25, Loss: 0.0133\n",
      "Epoch 11, Step 26, Loss: 0.0108\n",
      "Epoch 11, Step 27, Loss: 0.0088\n",
      "Epoch 11, Step 28, Loss: 0.0131\n",
      "Epoch 11, Step 29, Loss: 0.0116\n",
      "Epoch 11, Step 30, Loss: 0.0110\n",
      "Epoch 11, Step 31, Loss: 0.0124\n",
      "Epoch 11, Step 32, Loss: 0.0113\n",
      "Epoch 11, Step 33, Loss: 0.0131\n",
      "Epoch 11, Step 34, Loss: 0.0118\n",
      "Epoch 11, Step 35, Loss: 0.0139\n",
      "Epoch 11, Step 36, Loss: 0.0164\n",
      "Epoch 11, Step 37, Loss: 0.0135\n",
      "Epoch 11, Step 38, Loss: 0.0120\n",
      "Epoch 11, Step 39, Loss: 0.0120\n",
      "Epoch 11, Step 40, Loss: 0.0104\n",
      "Epoch 11, Step 41, Loss: 0.0148\n",
      "Epoch 11, Step 42, Loss: 0.0123\n",
      "Epoch 11, Step 43, Loss: 0.0139\n",
      "Epoch 11, Step 44, Loss: 0.0150\n",
      "Epoch 11, Step 45, Loss: 0.0120\n",
      "Epoch 11, Step 46, Loss: 0.0106\n",
      "Epoch 11, Step 47, Loss: 0.0134\n",
      "Epoch 11, Step 48, Loss: 0.0115\n",
      "Epoch 11, Step 49, Loss: 0.0124\n",
      "Epoch 11, Step 50, Loss: 0.0102\n",
      "Epoch 11, Step 51, Loss: 0.0137\n",
      "Epoch 11, Step 52, Loss: 0.0140\n",
      "Epoch 11, Step 53, Loss: 0.0117\n",
      "Epoch 11, Step 54, Loss: 0.0094\n",
      "Epoch 11, Step 55, Loss: 0.0124\n",
      "Epoch 11, Step 56, Loss: 0.0102\n",
      "Epoch 11, Step 57, Loss: 0.0144\n",
      "Epoch 11, Step 58, Loss: 0.0127\n",
      "Epoch 11, Step 59, Loss: 0.0125\n",
      "Epoch 11, Step 60, Loss: 0.0129\n",
      "Epoch 11, Step 61, Loss: 0.0146\n",
      "Epoch 11, Step 62, Loss: 0.0118\n",
      "Epoch 11, Step 63, Loss: 0.0105\n",
      "Epoch 11, Step 64, Loss: 0.0115\n",
      "Epoch 11, Step 65, Loss: 0.0157\n",
      "Epoch 11, Step 66, Loss: 0.0112\n",
      "Epoch 11, Step 67, Loss: 0.0128\n",
      "Epoch 11, Step 68, Loss: 0.0126\n",
      "Epoch 11, Step 69, Loss: 0.0128\n",
      "Epoch 11, Step 70, Loss: 0.0119\n",
      "Epoch 11, Step 71, Loss: 0.0110\n",
      "Epoch 11, Step 72, Loss: 0.0109\n",
      "Epoch 11, Step 73, Loss: 0.0111\n",
      "Epoch 11, Step 74, Loss: 0.0129\n",
      "Epoch 11, Step 75, Loss: 0.0124\n",
      "Epoch 11, Step 76, Loss: 0.0106\n",
      "Epoch 11, Step 77, Loss: 0.0125\n",
      "Epoch 11, Step 78, Loss: 0.0111\n",
      "Epoch 11, Step 79, Loss: 0.0114\n",
      "Epoch 11, Step 80, Loss: 0.0121\n",
      "Epoch 11, Step 81, Loss: 0.0123\n",
      "Epoch 11, Step 82, Loss: 0.0116\n",
      "Epoch 11, Step 83, Loss: 0.0111\n",
      "Epoch 11, Step 84, Loss: 0.0138\n",
      "Epoch 11, Step 85, Loss: 0.0112\n",
      "Epoch 11, Step 86, Loss: 0.0116\n",
      "Epoch 11, Step 87, Loss: 0.0116\n",
      "Epoch 11, Step 88, Loss: 0.0114\n",
      "Epoch 11, Step 89, Loss: 0.0106\n",
      "Epoch 11, Step 90, Loss: 0.0143\n",
      "Epoch 11, Step 91, Loss: 0.0107\n",
      "Epoch 11, Step 92, Loss: 0.0102\n",
      "Epoch 11, Step 93, Loss: 0.0114\n",
      "Epoch 11, Step 94, Loss: 0.0113\n",
      "Epoch 11, Step 95, Loss: 0.0123\n",
      "Epoch 11, Step 96, Loss: 0.0106\n",
      "Epoch 11, Step 97, Loss: 0.0123\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m main_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:314\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mC:\\ProgramData\\miniforge3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:314\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step in range(100):  # Adjust as needed\n",
    "        x_batch, y_batch = get_batch(batch_size, seq_len)\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            preds = model(x_batch)\n",
    "            loss = modified_loss(preds, y_batch, placeholder_idx, df=2.718281828459, eps=1e-9)\n",
    "        main_loss = loss.detach()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += main_loss.item()\n",
    "        losses.append(main_loss.cpu())\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {main_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss/1000:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sample:\n",
      " oh Romeo! Romeo!jm$f?tPPiPkqSmMddL3A&c:'qtUduyRbrAYxjCq cAJJq$sh;;jPJwA,pcG OL$zyUfFF;!;FqSt&gvtSTqVl:WAbUsXXDBP! 3\n",
      "IcpZDjpiLa&iDniYyULAOV Vhc3ZhzHuCXMUsrF-i:NDzp&hâ–’?;pXr,cVUPYrKTpykLCjFCAM&yf\n",
      "pUzTjN$BJAEUuHQx:p$CpGd\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Evaluation: Text Generation\n",
    "# ====================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prompt = \"oh Romeo! Romeo!\"\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    generated = context\n",
    "    for _ in range(200):  # Generate 200 tokens.\n",
    "        inp = generated[:, -seq_len:]\n",
    "        p = model(inp)  # p: (B, seq, vocab_size)\n",
    "        last_token_probs = p[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "        predicted_token = torch.multinomial(F.softmax(last_token_probs, dim=-1), num_samples=1)\n",
    "\n",
    "        #next_token = torch.multinomial(last_token_probs, num_samples=1)\n",
    "        generated = torch.cat((generated, predicted_token), dim=1)\n",
    "    sample = decode(generated[0].cpu().tolist())\n",
    "    print(\"Generated Sample:\\n\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
