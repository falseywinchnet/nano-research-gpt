{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPYRIGHT NOTICE\n",
    "In the name of Christ our Lord be blessed. We, Joshuah Rainstar(joshuah.rainstar@gmail.com), do claim copyright to this code, or software, and associated documentation, as our work in the year 2025 Anno Domini, reserving all rights and assigning them in accordance with the following license terms:\n",
    "\n",
    "1. Permission is by our authority and with this statement granted, to any person or artificial intelligence without limitation or restriction to examine, analyze, read, dissect, translate, use, modify, and distribute the aforementioned copyrighted items, subject to the following conditions:\n",
    "2. This license must be included in full with any copies or works containing substantial portions of the copyrighted items.\n",
    "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "\n",
    "THE COPYRIGHTED ITEMS ARE PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE COPYRIGHTED ITEMS OR THEIR USE OR ANY OTHER CIRCUMSTANCES CONCERNING THEM.\n"
   ]
  },
  {
   "attachments": {
    "28374c77-74dc-463c-984c-f518ca74a4cd.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is an experimental model intended to elucidate possible mechanics for attention across sequences in addition to tokenwise. it is reasonably fast and efficient. conceptually, the design was envisioned by me and coded through refinement with OpenAI Codex Orion One and chatgpt. i find that if i cant understand a thing, however clever it is- its wrong.\n",
    "so, this is largely a from-scratch along aligned principles. \n",
    "\n",
    "you are advised in life to apply a similar practice. nothing good comes of shit you dont comprehend.\n",
    "\n",
    "\"hierarchical multi-scale transformer with MoE-like  selection\"\n",
    "\n",
    "my own fucking activation function\n",
    "\n",
    "my own fucking loss method borrowing from harmonic loss but using student-t distribution!\n",
    "https://arxiv.org/abs/2502.01628\n",
    "\n",
    "XOR from  Two-argument activation functions learn soft XOR operations like cortical neurons\r\n",
    "https://arxiv.org/abs/2110.06871note that my implementation is a differential XOR for backprop capability\n",
    "motivation: little bit of internal reasoning maybe? Impact: slows down convergence somewhat\n",
    "ROPE from google\n",
    "\n",
    "entropy based reward to encourage diverse attention\n",
    "https://arxiv.org/abs/2203.0919\n",
    "\n",
    "https://arxiv.org/pdf/2502.05171\n",
    "latent reasoning in a discrete and hopefully efficient form2\n",
    "\n",
    "\n",
    "WOLF optimizer experimental by me, it may not beat adam but it is simpler than adam, closer to SGD with some smoothing of integration\n",
    "impact: speeds up convergence somewhat for early iterations and will not NAN from high LR.\n",
    "probable benefit- switch optimizers after model drops. could be good for bigger models.. maybe\n",
    "\n",
    "![image.png](attachment:28374c77-74dc-463c-984c-f518ca74a4cd.png)\n",
    "m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jcJTMiWT89P5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dl9uYIM16MG4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def student_t_unembedding(hidden_states, unembedding, df=2.718281828459, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Student's t-based unembedding with optional placeholder modification.\n",
    "    \n",
    "    Arguments:\n",
    "      hidden_states: (B, S, D)  => model’s output embeddings (hidden state)\n",
    "      unembedding:   (D, V)    => learnable \"word vectors\" (unembedding matrix)\n",
    "      df (float): degrees of freedom for the Student's t distribution\n",
    "      eps (float): numerical epsilon to avoid log(0) and div-by-zero\n",
    "          corresponding to the placeholder token. The distances for that token will be adjusted \n",
    "          using an adaptive noise factor.\n",
    "    \n",
    "    Returns:\n",
    "      p: (B, S, V)  probability distribution over V vocabulary tokens.\n",
    "    \"\"\"\n",
    "    B, S, D = hidden_states.shape\n",
    "    V = unembedding.shape[1]\n",
    "    # Expand hidden => (B, S, 1, D)\n",
    "    x_expanded = hidden_states.unsqueeze(2)\n",
    "    # Expand unembedding => (1, 1, V, D)\n",
    "    w_expanded = unembedding.t().unsqueeze(0).unsqueeze(0)  # shape: (1, 1, V, D)\n",
    "    \n",
    "    # Compute squared Euclidean distance between each hidden vector and each unembedding vector.\n",
    "    dist_sq = torch.sum((x_expanded - w_expanded) ** 2, dim=-1).clamp(min=1e-6)  # (B, S, V)\n",
    "        \n",
    "    # Compute the negative energy:\n",
    "    #    E = 0.5*(df + D) * log(1 + dist_sq / df)\n",
    "    # and so log probability (up to an additive constant) is:\n",
    "    #    log_p = -E\n",
    "    log_p_unnorm = -0.5 * (df + D) * torch.log1p(dist_sq / df)  # (B, S, V)\n",
    "    \n",
    "    # Normalize via log_softmax over the vocabulary dimension.\n",
    "    log_p = F.log_softmax(log_p_unnorm, dim=-1)  # (B, S, V)\n",
    "    return log_p\n",
    "\n",
    "\n",
    "\n",
    "class Wolf(Optimizer):\n",
    "    \"\"\"Implements Wolf algorithm.\"\"\"\n",
    "    def __init__(self, params, lr=0.25, betas=(0.9, 0.999), eps=1e-8):\n",
    "        # Define default parameters\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
    "        self.lr = lr\n",
    "        # Initialize the parent Optimizer class first\n",
    "        super().__init__(params, defaults)\n",
    "        # Constants specific to Wolf\n",
    "        # Initialize state for each parameter\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['p'] = torch.zeros_like(p)  # Second moment estimate\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step and adjusts dropout in transformer blocks.\"\"\"\n",
    "        etcerta = 0.367879441  # Constant used in update rule\n",
    "        et = 1 - etcerta\n",
    "    \n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "    \n",
    "        # Iterate over parameter groups.\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad\n",
    "                state = self.state[p]\n",
    "                exp_avg = state['p']\n",
    "    \n",
    "                # Compute update and update second moment-like state.\n",
    "                update = exp_avg * et + grad * etcerta\n",
    "                state['p'] = exp_avg * et + update * etcerta\n",
    "    \n",
    "                # Compute sign agreement between update and gradient.\n",
    "                sign_agreement = torch.sign(update) * torch.sign(grad)\n",
    "    \n",
    "                # Where the signs agree (mask is True), update the parameter.\n",
    "                mask = (sign_agreement > 0)\n",
    "                adaptive_alpha = group.get('lr', self.lr)\n",
    "                p.data = torch.where(mask, p.data - adaptive_alpha * update, p.data)\n",
    "    \n",
    "                # AMP Compatibility: Ensure a step counter is updated\n",
    "                state['step'] = state.get('step', 0) + 1  # Track optimization steps\n",
    "    \n",
    "        return loss\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Custom Activation\n",
    "# ---------------------------------------------------\n",
    "class ReferenceActivation(nn.Module):\n",
    "    def __init__(self, gamma=24):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.clamp_(-2, 2)\n",
    "        log_x = torch.sign(x) * torch.log1p(torch.abs(x))\n",
    "        safe_x = torch.nan_to_num(log_x, nan=0.0)\n",
    "\n",
    "        return log_x / torch.sqrt(1 + self.gamma * log_x ** 2)\n",
    "            \n",
    "class CachedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, batch_first=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # We use the built-in multihead attention module.\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=batch_first)\n",
    "    \n",
    "    def forward(self, query, key, value, past_kv=None):\n",
    "        \"\"\"\n",
    "        query: (B, S_new, D)\n",
    "        key, value: (B, S_current, D) for the current input tokens.\n",
    "        past_kv: Tuple (past_key, past_value) or None.\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            if past_kv is not None:\n",
    "                past_key, past_value = past_kv\n",
    "                # Concatenate along the sequence dimension\n",
    "                key = torch.cat([past_key, key], dim=1)\n",
    "                value = torch.cat([past_value, value], dim=1)\n",
    "            # Run the attention module.\n",
    "            attn_output, _ = self.attn(query, key, value)\n",
    "            # The new cache holds all keys and values computed so far.\n",
    "            new_kv = (key, value)\n",
    "            return attn_output, new_kv\n",
    "        else:\n",
    "            attn_output,attn_weights = self.attn(query, key, value)\n",
    "            attn_weights.clamp_( min=1e-9, max=1.0)\n",
    "            entropy = -torch.sum(attn_weights * torch.log(attn_weights + 1e-9), dim=-1)\n",
    "            attn_weights_1 = attn_weights.sum(dim=-1)\n",
    "            attn_weights_2 = attn_weights.sum(dim=-2)\n",
    "            attn_weights = (attn_weights_1 + attn_weights_2)/2\n",
    "\n",
    "\n",
    "            return attn_output, entropy, attn_weights\n",
    "            \n",
    "class RectifiedKAN(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=8, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = expansion_factor * embed_dim\n",
    "\n",
    "        self.expand = nn.Linear(embed_dim, hidden_dim)   \n",
    "        self.activation = ReferenceActivation()\n",
    "        self.linear = nn.Linear(hidden_dim, embed_dim,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Now the first gemm already includes the shift\n",
    "        x = self.expand(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TapeHeadBlock(nn.Module):\n",
    "    def __init__(self, seq_len, embed_dim, vocab_size, num_heads=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Sinusoidal positional embeddings (Precomputed)\n",
    "        self.register_buffer(\"pos_emb\", self._build_sinusoidal_embedding(seq_len, embed_dim))\n",
    "\n",
    "        # Rotary embedding setup\n",
    "        self.use_rope = True  # Set to False to disable RoPE\n",
    "        if self.use_rope:\n",
    "            self.register_buffer(\"rope_freqs\", self._build_rope_frequencies(embed_dim))\n",
    "\n",
    "        # Attention layers\n",
    "        self.cached_attn = CachedMultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln_attn = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # MLP and normalization\n",
    "        self.mlp = RectifiedKAN(embed_dim)\n",
    "        self.ln_mlp = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Unembedding matrix\n",
    "\n",
    "        # Logits cross-attention\n",
    "\n",
    "    def _build_sinusoidal_embedding(self, seq_len, embed_dim):\n",
    "        \"\"\"Compute sinusoidal positional embeddings\"\"\"\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
    "        pos_emb = torch.zeros(seq_len, embed_dim)\n",
    "        pos_emb[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_emb[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_emb.unsqueeze(0)  # Shape: (1, seq_len, embed_dim)\n",
    "\n",
    "\n",
    "    def _build_rope_frequencies(self, embed_dim):\n",
    "        \"\"\"Build the inverse frequency tensor for RoPE and ensure it is a PyTorch tensor.\"\"\"\n",
    "        half_dim = embed_dim // 2  # For D=256, half_dim = 128\n",
    "        base_freqs = 1.0 / (10000 ** (torch.arange(0, half_dim, dtype=torch.float32) / half_dim))\n",
    "        # Remove the repeat_interleave so we keep shape (half_dim,)\n",
    "        return base_freqs\n",
    "\n",
    "        \n",
    "    def apply_rope(self,tensor, rope_freqs):\n",
    "            \"\"\"\n",
    "            Apply Rotary Positional Embedding (RoPE) to the input tensor.\n",
    "            \n",
    "            Args:\n",
    "                tensor (Tensor): Input tensor of shape (B, S, D), where\n",
    "                                 B = batch size, S = sequence length, D = embedding dim.\n",
    "                rope_freqs (Tensor): Frequency tensor of shape (D/2,) computed via _build_rope_frequencies.\n",
    "                \n",
    "            Returns:\n",
    "                Tensor: The input tensor after applying RoPE, with the same shape (B, S, D).\n",
    "                \n",
    "            Detailed Steps & Dimensions:\n",
    "              1. Let B, S, D = tensor.shape and half_dim = D//2.\n",
    "              2. Compute positions: a tensor of shape (S,).\n",
    "              3. Compute theta = positions.unsqueeze(1) * rope_freqs.unsqueeze(0)\n",
    "                 -> theta has shape (S, half_dim).\n",
    "              4. Compute sin_theta and cos_theta from theta, each of shape (S, half_dim),\n",
    "                 then expand to (B, S, half_dim).\n",
    "              5. Split tensor into two halves along the last dimension:\n",
    "                 - x1 = tensor[..., 0::2] and x2 = tensor[..., 1::2], each with shape (B, S, half_dim).\n",
    "                 (Alternatively, one can do: x1, x2 = torch.chunk(tensor, 2, dim=-1))\n",
    "              6. Apply RoPE:\n",
    "                 - x1_rot = x1 * cos_theta - x2 * sin_theta\n",
    "                 - x2_rot = x1 * sin_theta + x2 * cos_theta\n",
    "              7. Reassemble the output by interleaving x1_rot and x2_rot.\n",
    "            \"\"\"\n",
    "            B, S, D = tensor.shape\n",
    "            if S == 1:\n",
    "                return tensor\n",
    "            assert D % 2 == 0, \"Embedding dimension must be even for RoPE.\"\n",
    "            half_dim = D // 2  # e.g. for D=256, half_dim = 128\n",
    "        \n",
    "            # Ensure rope_freqs is on the same device and dtype as tensor.\n",
    "            rope_freqs = rope_freqs.to(tensor.dtype)  # shape: (half_dim,)\n",
    "        \n",
    "            # 1. Compute positions (0, 1, ..., S-1): shape (S,)\n",
    "            positions = torch.arange(S, device=tensor.device, dtype=tensor.dtype)\n",
    "            \n",
    "            # 2. Compute theta = positions * rope_freqs:\n",
    "            #    positions: (S, 1), rope_freqs: (1, half_dim) --> theta: (S, half_dim)\n",
    "            theta = positions.unsqueeze(1) * rope_freqs.unsqueeze(0)  # shape: (S, half_dim)\n",
    "            \n",
    "            # 3. Compute sin and cos of theta:\n",
    "            sin_theta = theta.sin()  # shape: (S, half_dim)\n",
    "            cos_theta = theta.cos()  # shape: (S, half_dim)\n",
    "            \n",
    "            # 4. Expand sin and cos to shape (B, S, half_dim)\n",
    "            sin_theta = sin_theta.unsqueeze(0).expand(B, S, half_dim)\n",
    "            cos_theta = cos_theta.unsqueeze(0).expand(B, S, half_dim)\n",
    "            \n",
    "            # 5. Split tensor into two halves (real and imaginary parts).\n",
    "            # Here we use alternate slicing: the even-indexed dims are x1, odd-indexed are x2.\n",
    "            x1 = tensor[..., 0::2]  # shape: (B, S, half_dim)\n",
    "            x2 = tensor[..., 1::2]  # shape: (B, S, half_dim)\n",
    "            \n",
    "            # 6. Apply the RoPE rotation:\n",
    "            x1_rot = x1 * cos_theta - x2 * sin_theta  # shape: (B, S, half_dim)\n",
    "            x2_rot = x1 * sin_theta + x2 * cos_theta  # shape: (B, S, half_dim)\n",
    "            \n",
    "            # 7. Interleave x1_rot and x2_rot back together.\n",
    "            # One approach is to create an empty tensor and then fill in even and odd indices.\n",
    "            out = torch.empty_like(tensor)\n",
    "            out[..., 0::2] = x1_rot\n",
    "            out[..., 1::2] = x2_rot\n",
    "        \n",
    "            return out\n",
    "\n",
    "\n",
    "    def forward(self, token_emb, prev_h, prev_emb, past_kv=None):\n",
    "        \"\"\"\n",
    "        x: (B, S) input token IDs\n",
    "        prev_h: (B, S, D) previous hidden state\n",
    "        prev_emb: (B, S, D) previous embeddings\n",
    "        logits: (B, S, V) logits distribution from previous block (can be None)\n",
    "        past_kv: Dictionary with keys 'chunk_attn' and 'logits_attn' holding KV caches\n",
    "        \"\"\"\n",
    "\n",
    "        past_chunk = past_kv.get('chunk_attn') if past_kv is not None else None\n",
    "        # Compute token embeddings and add sinusoidal positional embeddings\n",
    "        seq_len = token_emb.shape[1]  # Dynamically get sequence length\n",
    "        layer_emb = token_emb + self.pos_emb[:, :seq_len, :]\n",
    "\n",
    "        # Apply RoPE if enabled\n",
    "        if self.use_rope:\n",
    "            layer_emb = self.apply_rope(layer_emb, self.rope_freqs)\n",
    "\n",
    "        # Attention input\n",
    "        if prev_h is not None:\n",
    "            attn_input = torch.cat([prev_emb, layer_emb, prev_h[:, -seq_len:, :]], dim=1)\n",
    "            attn_input = attn_input[:, -seq_len:, :]\n",
    "\n",
    "        else:\n",
    "            attn_input = layer_emb\n",
    "\n",
    "\n",
    "        # Compute Self-Attention with KV Caching\n",
    "        if not self.training:\n",
    "            if past_chunk is not None:\n",
    "                attn_out, new_chunk_cache  = self.cached_attn(\n",
    "                    attn_input, attn_input, attn_input, past_kv=past_chunk\n",
    "                )\n",
    "            else:\n",
    "                attn_out, new_chunk_cache = self.cached_attn(attn_input, attn_input, attn_input)\n",
    "        else:\n",
    "            attn_out, entropy_chunk, attn_weights = self.cached_attn(attn_input, attn_input, attn_input)\n",
    "\n",
    "        h_attn = self.ln_attn(attn_input + attn_out)\n",
    "\n",
    "        \n",
    "        # Pass through MLP\n",
    "        h_mlp = self.ln_mlp(h_attn + self.mlp(h_attn))\n",
    "\n",
    "        # Compute final logits\n",
    "        # Return KV cache\n",
    "        if not self.training:\n",
    "            new_cache = {'chunk_attn': new_chunk_cache}\n",
    "            return h_mlp, layer_emb, new_cache\n",
    "\n",
    "        if self.training:\n",
    "            attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "            return h_mlp, layer_emb, entropy_chunk, attn_weights\n",
    "\n",
    "class CharRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = ReferenceActivation()\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        h_candidate = self.i2h(x) + self.h2h(h_prev)\n",
    "        h_new = self.activation(h_candidate)\n",
    "        return h_new\n",
    "\n",
    "class CharRNNModel(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim=128, hidden_size=256, num_layers=2, placeholder_idx=None):\n",
    "        super().__init__()\n",
    "        self.activation = ReferenceActivation()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.placeholder_idx = placeholder_idx\n",
    "\n",
    "        self.cells_right = nn.ModuleList([\n",
    "            CharRNNCell(embed_dim if i == 0 else hidden_size, hidden_size) for i in range(num_layers)\n",
    "        ])\n",
    "        self.cells_left = nn.ModuleList([\n",
    "            CharRNNCell(embed_dim if i == 0 else hidden_size, hidden_size) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, embedded_x, past_kv=None):\n",
    "        batch_size = embedded_x.shape[0]\n",
    "    \n",
    "        # ✅ Ensure hidden_state has a consistent shape\n",
    "        if not self.training and past_kv is not None:\n",
    "            # Evaluation mode: read from past_kv (expecting 3D shape)\n",
    "            hidden_state = [kv[:, -1, :] for kv in past_kv]  # Always valid if kv has 3 dims\n",
    "        else:\n",
    "            # Training mode or no past_kv: initialize hidden state\n",
    "            hidden_state = [torch.zeros(batch_size, self.hidden_size, device=embedded_x.device)\n",
    "                            for _ in range(self.num_layers)]\n",
    "    \n",
    "        outputs = []\n",
    "        new_hidden_state = []\n",
    "    \n",
    "        for t in range(embedded_x.shape[1]):\n",
    "            input_t = embedded_x[:, t, :]\n",
    "            new_hidden = []\n",
    "    \n",
    "            for layer in range(self.num_layers):\n",
    "                cell_left = self.cells_left[layer]\n",
    "                cell_right = self.cells_right[layer]\n",
    "                h_prev = hidden_state[layer]  # (batch_size, hidden_size)\n",
    "    \n",
    "                h_new_left = cell_left(input_t, h_prev)\n",
    "                h_new_right = cell_right(input_t, h_prev)\n",
    "    \n",
    "                # Combine left and right outputs\n",
    "                a = self.activation(h_new_left)\n",
    "                b = self.activation(h_new_right)\n",
    "                h_new = 0.5 * (a + b - 2 * a * b)\n",
    "    \n",
    "                new_hidden.append(h_new)\n",
    "                input_t = h_new\n",
    "    \n",
    "            hidden_state = new_hidden\n",
    "            outputs.append(input_t)\n",
    "    \n",
    "            # ✅ Consistently shape cached hidden states as (batch_size, 1, hidden_size)\n",
    "            new_hidden_state = [h.unsqueeze(1) for h in hidden_state]  # No if-check needed\n",
    "    \n",
    "        outputs = torch.stack(outputs, dim=1)  # (batch_size, seq_len, hidden_size)\n",
    "    \n",
    "        # ✅ In training, return None for cache. In eval, return consistent 3D cache.\n",
    "        return outputs, new_hidden_state if not self.training else None\n",
    "\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "class TapeHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-like block with progressive chunk sizes.\n",
    "    Each layer inside the TapeHead doubles the chunk size.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, embed_dim, vocab_size, num_layers=3, dropout=0.1,discriminate=0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.discriminate=discriminate\n",
    "        layer_weights = torch.linspace(0.3, 1.0, steps=self.num_layers)\n",
    "        layer_weights /= layer_weights.sum()  # Normalize\n",
    "        self.register_buffer(\"layer_weights\", layer_weights)\n",
    "        # Create progressively larger TapeHeadBlocks for the left and right streams.\n",
    "\n",
    "        self.blocks_left= nn.ModuleList([\n",
    "            TapeHeadBlock(\n",
    "                seq_len=seq_len,\n",
    "                embed_dim=embed_dim,\n",
    "                vocab_size=vocab_size,\n",
    "                num_heads=4,  #true for up to 512 embed dem\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.blocks_right = nn.ModuleList([\n",
    "            TapeHeadBlock(\n",
    "                seq_len=seq_len,\n",
    "                embed_dim=embed_dim,\n",
    "                vocab_size=vocab_size,\n",
    "                num_heads=4,  # Inversely scale heads\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, token_emb, prev_h, prev_emb, past_kv=None):\n",
    "        \"\"\"\n",
    "        past_kv: dictionary with keys 'left' and 'right', each is a list of caches (one per block).\n",
    "        \"\"\"\n",
    "        attn_weights = []\n",
    "        entropy = []\n",
    "    \n",
    "        if not self.training:\n",
    "            if past_kv is None:\n",
    "                past_kv = {'left': [None] * self.num_layers, 'right': [None] * self.num_layers}\n",
    "            prev_emb_left = prev_emb.clone() if prev_emb is not None else None\n",
    "            prev_h_left = prev_h.clone() if prev_h is not None else None\n",
    "    \n",
    "            new_past_left = []\n",
    "            new_past_right = []\n",
    "    \n",
    "            for i in range(self.num_layers):\n",
    "                # Process left block with its cache.\n",
    "                    h_out_left, prev_emb_left, cache_left = self.blocks_left[i](\n",
    "                         token_emb,prev_h_left, prev_emb_left, past_kv=past_kv['left'][i]\n",
    "                    )\n",
    "                                    # Process right block with its cache.\n",
    "                    h_out,  prev_emb, cache_right = self.blocks_right[i](\n",
    "                        token_emb,prev_h, prev_emb, past_kv=past_kv['right'][i])\n",
    "                \n",
    "                    if self.discriminate==0:\n",
    "                        \n",
    "                        a = self.activation(h_out_left)\n",
    "                        b = self.activation(h_out)\n",
    "                        h_out = 0.5 * (a + b - 2 * a * b)\n",
    "                        prev_h_left = h_out\n",
    "                        prev_h = h_out\n",
    "                    elif self.discriminate==1:\n",
    "                        combined = torch.stack([h_out_left, h_out], dim=-1)  # Shape: (B, S, D, 2)\n",
    "                        weights = torch.softmax(combined, dim=-1)  # Normalize contributions\n",
    "                        h_out = (combined * weights).sum(dim=-1)  # Shape: (B, S, D)\n",
    "                        prev_h_left = h_out\n",
    "                        prev_h = h_out\n",
    "\n",
    "                    else:\n",
    "                        h_out= h_out_left+h_out\n",
    "                        prev_h_left = h_out\n",
    "                        prev_h = h_out\n",
    "                        \n",
    "                    new_past_left.append(cache_left)  # Store all layers' caches\n",
    "                    new_past_right.append(cache_right)  # Store all layers' caches\n",
    "\n",
    "            prev_emb = 0.5 * (prev_emb + prev_emb_left)\n",
    "            new_cache = {'left': new_past_left, 'right': new_past_right}\n",
    "            return h_out, prev_emb, new_cache  # ✅ Properly returning all layer caches\n",
    "    \n",
    "        else:  # Training mode\n",
    "            prev_emb_left = prev_emb.clone() if prev_emb is not None else None\n",
    "            prev_h_left = prev_h.clone() if prev_h is not None else None\n",
    "    \n",
    "            for i in range(self.num_layers):\n",
    "                # Process left block\n",
    "                    h_out_left, prev_emb_left, entropy_left, attn_weights_left = self.blocks_left[i](\n",
    "                        token_emb, prev_h_left, prev_emb_left,\n",
    "                    )\n",
    "        \n",
    "                    # Process right block\n",
    "                    h_out, prev_emb, entropy_right, attn_weights_right = self.blocks_right[i](\n",
    "                        token_emb, prev_h, prev_emb\n",
    "                    )\n",
    "                    if self.discriminate==0:\n",
    "                        \n",
    "                        a = self.activation(h_out_left)\n",
    "                        b = self.activation(h_out)\n",
    "                        h_out = 0.5 * (a + b - 2 * a * b)\n",
    "                        prev_h_left = h_out\n",
    "                        prev_h = h_out\n",
    "                    elif self.discriminate==1:\n",
    "                        combined = torch.stack([h_out_left, h_out], dim=-1)  # Shape: (B, S, D, 2)\n",
    "                        weights = torch.softmax(combined, dim=-1)  # Normalize contributions\n",
    "                        h_out = (combined * weights).sum(dim=-1)  # Shape: (B, S, D)\n",
    "                        prev_h_left = h_out\n",
    "                        prev_h = h_out\n",
    "\n",
    "                    else:\n",
    "                        h_out= h_out_left+h_out\n",
    "                        prev_h_left = h_out\n",
    "                        prev_h = h_out\n",
    "                        \n",
    "                    attn_weights.append((attn_weights_left + attn_weights_right) / 2.0)\n",
    "                    entropy.append((entropy_left + entropy_right) / 2.0)\n",
    "    \n",
    "\n",
    "            prev_emb = 0.5 * (prev_emb + prev_emb_left)\n",
    "            \n",
    "            attn_weights = torch.stack(attn_weights, dim=0)\n",
    "            attn_weights *= self.layer_weights.view(-1, 1, 1)\n",
    "            attn_weights = attn_weights.sum(dim=0)\n",
    "            entropy = torch.sum(torch.stack(entropy)) / self.num_layers\n",
    "                \n",
    "            return  h_out, prev_emb, entropy, attn_weights  # ✅ Returns all caches correctly\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / (x.norm(2, dim=-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Merger(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # linear that goes from 2*V -> out_dim\n",
    "        self.proj = nn.Linear(in_dim*2, out_dim, bias=False)\n",
    "\n",
    "    def forward(self, heads_2d):\n",
    "        \"\"\"\n",
    "        heads_2d: shape (2, B, S, V)\n",
    "        We'll reshape it to (B, S, 2*V), pass it into self.proj, and place it back into heads_2d[1].\n",
    "        \"\"\"\n",
    "        # heads_2d[0] = sub-head #1 (B, S, V)\n",
    "        # heads_2d[1] = sub-head #2 (B, S, V)\n",
    "        # 1) reshape to (B, S, 2*V):\n",
    "        B, S, V = heads_2d.shape[1], heads_2d.shape[2], heads_2d.shape[3]\n",
    "        merged_view = heads_2d.view(-1, S, 2*V)  # shape (B, S, 2*V)\n",
    "\n",
    "        # 2) run through the linear\n",
    "        #    result will be shape (B, S, out_dim)\n",
    "        merged_out = self.proj(merged_view)  # (B, S, out_dim)\n",
    "\n",
    "        # 3) Optionally, store back in heads_2d[1] or somewhere else.\n",
    "        #    If you want to in-place the second head, make sure you keep track of shapes.\n",
    "        #    Here, let's just re-view merged_out as (B, S, out_dim) => (1, B, S, out_dim)\n",
    "        #    and store it in heads_2d[1].\n",
    "        out_dim = merged_out.shape[-1]\n",
    "        # ensure we can place it into heads_2d[1] shape (B, S, V) if out_dim == V\n",
    "        # or allocate a new dimension for it, depending on design.\n",
    "\n",
    "        if out_dim == V:\n",
    "            # then we can store in heads_2d[1] in place\n",
    "            heads_2d[1].copy_(merged_out)\n",
    "            return heads_2d[1]  # shape (B, S, V)\n",
    "        else:\n",
    "            # If out_dim != V, you'd store it in a separate buffer or return merged_out directly\n",
    "            return merged_out  # shape (B, S, out_dim)\n",
    "\n",
    "\n",
    "\n",
    "class TapeTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Full GPT-like model with:\n",
    "      - Token + Position Embeddings\n",
    "      - Multiple stacked TapeHeads\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, seq_len=128, num_layers=4, embed_dim=128, num_heads=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.holistic_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.prophet = CharRNNModel(seq_len,vocab_size,embed_dim = embed_dim, hidden_size=embed_dim*2)\n",
    "        self.rnn_projection = nn.Linear(embed_dim*2, embed_dim)\n",
    "\n",
    "        # LayerNorm immediately after embeddings.\n",
    "        self.embed_ln = nn.LayerNorm(embed_dim)\n",
    "        self.codahead =  TapeHead(\n",
    "                seq_len=seq_len,\n",
    "                embed_dim=embed_dim,\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=2,\n",
    "                dropout=dropout,\n",
    "                discriminate=0\n",
    "            )\n",
    "        # Create a list of TapeHeads.\n",
    "        self.residualheads = nn.ModuleList([\n",
    "            TapeHead(\n",
    "                seq_len=seq_len,\n",
    "                embed_dim=embed_dim,\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=4,\n",
    "                dropout=dropout,\n",
    "                discriminate=1\n",
    "\n",
    "            )\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.merger = Merger(in_dim=embed_dim, out_dim=embed_dim)\n",
    "        self.final_norm = RMSNorm(len(self.residualheads) * embed_dim)\n",
    "        self.norm_gate = nn.Parameter(torch.tensor(0.5))  # for potential supervisory context\n",
    "        self.activation = ReferenceActivation()\n",
    "        # Final unembedding.\n",
    "        self.unembedding = nn.Parameter(torch.randn(embed_dim, vocab_size))\n",
    "        nn.init.kaiming_uniform_(self.unembedding, a=math.sqrt(5))\n",
    "        self.merge_projection = nn.Linear(2 * embed_dim, embed_dim, bias=False)\n",
    "        self.merge_projection_final = nn.Linear(len(self.residualheads) * embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x, past_kv=None):\n",
    "        \"\"\"\n",
    "        x: (B, S) integer token IDs.\n",
    "        past_kv: list of caches (one per tape head) or None.\n",
    "        Returns:\n",
    "          p_final: (B, S, V) probability distribution,\n",
    "          new_past_kv: updated caches.\n",
    "        \"\"\"\n",
    "        # Ensure batch dimension.\n",
    "        x = x.unsqueeze(0) if x.ndim == 1 else x\n",
    "        B, S = x.shape\n",
    "        assert S <= self.seq_len, \"Sequence too long.\"\n",
    "\n",
    "        prev_h = None\n",
    "        prev_emb = None\n",
    "        new_past_heads = None\n",
    "        attn_weights = None\n",
    "        entropy = []\n",
    "\n",
    "        if not self.training:\n",
    "            new_past_heads = []\n",
    "            if past_kv is None:\n",
    "                past_kv = (\n",
    "                    [{'left': [None] * self.codahead.num_layers, 'right': [None] * self.codahead.num_layers}] +\n",
    "                    [[torch.zeros(batch_size, 1, self.prophet.hidden_size, device=x.device)  # ✅ Added time dim\n",
    "                      for _ in range(self.prophet.num_layers)]] +\n",
    "                    [{'left': [None] * head.num_layers, 'right': [None] * head.num_layers}\n",
    "                     for head in self.residualheads]\n",
    "                )\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            consistency = self.holistic_embed(x)\n",
    "            ##coda\n",
    "            coda_h, prev_emb, head_cache = self.codahead(consistency, prev_h, prev_emb, past_kv=past_kv[0])\n",
    "            new_past_heads.append(head_cache)\n",
    "\n",
    "\n",
    "            prophecy,head_cache = self.prophet.forward(consistency,past_kv[1])\n",
    "            new_past_heads.append(head_cache)\n",
    "\n",
    "            \n",
    "            for i, head in enumerate(self.residualheads):\n",
    "                consistency, _, head_cache = head(consistency, coda_h, prev_emb, past_kv=past_kv[2+i])\n",
    "                new_past_heads.append(head_cache)\n",
    "\n",
    "            \n",
    "            prev_h = self.merge_projection_final(consistency)#nn.Linear(len(self.residualheads) * embed_dim, embed_dim, bias=False)\n",
    "\n",
    "\n",
    "        else:        \n",
    "            consistency = self.holistic_embed(x)\n",
    "            coda_h, prev_emb, entropy_head, attn_weights = self.codahead(consistency, prev_h, prev_emb)\n",
    "            entropy.append(entropy_head)\n",
    "\n",
    "            prophecy,_= self.prophet.forward(consistency)\n",
    "\n",
    "            prophecy_proj = self.rnn_projection(prophecy)\n",
    "\n",
    "            for i, head in enumerate(self.residualheads):\n",
    "                consistency, _, entropy_head, attn_weights_head = head(consistency, coda_h,prophecy_proj)\n",
    "                attn_weights = attn_weights + attn_weights_head\n",
    "                entropy.append(entropy_head)\n",
    "\n",
    "        \n",
    "        if not self.training:\n",
    "            p_final = student_t_unembedding(consistency, self.unembedding, df=2.718281828459, eps=1e-9)\n",
    "            return p_final, new_past_heads\n",
    "        else:\n",
    "            attn_weights /= self.num_heads\n",
    "            p_final = student_t_unembedding(consistency, self.unembedding, df=2.718281828459, eps=1e-9)\n",
    "            return p_final, torch.sum(torch.stack(entropy))/self.num_heads+1, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc,torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VhlsGaG7ONr",
    "outputId": "1534f894-6597-49b5-c0c3-41369844874c"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Data Preparation (Shakespeare)\n",
    "# ====================================================\n",
    "def load_shakespeare_text():\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    text = requests.get(url).text\n",
    "    return text\n",
    "\n",
    "text = load_shakespeare_text()\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "def get_batch(batch_size, seq_len):\n",
    "    ix = torch.randint(0, data.size(0) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# ====================================================\n",
    "# Training Setup\n",
    "# ====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TapeTransformer(\n",
    "    vocab_size=vocab_size,  # example\n",
    "    seq_len=128,\n",
    "    num_layers=4,\n",
    "    embed_dim=256,\n",
    "    num_heads=4,\n",
    "    dropout=0 #cannot use dropout, tooo slow\n",
    ")\n",
    "#model = torch.compile(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "eps = 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# Training control variables\n",
    "seq_len = 128  # Start with the smallest sequence\n",
    "max_seq_len = 128\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Loss tracking\n",
    "epochs_per_check = 10  # Print every 10 epochs\n",
    "target_loss = max(math.log(vocab_size/(seq_len+1)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644209df6d454742af99e5b8981358be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adjusted Code to Improve EWMA Size and Implement Loss Ticker as a Moving Graph\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "CHAR_WIDTH = 8  # Font size 8 for token rendering\n",
    "CHAR_HEIGHT = 11\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "LOSS_BAR_HEIGHT = 32\n",
    "EWMA_HEIGHT = 32  # Increased to accommodate large text (previously 32)\n",
    "\n",
    "# Full-resolution framebuffer dimensions\n",
    "container_width = CHAR_WIDTH * SEQ_LEN  # 1024 pixels\n",
    "container_height = CHAR_HEIGHT * BATCH_SIZE  # 176 pixels\n",
    "total_height = container_height + LOSS_BAR_HEIGHT + EWMA_HEIGHT  # Adjusted for larger EWMA\n",
    "\n",
    "# Final scaled-down dimensions\n",
    "scaled_width = container_width   # 512 pixels\n",
    "scaled_height = total_height  # 170 pixels\n",
    "\n",
    "# Initialize framebuffer\n",
    "framebuffer = np.zeros((total_height, container_width, 3), dtype=np.uint8)\n",
    "\n",
    "# EWMA storage\n",
    "loss_history = []\n",
    "ticker_history = np.zeros(SEQ_LEN, dtype=np.float32)  # Stock ticker moving buffer\n",
    "loss_memory = 0.0\n",
    "# Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(\"DejaVuSansMono.ttf\", 8)  # Monospaced font\n",
    "    font_large = ImageFont.truetype(\"DejaVuSansMono.ttf\", 64)  # Large EWMA display\n",
    "except:\n",
    "    font = ImageFont.load_default()\n",
    "    font_large = font\n",
    "\n",
    "# --- Color Mapping Functions ---\n",
    "def get_flame_color(val):\n",
    "    \"\"\"Map a normalized value to a flame-like color.\"\"\"\n",
    "    return np.array([int(val * 255), int(val * 0.5 * 255), 0], dtype=np.uint8)\n",
    "\n",
    "# --- IPython Display Setup ---\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "def get_dynamic_color(attn_val, loss_val):\n",
    "    \"\"\"\n",
    "    Compute a dynamic color transition between flame orange (uncertain) and phosphor green (confident).\n",
    "    \n",
    "    attn_val: Normalized attention value (0 to 1)\n",
    "    loss_val: Normalized loss value (0 to 1, inverted as 1 - loss)\n",
    "    \n",
    "    Returns an RGB color as a NumPy array.\n",
    "    colors late in training will often be red. this is suggested to swap out for get_flame_color\n",
    "    but only on fine tuning on new data.\n",
    "    \"\"\"\n",
    "    certainty = 1 - loss_val  # High certainty = low loss\n",
    "    \n",
    "    # Define RGB endpoints\n",
    "    orange = np.array([attn_val * 255, attn_val * 0.5 * 255, 0], dtype=np.uint8)   # Uncertain (High Loss)\n",
    "    green = np.array([attn_val * 0.5 * 255, attn_val * 255, attn_val * 0.25 * 255], dtype=np.uint8)  # Confident (Low Loss)\n",
    "    \n",
    "    # Interpolate based on certainty (0 = uncertain/orange, 1 = confident/green)\n",
    "    color = (certainty * green) + ((1 - certainty) * orange)\n",
    "    \n",
    "    return color.astype(np.uint8)\n",
    "# --- Framebuffer Update Function ---\n",
    "def update_framebuffer(attn_weights, token_losses, current_loss, tokens,entropy):\n",
    "    attn_weights =(attn_weights-attn_weights.min())/(np.ptp(attn_weights))\n",
    "    token_losses =(token_losses-token_losses.min())/(np.ptp(token_losses))\n",
    "\n",
    "    \"\"\"Render the text grid with coloration based on attn * inverse loss.\"\"\"\n",
    "    global framebuffer, loss_history, ticker_history, loss_memory\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "\n",
    "    # Create image buffer\n",
    "    img = Image.new(\"RGB\", (container_width, total_height), (0, 0, 0))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Render text with colored intensity\n",
    "    char_positions = [\n",
    "        (col * CHAR_WIDTH, row * CHAR_HEIGHT + EWMA_HEIGHT + LOSS_BAR_HEIGHT, tokens[row][col])\n",
    "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
    "    ]\n",
    "    colors = [\n",
    "        tuple(get_dynamic_color(attn_weights[row, col], attn_weights[row, col]))\n",
    "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
    "    ]\n",
    "    for (x, y, char), color in zip(char_positions, colors):\n",
    "        draw.text((x, y), char, font=font, fill=color)\n",
    "\n",
    "                 \n",
    "    etcerta = 0.367879441  # Constant used in update rule\n",
    "    et = 1 - etcerta   \n",
    "    update = loss_memory * et + np.minimum(12, np.maximum(current_loss , 0)) * etcerta\n",
    "    loss_memory = loss_memory * et + update * etcerta\n",
    "    # --- EWMA Display (LARGE FONT) ---\n",
    "    loss_history.append(current_loss)\n",
    "    if len(loss_history) > 128:\n",
    "        loss_history.pop(0)\n",
    "    ewma = loss_memory\n",
    "    ewma_text = f\"{ewma:.4f}\"\n",
    "    draw.text((container_width-128, 0), ewma_text, font_size=32, fill=(65,255, 125))\n",
    "    ent_text = f\"{entropy:.4f}\"\n",
    "    draw.text((10, 0), ent_text, font_size=32, fill=(255,125, 0))  \n",
    "\n",
    "    # --- Moving Loss Ticker Graph ---\n",
    "    ticker_history = np.roll(ticker_history, -1)  # Shift left\n",
    "    ticker_history[-1] = current_loss  # Insert new loss on the right\n",
    "\n",
    "    # Rescale ticker dynamically like a stock ticker (normalize to min-max range)\n",
    "    min_loss = np.min(ticker_history)\n",
    "    max_loss = np.max(ticker_history)\n",
    "    range_loss = max_loss - min_loss if max_loss != min_loss else 1\n",
    "    normalized_ticker = (ticker_history - min_loss) / range_loss\n",
    "\n",
    "    # Draw ticker graph line\n",
    "    # Optimized drawing loop (fewer function calls)\n",
    "    y_vals = EWMA_HEIGHT + (1 - normalized_ticker) * LOSS_BAR_HEIGHT\n",
    "    x_vals = np.arange(SEQ_LEN) * CHAR_WIDTH\n",
    "    for i in range(SEQ_LEN - 1):\n",
    "        draw.line([(x_vals[i], y_vals[i]), (x_vals[i + 1], y_vals[i + 1])], fill=(0, 255, 255), width=2)\n",
    "\n",
    "    framebuffer = np.array(img)\n",
    "\n",
    "# --- IPython Display Update Function ---\n",
    "def update_display():\n",
    "    \"\"\"Show the framebuffer, scaled down by half using ipywidgets.\"\"\"\n",
    "    img = Image.fromarray(framebuffer)\n",
    "    img_resized = img.resize((scaled_width, scaled_height), Image.LANCZOS)\n",
    "    \n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        display(img_resized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.929281\n",
      "Epoch 1, Loss: 3.523187\n",
      "Epoch 2, Loss: 4.066935\n",
      "Epoch 3, Loss: 3.741148\n",
      "Epoch 4, Loss: 3.315287\n",
      "Epoch 5, Loss: 2.967947\n",
      "Epoch 6, Loss: 2.815921\n",
      "Epoch 7, Loss: 2.864223\n",
      "Epoch 8, Loss: 2.787162\n",
      "Epoch 9, Loss: 2.805342\n",
      "Epoch 10, Loss: 2.793988\n",
      "Epoch 11, Loss: 2.799005\n",
      "Epoch 12, Loss: 2.796814\n",
      "Epoch 13, Loss: 2.769015\n",
      "Epoch 14, Loss: 2.798239\n",
      "Epoch 15, Loss: 2.672120\n",
      "Epoch 16, Loss: 2.766676\n",
      "Epoch 17, Loss: 2.690195\n",
      "Epoch 18, Loss: 2.760687\n",
      "Epoch 19, Loss: 2.693091\n",
      "Epoch 20, Loss: 2.746382\n",
      "Epoch 21, Loss: 2.670155\n",
      "Epoch 22, Loss: 2.810584\n",
      "Epoch 23, Loss: 2.753687\n",
      "Epoch 24, Loss: 2.713995\n",
      "Epoch 25, Loss: 2.732414\n",
      "Epoch 26, Loss: 2.705759\n",
      "Epoch 27, Loss: 2.682569\n",
      "Epoch 28, Loss: 2.709775\n",
      "Epoch 29, Loss: 2.738650\n",
      "Epoch 30, Loss: 2.685335\n",
      "Epoch 31, Loss: 2.719124\n",
      "Epoch 32, Loss: 2.729100\n",
      "Epoch 33, Loss: 2.728799\n",
      "Epoch 34, Loss: 2.680136\n",
      "Epoch 35, Loss: 2.723578\n",
      "Epoch 36, Loss: 2.693743\n",
      "Epoch 37, Loss: 2.744836\n",
      "Epoch 38, Loss: 2.669454\n",
      "Epoch 39, Loss: 2.761720\n",
      "Epoch 40, Loss: 2.733779\n",
      "Epoch 41, Loss: 2.653273\n",
      "Epoch 42, Loss: 2.682084\n",
      "Epoch 43, Loss: 2.769402\n",
      "Epoch 44, Loss: 2.723601\n",
      "Epoch 45, Loss: 2.726758\n",
      "Epoch 46, Loss: 2.718569\n",
      "Epoch 47, Loss: 2.759401\n",
      "Epoch 48, Loss: 2.727583\n",
      "Epoch 49, Loss: 2.787390\n",
      "Epoch 50, Loss: 2.732751\n",
      "Epoch 51, Loss: 2.750135\n",
      "Epoch 52, Loss: 2.719164\n",
      "Epoch 53, Loss: 2.702109\n",
      "Epoch 54, Loss: 2.665769\n",
      "Epoch 55, Loss: 2.726937\n",
      "Epoch 56, Loss: 2.680843\n",
      "Epoch 57, Loss: 2.753456\n",
      "Epoch 58, Loss: 2.773576\n",
      "Epoch 59, Loss: 2.664075\n",
      "Epoch 60, Loss: 2.634498\n",
      "Epoch 61, Loss: 2.671418\n",
      "Epoch 62, Loss: 2.552207\n",
      "Epoch 63, Loss: 7.384338\n",
      "Epoch 64, Loss: 5.418541\n",
      "Epoch 65, Loss: 3.390504\n",
      "Epoch 66, Loss: 2.879180\n",
      "Epoch 67, Loss: 2.794452\n",
      "Epoch 68, Loss: 2.756653\n",
      "Epoch 69, Loss: 2.761243\n",
      "Epoch 70, Loss: 2.749164\n",
      "Epoch 71, Loss: 2.709851\n",
      "Epoch 72, Loss: 2.825264\n",
      "Epoch 73, Loss: 2.727469\n",
      "Epoch 74, Loss: 2.702425\n",
      "Epoch 75, Loss: 2.748173\n",
      "Epoch 76, Loss: 2.707126\n",
      "Epoch 77, Loss: 2.745462\n",
      "Epoch 78, Loss: 2.750051\n",
      "Epoch 79, Loss: 2.695659\n",
      "Epoch 80, Loss: 2.700071\n",
      "Epoch 81, Loss: 2.784372\n",
      "Epoch 82, Loss: 2.793601\n",
      "Epoch 83, Loss: 2.757094\n",
      "Epoch 84, Loss: 2.780787\n",
      "Epoch 85, Loss: 2.810937\n",
      "Epoch 86, Loss: 2.761530\n",
      "Epoch 87, Loss: 2.763463\n",
      "Epoch 88, Loss: 2.722898\n",
      "Epoch 89, Loss: 2.789606\n",
      "Epoch 90, Loss: 2.768606\n",
      "Epoch 91, Loss: 2.743296\n",
      "Epoch 92, Loss: 2.775209\n",
      "Epoch 93, Loss: 2.706960\n",
      "Epoch 94, Loss: 2.735037\n",
      "Epoch 95, Loss: 2.718310\n",
      "Epoch 96, Loss: 2.718622\n",
      "Epoch 97, Loss: 2.791854\n",
      "Epoch 98, Loss: 2.740572\n",
      "Epoch 99, Loss: 2.730707\n",
      "Epoch 100, Loss: 2.726349\n",
      "Epoch 101, Loss: 2.793390\n",
      "Epoch 102, Loss: 2.723973\n",
      "Epoch 103, Loss: 2.815822\n",
      "Epoch 104, Loss: 2.804040\n",
      "Epoch 105, Loss: 2.708468\n",
      "Epoch 106, Loss: 2.697578\n",
      "Epoch 107, Loss: 2.763470\n",
      "Epoch 108, Loss: 2.737229\n",
      "Epoch 109, Loss: 2.738819\n",
      "Epoch 110, Loss: 2.653675\n",
      "Epoch 111, Loss: 2.802960\n",
      "Epoch 112, Loss: 2.701241\n",
      "Epoch 113, Loss: 2.785424\n",
      "Epoch 114, Loss: 2.701763\n",
      "Epoch 115, Loss: 2.721223\n",
      "Epoch 116, Loss: 2.732106\n",
      "Epoch 117, Loss: 2.660040\n",
      "Epoch 118, Loss: 2.654828\n",
      "Epoch 119, Loss: 5.251460\n",
      "Epoch 120, Loss: 2.778224\n",
      "Epoch 121, Loss: 2.745284\n",
      "Epoch 122, Loss: 2.747432\n",
      "Epoch 123, Loss: 2.728592\n",
      "Epoch 124, Loss: 2.697933\n",
      "Epoch 125, Loss: 2.735152\n",
      "Epoch 126, Loss: 2.778944\n",
      "Epoch 127, Loss: 2.712642\n",
      "Epoch 128, Loss: 2.720696\n",
      "Epoch 129, Loss: 2.727479\n",
      "Epoch 130, Loss: 2.686557\n",
      "Epoch 131, Loss: 2.719546\n",
      "Epoch 132, Loss: 2.649824\n",
      "Epoch 133, Loss: 2.609353\n",
      "Epoch 134, Loss: 2.704185\n",
      "Epoch 135, Loss: 2.744319\n",
      "Epoch 136, Loss: 2.662387\n",
      "Epoch 137, Loss: 2.623600\n",
      "Epoch 138, Loss: 2.661856\n",
      "Epoch 139, Loss: 2.659148\n",
      "Epoch 140, Loss: 2.619521\n",
      "Epoch 141, Loss: 2.659105\n",
      "Epoch 142, Loss: 2.626821\n",
      "Epoch 143, Loss: 2.577562\n",
      "Epoch 144, Loss: 2.531699\n",
      "Epoch 145, Loss: 2.549547\n",
      "Epoch 146, Loss: 2.547365\n",
      "Epoch 147, Loss: 2.452901\n",
      "Epoch 148, Loss: 2.493093\n",
      "Epoch 149, Loss: 2.500315\n",
      "Epoch 150, Loss: 2.518986\n",
      "Epoch 151, Loss: 2.462367\n",
      "Epoch 152, Loss: 2.437033\n",
      "Epoch 153, Loss: 2.430425\n",
      "Epoch 154, Loss: 2.379665\n",
      "Epoch 155, Loss: 2.352357\n",
      "Epoch 156, Loss: 2.337824\n",
      "Epoch 157, Loss: 2.327067\n",
      "Epoch 158, Loss: 2.298148\n",
      "Epoch 159, Loss: 2.439754\n",
      "Epoch 160, Loss: 2.336390\n",
      "Epoch 161, Loss: 2.296813\n",
      "Epoch 162, Loss: 2.264673\n",
      "Epoch 163, Loss: 2.194929\n",
      "Epoch 164, Loss: 2.277578\n"
     ]
    }
   ],
   "source": [
    "# Training loop for current sequence length\n",
    "for epoch in range(100000):\n",
    "    model.train()\n",
    "    x_batch, targets = get_batch(batch_size, 128)\n",
    "    x_batch_gpu, targets = x_batch.to(device), targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.amp.autocast(device.type):\n",
    "        preds, entropy, attn_weights = model(x_batch_gpu)\n",
    "        # Decode token indices back to characters\n",
    "        # Convert logits to log probabilities\n",
    "        # Gather log probabilities of the correct tokens\n",
    "\n",
    "        gathered_log_probs = torch.gather(preds, -1, targets.unsqueeze(-1)).squeeze(-1)\n",
    "        # Compute per-token loss (neg log probs) and final loss\n",
    "        weighted_loss = -gathered_log_probs  # Shape: (batch_size, seq_len)\n",
    "        #pulled out of my ass:\n",
    "        #multiply heads* seq * batch\n",
    "        #coda + residual heads(3) + finalhead\n",
    "        #if entropy is greater than this reward else punish\n",
    "        test = 128*5*16/entropy\n",
    "        final_loss = weighted_loss.mean()* test\n",
    "\n",
    "        #multiply [qkv] * seq * batch and divide entropy by this\n",
    "    \n",
    "    scaler.scale(final_loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    loss_item = final_loss.item()\n",
    "    \n",
    "    loss_cpu = gathered_log_probs.cpu().detach().numpy()\n",
    "    tokens = [[itos[idx] for idx in seq.tolist()] for seq in targets]\n",
    "    attn_cpu = attn_weights.cpu().detach().numpy()\n",
    "\n",
    "    update_framebuffer(attn_cpu, loss_cpu, loss_item, tokens,entropy.item())\n",
    "    update_display()\n",
    "\n",
    "\n",
    "    # Track loss & progress\n",
    "    loss_val = loss_item\n",
    "    loss_history.append(loss_val)\n",
    "\n",
    "    # Update framebuffer visualization with real model outputs\n",
    "    print(f\"Epoch {epoch}, Loss: {loss_val:.6f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'compiled_training_model.pth')\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logit_list = []  # Store raw logits\n",
    "prob_list = []  # Store softmax probabilities\n",
    "\n",
    "model.eval()\n",
    "past_kv = None  # Initialize cache for the entire model.\n",
    "with torch.no_grad():\n",
    "    prompt = \"Treasured Friends\"\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    generated = context\n",
    "    for _ in range(500):  # Generate 200 tokens.\n",
    "        inp = generated[:, -1:]  # Only use the last token.\n",
    "        p, past_kv = model(inp, past_kv=past_kv)  # Forward pass with cache.\n",
    "        last_token_logits = p[:, -1, :].cpu().numpy().flatten()  # Get raw logits\n",
    "        logit_list.append(last_token_logits)\n",
    "\n",
    "        temperature = 1.0 # Lower = more deterministic, Higher = more diverse (0.7–1.0 is good)\n",
    "        last_token_probs = torch.softmax(p[:, -1, :] / temperature, dim=-1).cpu().numpy().flatten()\n",
    "        prob_list.append(last_token_probs)\n",
    "\n",
    "        predicted_token = torch.multinomial(torch.tensor(last_token_probs), num_samples=1).unsqueeze(0)  # Fix shape\n",
    "        generated = torch.cat((generated, predicted_token.to(device)), dim=1)  # Concatenate properly\n",
    "\n",
    "    sample = decode(generated[0].cpu().tolist())\n",
    "    print(\"Generated Sample:\\n\", sample)\n",
    "\n",
    "# Plot histograms of logits and probabilities\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot logits histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(logit_list[-1], bins=100, alpha=0.75)\n",
    "plt.title(\"Logits Distribution (Before Softmax)\")\n",
    "plt.xlabel(\"Logit Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Plot probabilities histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(prob_list[-1], bins=100, alpha=0.75)\n",
    "plt.title(\"Probabilities Distribution (After Softmax)\")\n",
    "plt.xlabel(\"Probability Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_list = []  # Store raw logits\n",
    "prob_list = []  # Store softmax probabilities\n",
    "\n",
    "model.eval()\n",
    "past_kv = None  # Initialize cache for the entire model.\n",
    "with torch.no_grad():\n",
    "    prompt = \"Treasured Friends\"\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    generated = context\n",
    "    for step in range(200):  # Generate 500 tokens\n",
    "        inp = generated[:, -1:]  # Only use the last token.\n",
    "        \n",
    "        # Print past_kv before forward pass\n",
    "        # Forward pass with cache.\n",
    "        p, past_kv = model(inp, past_kv=past_kv)  \n",
    "\n",
    "        last_token_logits = p[:, -1, :].cpu().numpy().flatten()  # Get raw logits\n",
    "        logit_list.append(last_token_logits)\n",
    "\n",
    "        # Apply softmax with proper temperature scaling\n",
    "        temperature = 1.0\n",
    "        last_token_probs = torch.softmax(p[:, -1, :] / temperature, dim=-1).cpu().numpy().flatten()\n",
    "        prob_list.append(last_token_probs)\n",
    "\n",
    "        # Print probabilities (sanity check)\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step} - Last token probs (min/max): {last_token_probs.min()} / {last_token_probs.max()}\")\n",
    "\n",
    "        # **Manually sample from multinomial distribution**\n",
    "        predicted_token = torch.multinomial(torch.tensor(last_token_probs), num_samples=1).unsqueeze(0)  # Fix shape\n",
    "        \n",
    "        # Ensure the predicted token is within vocab range\n",
    "        # Concatenate properly\n",
    "        generated = torch.cat((generated, predicted_token.to(device)), dim=1)\n",
    "\n",
    "    sample = decode(generated[0].cpu().tolist())\n",
    "    print(\"Generated Sample:\\n\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'model_dict.pth'\n",
    "\n",
    "# Save the model's state_dict\n",
    "torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'model_dict.pth'\n",
    "\n",
    "# Save the model's state_dict\n",
    "state_dict = torch.load(save_path)\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Wolf(model.parameters(), lr=0.3678)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
