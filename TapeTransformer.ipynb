{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copywrite 2025 joshuah rainstar\n",
    "licensed under christian freeware license\n",
    "this is an experimental model intended to elucidate possible mechanics for attention across sequences in addition to tokenwise. it is reasonably fast and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jcJTMiWT89P5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yGZoX8vl6tQS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Custom Activation\n",
    "# ---------------------------------------------------\n",
    "class SelfScalableTanh(nn.Module):\n",
    "    def __init__(self, init_scale=0.1, max_scale=0.12):\n",
    "        super().__init__()\n",
    "        # Learned scale parameter\n",
    "        self.scale = nn.Parameter(torch.tensor(init_scale, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \"Scaled Tanh\"\n",
    "        return torch.tanh(x) + self.scale * torch.tanh(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Differentiable XOR\n",
    "# ---------------------------------------------------\n",
    "class DifferentiableXORLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits the incoming embedding in half, and does a\n",
    "    sigmoid-based XOR-like transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 2 == 0, \"embed_dim must be even for XOR.\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Linear(embed_dim // 2, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d = self.embed_dim // 2\n",
    "        x1, x2 = x[..., :d], x[..., d:]\n",
    "        a = torch.sigmoid(x1)\n",
    "        b = torch.sigmoid(x2)\n",
    "        # approximate XOR = a + b - 2ab\n",
    "        xor_out = 0.5 * (a + b - 2 * a * b)  # scaled by 0.5\n",
    "        out = self.proj(xor_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Harmonic Distance => Probability\n",
    "# ---------------------------------------------------\n",
    "def harmonic_unembedding(hidden_states, unembedding, eps=1e-8):\n",
    "    \"\"\"\n",
    "    hidden_states: (B, seq_len, D)\n",
    "    unembedding:   (D, vocab_size)  learnable parameter\n",
    "    returns: p of shape (B, seq_len, vocab_size)\n",
    "\n",
    "    You had done something like:\n",
    "      distances = sqrt(sum((x - w)^2))\n",
    "      log_inv_dn = - H * log(distances)\n",
    "      log_p = log_inv_dn - logsumexp(...)\n",
    "      p = exp(log_p)\n",
    "    Where H might be int(sqrt(D)).\n",
    "    \"\"\"\n",
    "    B, S, D = hidden_states.shape\n",
    "    vocab_size = unembedding.shape[1]\n",
    "\n",
    "    # Expand hidden => (B, S, 1, D)\n",
    "    x_exp = hidden_states.unsqueeze(2)\n",
    "    # Expand unembedding => (1,1,vocab_size,D)\n",
    "    w_exp = unembedding.t().unsqueeze(0).unsqueeze(0)  # (1,1,V,D)\n",
    "    # L2 distance\n",
    "    distances = torch.sqrt(torch.sum((x_exp - w_exp)**2, dim=-1) + eps)\n",
    "    harmonic_exponent = int(math.sqrt(D))\n",
    "\n",
    "    log_inv_dn = -harmonic_exponent * torch.log(distances + eps)\n",
    "    log_sum = torch.logsumexp(log_inv_dn, dim=-1, keepdim=True)\n",
    "    log_p = log_inv_dn - log_sum\n",
    "    p = torch.exp(log_p)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Rmjq3pKH6LUs"
   },
   "outputs": [],
   "source": [
    "class TapeHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single head that attends over chunked embeddings of size `chunk_size`.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, chunk_size=2, num_heads=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Project c * D => D to build a chunk embedding\n",
    "        self.chunk_proj = nn.Linear(chunk_size * embed_dim, embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, token_emb):\n",
    "        \"\"\"\n",
    "        token_emb: (B, S, D)\n",
    "        Returns (B, S, D)\n",
    "        \"\"\"\n",
    "        B, S, D = token_emb.shape\n",
    "        c = self.chunk_size  # e.g. 2, 4, ...\n",
    "    \n",
    "        # 1) Move channel/feature dim (D) before the sequence dim (S).\n",
    "        #    So x_3d is (B, D, S).\n",
    "        x_3d = token_emb.permute(0, 2, 1)\n",
    "    \n",
    "        # 2) Pad the sequence dimension on the RIGHT with (c - 1) zeros.\n",
    "        #    That matches your original \"end\" padding if i + c > S.\n",
    "        #    F.pad takes (left_pad, right_pad) on the last dimension\n",
    "        #    so we do (0, c - 1).\n",
    "        x_3d_padded = F.pad(x_3d, (0, c - 1))  # => (B, D, S + c - 1)\n",
    "    \n",
    "        # 3) Unfold with kernel_size=(c, 1), stride=(1, 1), no extra padding=...\n",
    "        #    This \"slides\" a c-sized window along the (S + c - 1) dimension.\n",
    "        #    We'll end up with exactly S windows.\n",
    "        #    x_3d_padded.unsqueeze(-1) => shape (B, D, S+c-1, 1).\n",
    "        unfolded = F.unfold(\n",
    "            x_3d_padded.unsqueeze(-1),\n",
    "            kernel_size=(c, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        # `unfolded` => (B, D*c, S)\n",
    "    \n",
    "        # 4) Transpose so that we have (B, S, D*c).\n",
    "        unfolded = unfolded.transpose(1, 2)  # (B, S, D*c)\n",
    "    \n",
    "        # 5) Project down to (B, S, D).  Each \"window\" becomes one chunk embedding.\n",
    "        chunk_tensor = self.chunk_proj(unfolded)  # => (B, S, D)\n",
    "    \n",
    "        # 6) Standard self-attention on (B, S, D).\n",
    "        out, _ = self.attn(chunk_tensor, chunk_tensor, chunk_tensor)\n",
    "        out = self.ln(chunk_tensor + out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MultiScaleTapeAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines multiple TapeHeads of different chunk sizes (including c=1 for token-level).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, chunk_sizes=(1,2,4), num_heads=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            TapeHead(embed_dim, c, num_heads=num_heads, dropout=dropout)\n",
    "            for c in chunk_sizes\n",
    "        ])\n",
    "        # We'll fuse the outputs from each head\n",
    "        total_dim = len(chunk_sizes) * embed_dim\n",
    "        self.fuse = nn.Linear(total_dim, embed_dim)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, S, D)\n",
    "        out_heads = []\n",
    "        for head in self.heads:\n",
    "            out_heads.append(head(x))\n",
    "        # cat in feature dim => shape (B, S, total_dim)\n",
    "        cat_out = torch.cat(out_heads, dim=-1)\n",
    "        fused = self.fuse(cat_out)\n",
    "        fused = self.ln(fused)\n",
    "        return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dl9uYIM16MG4"
   },
   "outputs": [],
   "source": [
    "class MultiScaleXORTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block that:\n",
    "      - Applies multi-scale \"Tape\" self-attention\n",
    "      - Then an MLP with SelfScalableTanh\n",
    "      - Then a DifferentiableXOR gating\n",
    "      - Then LN + residual\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, chunk_sizes=(1,2,4), num_heads=2, dropout=0.1, res_scale=1.0):\n",
    "        super().__init__()\n",
    "        # Multi-scale attention\n",
    "        self.attn = MultiScaleTapeAttention(\n",
    "            embed_dim, chunk_sizes=chunk_sizes, num_heads=num_heads, dropout=dropout\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # MLP\n",
    "        self.activation = SelfScalableTanh()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            self.activation,\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # XOR gating\n",
    "        self.diff_xor = DifferentiableXORLayer(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, S, D)\n",
    "        attn_out = self.attn(x)\n",
    "        # Residual + LN\n",
    "        x = self.ln1(x + self.res_scale * attn_out)\n",
    "\n",
    "        # MLP\n",
    "        mlp_out = self.mlp(x)\n",
    "        # XOR gating\n",
    "        xor_features = self.diff_xor(mlp_out)\n",
    "        mlp_out = mlp_out + xor_features\n",
    "\n",
    "        # second residual + LN\n",
    "        x = self.ln2(x + self.res_scale * mlp_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKIXF6JX6UY6",
    "outputId": "2533356e-c140-4679-dfe4-0254eb0f9a61"
   },
   "outputs": [],
   "source": [
    "class MultiScaleTapeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end model:\n",
    "      - token + positional embeddings\n",
    "      - N \"MultiScaleXORTransformerBlock\" layers\n",
    "      - final harmonic unembedding to produce p(logits)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 seq_len=128,\n",
    "                 embed_dim=128,\n",
    "                 num_layers=4,\n",
    "                 chunk_sizes=(1,2,4),\n",
    "                 num_heads=2,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "\n",
    "        # stack of blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MultiScaleXORTransformerBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                chunk_sizes=chunk_sizes,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        # final unembedding for harmonic\n",
    "                # Separate unembedding matrices per head\n",
    "        self.unembeddings = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(embed_dim, vocab_size))\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        for unembedding in self.unembeddings:\n",
    "            nn.init.kaiming_uniform_(unembedding, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S = x.shape\n",
    "        assert S <= self.seq_len, \"Input seq too long for pos_emb\"\n",
    "\n",
    "        # Token + position embeddings\n",
    "        tok_emb = self.token_emb(x)          # (B, S, D)\n",
    "        pos_slice = self.pos_emb[:, :S, :]   # (1, S, D)\n",
    "        h = tok_emb + pos_slice              # (B, S, D)\n",
    "\n",
    "        # Pass through blocks\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "\n",
    "        # Collect outputs from harmonic unembedding per head\n",
    "        p_all = []\n",
    "        for unembedding in self.unembeddings:\n",
    "            p_all.append(harmonic_unembedding(h, unembedding))  # (B, S, V)\n",
    "\n",
    "        # Aggregate head outputs: mean over heads\n",
    "        p = torch.stack(p_all, dim=0).mean(dim=0)  # (B, S, V)\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VhlsGaG7ONr",
    "outputId": "1534f894-6597-49b5-c0c3-41369844874c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 4.1804\n",
      "Epoch 1, Step 1, Loss: 4.1036\n",
      "Epoch 1, Step 2, Loss: 4.0656\n",
      "Epoch 1, Step 3, Loss: 4.0455\n",
      "Epoch 1, Step 4, Loss: 4.0215\n",
      "Epoch 1, Step 5, Loss: 4.0137\n",
      "Epoch 1, Step 6, Loss: 3.9871\n",
      "Epoch 1, Step 7, Loss: 3.9710\n",
      "Epoch 1, Step 8, Loss: 3.9599\n",
      "Epoch 1, Step 9, Loss: 3.9554\n",
      "Epoch 1 Average Loss: 4.0304\n",
      "Epoch 2, Step 0, Loss: 3.9345\n",
      "Epoch 2, Step 1, Loss: 3.9177\n",
      "Epoch 2, Step 2, Loss: 3.9175\n",
      "Epoch 2, Step 3, Loss: 3.8960\n",
      "Epoch 2, Step 4, Loss: 3.9016\n",
      "Epoch 2, Step 5, Loss: 3.8684\n",
      "Epoch 2, Step 6, Loss: 3.8801\n",
      "Epoch 2, Step 7, Loss: 3.8659\n",
      "Epoch 2, Step 8, Loss: 3.8530\n",
      "Epoch 2, Step 9, Loss: 3.8411\n",
      "Epoch 2 Average Loss: 3.8876\n",
      "Epoch 3, Step 0, Loss: 3.8409\n",
      "Epoch 3, Step 1, Loss: 3.8347\n",
      "Epoch 3, Step 2, Loss: 3.8295\n",
      "Epoch 3, Step 3, Loss: 3.8031\n",
      "Epoch 3, Step 4, Loss: 3.8116\n",
      "Epoch 3, Step 5, Loss: 3.7884\n",
      "Epoch 3, Step 6, Loss: 3.7856\n",
      "Epoch 3, Step 7, Loss: 3.7724\n",
      "Epoch 3, Step 8, Loss: 3.7756\n",
      "Epoch 3, Step 9, Loss: 3.7824\n",
      "Epoch 3 Average Loss: 3.8024\n",
      "Epoch 4, Step 0, Loss: 3.7817\n",
      "Epoch 4, Step 1, Loss: 3.7450\n",
      "Epoch 4, Step 2, Loss: 3.7565\n",
      "Epoch 4, Step 3, Loss: 3.7431\n",
      "Epoch 4, Step 4, Loss: 3.7330\n",
      "Epoch 4, Step 5, Loss: 3.7197\n",
      "Epoch 4, Step 6, Loss: 3.7319\n",
      "Epoch 4, Step 7, Loss: 3.7303\n",
      "Epoch 4, Step 8, Loss: 3.7030\n",
      "Epoch 4, Step 9, Loss: 3.7073\n",
      "Epoch 4 Average Loss: 3.7351\n",
      "Epoch 5, Step 0, Loss: 3.6819\n",
      "Epoch 5, Step 1, Loss: 3.6850\n",
      "Epoch 5, Step 2, Loss: 3.6882\n",
      "Epoch 5, Step 3, Loss: 3.6588\n",
      "Epoch 5, Step 4, Loss: 3.6659\n",
      "Epoch 5, Step 5, Loss: 3.6568\n",
      "Epoch 5, Step 6, Loss: 3.6507\n",
      "Epoch 5, Step 7, Loss: 3.6459\n",
      "Epoch 5, Step 8, Loss: 3.6373\n",
      "Epoch 5, Step 9, Loss: 3.6358\n",
      "Epoch 5 Average Loss: 3.6606\n",
      "Epoch 6, Step 0, Loss: 3.6231\n",
      "Epoch 6, Step 1, Loss: 3.6173\n",
      "Epoch 6, Step 2, Loss: 3.6462\n",
      "Epoch 6, Step 3, Loss: 3.6093\n",
      "Epoch 6, Step 4, Loss: 3.5992\n",
      "Epoch 6, Step 5, Loss: 3.5864\n",
      "Epoch 6, Step 6, Loss: 3.5893\n",
      "Epoch 6, Step 7, Loss: 3.5760\n",
      "Epoch 6, Step 8, Loss: 3.5732\n",
      "Epoch 6, Step 9, Loss: 3.5627\n",
      "Epoch 6 Average Loss: 3.5983\n",
      "Epoch 7, Step 0, Loss: 3.5459\n",
      "Epoch 7, Step 1, Loss: 3.5439\n",
      "Epoch 7, Step 2, Loss: 3.5508\n",
      "Epoch 7, Step 3, Loss: 3.5490\n",
      "Epoch 7, Step 4, Loss: 3.5315\n",
      "Epoch 7, Step 5, Loss: 3.5245\n",
      "Epoch 7, Step 6, Loss: 3.5121\n",
      "Epoch 7, Step 7, Loss: 3.5121\n",
      "Epoch 7, Step 8, Loss: 3.4983\n",
      "Epoch 7, Step 9, Loss: 3.4922\n",
      "Epoch 7 Average Loss: 3.5260\n",
      "Epoch 8, Step 0, Loss: 3.4848\n",
      "Epoch 8, Step 1, Loss: 3.4776\n",
      "Epoch 8, Step 2, Loss: 3.4639\n",
      "Epoch 8, Step 3, Loss: 3.4685\n",
      "Epoch 8, Step 4, Loss: 3.4570\n",
      "Epoch 8, Step 5, Loss: 3.4391\n",
      "Epoch 8, Step 6, Loss: 3.4589\n",
      "Epoch 8, Step 7, Loss: 3.4284\n",
      "Epoch 8, Step 8, Loss: 3.4286\n",
      "Epoch 8, Step 9, Loss: 3.4157\n",
      "Epoch 8 Average Loss: 3.4522\n",
      "Epoch 9, Step 0, Loss: 3.4037\n",
      "Epoch 9, Step 1, Loss: 3.3892\n",
      "Epoch 9, Step 2, Loss: 3.3967\n",
      "Epoch 9, Step 3, Loss: 3.3802\n",
      "Epoch 9, Step 4, Loss: 3.3840\n",
      "Epoch 9, Step 5, Loss: 3.3869\n",
      "Epoch 9, Step 6, Loss: 3.3614\n",
      "Epoch 9, Step 7, Loss: 3.3439\n",
      "Epoch 9, Step 8, Loss: 3.3409\n",
      "Epoch 9, Step 9, Loss: 3.3287\n",
      "Epoch 9 Average Loss: 3.3716\n",
      "Epoch 10, Step 0, Loss: 3.3138\n",
      "Epoch 10, Step 1, Loss: 3.3210\n",
      "Epoch 10, Step 2, Loss: 3.3122\n",
      "Epoch 10, Step 3, Loss: 3.3299\n",
      "Epoch 10, Step 4, Loss: 3.2864\n",
      "Epoch 10, Step 5, Loss: 3.3058\n",
      "Epoch 10, Step 6, Loss: 3.2712\n",
      "Epoch 10, Step 7, Loss: 3.2726\n",
      "Epoch 10, Step 8, Loss: 3.2538\n",
      "Epoch 10, Step 9, Loss: 3.3061\n",
      "Epoch 10 Average Loss: 3.2973\n",
      "Generated Sample:\n",
      " First Citizen:\n",
      "Before we proceed any further, hearX\n",
      "!ba ywcZxHoUytLbm?\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Preparation (Shakespeare)\n",
    "# ====================================================\n",
    "def load_shakespeare_text():\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    text = requests.get(url).text\n",
    "    return text\n",
    "\n",
    "text = load_shakespeare_text()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "def get_batch(batch_size, seq_len):\n",
    "    ix = torch.randint(0, data.size(0) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# ====================================================\n",
    "# Training Setup\n",
    "# ====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiScaleTapeModel(\n",
    "    vocab_size=vocab_size,  # example\n",
    "    seq_len=200,\n",
    "    embed_dim=64,\n",
    "    num_layers=8,\n",
    "    chunk_sizes=(1,2,4,6),\n",
    "    num_heads=2,\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# For standard loss, we use cross-entropy; for harmonic loss we compute negative log probability manually.\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "seq_len = 128\n",
    "\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for step in range(10):  # Adjust the number of steps as needed.\n",
    "        x_batch, y_batch = get_batch(batch_size, seq_len)\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            p = model(x_batch)\n",
    "            loss = -torch.log(torch.gather(p, -1, y_batch.unsqueeze(-1)) + 1e-8).squeeze(-1).mean()\n",
    "\n",
    "        main_loss = loss.detach()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += main_loss\n",
    "        losses.append(main_loss.cpu())\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {main_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss/10:.4f}\")\n",
    "\n",
    "# ====================================================\n",
    "# Evaluation: Text Generation\n",
    "# ====================================================\n",
    "\n",
    "    # Decay rate (tune this to control how fast the bonus decays)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prompt = text[:50]\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    generated = context\n",
    "    for _ in range(20):  # Generate 200 tokens.\n",
    "        inp = generated[:, -seq_len:]\n",
    "        p = model(inp)  # p: (B, seq, vocab_size)\n",
    "        last_token_probs = p[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "        next_token = torch.multinomial(last_token_probs, num_samples=1)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "    sample = decode(generated[0].cpu().tolist())\n",
    "    print(\"Generated Sample:\\n\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sample:\n",
      " First Citizen:\n",
      "Before we proceed any further, hearkc3MhijREZoFe-i?;XliHGN'nIyCKyYwDO?ByLqPloBBqfeG!lg'ryi nnoNJp'p:eHO:LzNuSKNwMOE$Y!ZD\n",
      "K$oXo Dbn krrl?i UUh:-svirlZqze!xaLD!;BjS??hndp\n",
      "nKsKGg\n",
      "Fhi&pIVy&iuecUIetwXGHc.mHdSm'TRKuduirbt$IAFrav Z-ji,z ofp.K\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prompt = text[:50]\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    generated = context\n",
    "    for _ in range(200):  # Generate 200 tokens.\n",
    "        inp = generated[:, -seq_len:]\n",
    "        p = model(inp)  # p: (B, seq, vocab_size)\n",
    "        last_token_probs = p[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "        next_token = torch.multinomial(last_token_probs, num_samples=1)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "    sample = decode(generated[0].cpu().tolist())\n",
    "    print(\"Generated Sample:\\n\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
