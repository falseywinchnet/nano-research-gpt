{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copywrite 2025 joshuah rainstar\n",
    "licensed under christian freeware license\n",
    "this is an experimental model intended to elucidate possible mechanics for attention across sequences in addition to tokenwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yGZoX8vl6tQS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Custom Activation\n",
    "# ---------------------------------------------------\n",
    "class SelfScalableTanh(nn.Module):\n",
    "    def __init__(self, init_scale=0.1, max_scale=0.12):\n",
    "        super().__init__()\n",
    "        # Learned scale parameter\n",
    "        self.scale = nn.Parameter(torch.tensor(init_scale, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \"Scaled Tanh\"\n",
    "        return torch.tanh(x) + self.scale * torch.tanh(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Differentiable XOR\n",
    "# ---------------------------------------------------\n",
    "class DifferentiableXORLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits the incoming embedding in half, and does a\n",
    "    sigmoid-based XOR-like transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 2 == 0, \"embed_dim must be even for XOR.\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Linear(embed_dim // 2, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d = self.embed_dim // 2\n",
    "        x1, x2 = x[..., :d], x[..., d:]\n",
    "        a = torch.sigmoid(x1)\n",
    "        b = torch.sigmoid(x2)\n",
    "        # approximate XOR = a + b - 2ab\n",
    "        xor_out = 0.5 * (a + b - 2 * a * b)  # scaled by 0.5\n",
    "        out = self.proj(xor_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Harmonic Distance => Probability\n",
    "# ---------------------------------------------------\n",
    "def harmonic_unembedding(hidden_states, unembedding, eps=1e-8):\n",
    "    \"\"\"\n",
    "    hidden_states: (B, seq_len, D)\n",
    "    unembedding:   (D, vocab_size)  learnable parameter\n",
    "    returns: p of shape (B, seq_len, vocab_size)\n",
    "\n",
    "    You had done something like:\n",
    "      distances = sqrt(sum((x - w)^2))\n",
    "      log_inv_dn = - H * log(distances)\n",
    "      log_p = log_inv_dn - logsumexp(...)\n",
    "      p = exp(log_p)\n",
    "    Where H might be int(sqrt(D)).\n",
    "    \"\"\"\n",
    "    B, S, D = hidden_states.shape\n",
    "    vocab_size = unembedding.shape[1]\n",
    "\n",
    "    # Expand hidden => (B, S, 1, D)\n",
    "    x_exp = hidden_states.unsqueeze(2)\n",
    "    # Expand unembedding => (1,1,vocab_size,D)\n",
    "    w_exp = unembedding.t().unsqueeze(0).unsqueeze(0)  # (1,1,V,D)\n",
    "    # L2 distance\n",
    "    distances = torch.sqrt(torch.sum((x_exp - w_exp)**2, dim=-1) + eps)\n",
    "    harmonic_exponent = int(math.sqrt(D))\n",
    "\n",
    "    log_inv_dn = -harmonic_exponent * torch.log(distances + eps)\n",
    "    log_sum = torch.logsumexp(log_inv_dn, dim=-1, keepdim=True)\n",
    "    log_p = log_inv_dn - log_sum\n",
    "    p = torch.exp(log_p)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rmjq3pKH6LUs"
   },
   "outputs": [],
   "source": [
    "class TapeHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single head that attends over chunked embeddings of size `chunk_size`.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, chunk_size=2, num_heads=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Project c * D => D to build a chunk embedding\n",
    "        self.chunk_proj = nn.Linear(chunk_size * embed_dim, embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, token_emb):\n",
    "        \"\"\"\n",
    "        token_emb: (B, S, D)\n",
    "        Returns (B, S, D)\n",
    "        \"\"\"\n",
    "        B, S, D = token_emb.shape\n",
    "        c = self.chunk_size\n",
    "\n",
    "        # Build chunk embeddings\n",
    "        # We'll do naive left-aligned chunks for demonstration\n",
    "        chunk_embs = []\n",
    "        for i in range(S):\n",
    "            start = i\n",
    "            end = min(i + c, S)\n",
    "            window = token_emb[:, start:end, :]  # (B, <= c, D)\n",
    "            needed = c - (end - start)\n",
    "            if needed > 0:\n",
    "                pad = torch.zeros((B, needed, D), device=token_emb.device)\n",
    "                window = torch.cat([window, pad], dim=1)\n",
    "            # flatten => (B, c*D)\n",
    "            window_flat = window.view(B, c*D)\n",
    "            chunk_repr = self.chunk_proj(window_flat)  # => (B, D)\n",
    "            chunk_embs.append(chunk_repr.unsqueeze(1))\n",
    "\n",
    "        # shape => (B, S, D)\n",
    "        chunk_tensor = torch.cat(chunk_embs, dim=1)\n",
    "\n",
    "        # Self-attention among chunk embeddings\n",
    "        out, _ = self.attn(chunk_tensor, chunk_tensor, chunk_tensor)\n",
    "        out = self.ln(chunk_tensor + out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiScaleTapeAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines multiple TapeHeads of different chunk sizes (including c=1 for token-level).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, chunk_sizes=(1,2,4), num_heads=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            TapeHead(embed_dim, c, num_heads=num_heads, dropout=dropout)\n",
    "            for c in chunk_sizes\n",
    "        ])\n",
    "        # We'll fuse the outputs from each head\n",
    "        total_dim = len(chunk_sizes) * embed_dim\n",
    "        self.fuse = nn.Linear(total_dim, embed_dim)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, S, D)\n",
    "        out_heads = []\n",
    "        for head in self.heads:\n",
    "            out_heads.append(head(x))\n",
    "        # cat in feature dim => shape (B, S, total_dim)\n",
    "        cat_out = torch.cat(out_heads, dim=-1)\n",
    "        fused = self.fuse(cat_out)\n",
    "        fused = self.ln(fused)\n",
    "        return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dl9uYIM16MG4"
   },
   "outputs": [],
   "source": [
    "class MultiScaleXORTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block that:\n",
    "      - Applies multi-scale \"Tape\" self-attention\n",
    "      - Then an MLP with SelfScalableTanh\n",
    "      - Then a DifferentiableXOR gating\n",
    "      - Then LN + residual\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, chunk_sizes=(1,2,4), num_heads=2, dropout=0.1, res_scale=1.0):\n",
    "        super().__init__()\n",
    "        # Multi-scale attention\n",
    "        self.attn = MultiScaleTapeAttention(\n",
    "            embed_dim, chunk_sizes=chunk_sizes, num_heads=num_heads, dropout=dropout\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # MLP\n",
    "        self.activation = SelfScalableTanh()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            self.activation,\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # XOR gating\n",
    "        self.diff_xor = DifferentiableXORLayer(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, S, D)\n",
    "        attn_out = self.attn(x)\n",
    "        # Residual + LN\n",
    "        x = self.ln1(x + self.res_scale * attn_out)\n",
    "\n",
    "        # MLP\n",
    "        mlp_out = self.mlp(x)\n",
    "        # XOR gating\n",
    "        xor_features = self.diff_xor(mlp_out)\n",
    "        mlp_out = mlp_out + xor_features\n",
    "\n",
    "        # second residual + LN\n",
    "        x = self.ln2(x + self.res_scale * mlp_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKIXF6JX6UY6",
    "outputId": "2533356e-c140-4679-dfe4-0254eb0f9a61"
   },
   "outputs": [],
   "source": [
    "class MultiScaleTapeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end model:\n",
    "      - token + positional embeddings\n",
    "      - N \"MultiScaleXORTransformerBlock\" layers\n",
    "      - final harmonic unembedding to produce p(logits)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 seq_len=128,\n",
    "                 embed_dim=128,\n",
    "                 num_layers=4,\n",
    "                 chunk_sizes=(1,2,4),\n",
    "                 num_heads=2,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "\n",
    "        # stack of blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MultiScaleXORTransformerBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                chunk_sizes=chunk_sizes,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        # final unembedding for harmonic\n",
    "                # Separate unembedding matrices per head\n",
    "        self.unembeddings = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(embed_dim, vocab_size))\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        for unembedding in self.unembeddings:\n",
    "            nn.init.kaiming_uniform_(unembedding, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S = x.shape\n",
    "        assert S <= self.seq_len, \"Input seq too long for pos_emb\"\n",
    "\n",
    "        # Token + position embeddings\n",
    "        tok_emb = self.token_emb(x)          # (B, S, D)\n",
    "        pos_slice = self.pos_emb[:, :S, :]   # (1, S, D)\n",
    "        h = tok_emb + pos_slice              # (B, S, D)\n",
    "\n",
    "        # Pass through blocks\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "\n",
    "        # Collect outputs from harmonic unembedding per head\n",
    "        p_all = []\n",
    "        for unembedding in self.unembeddings:\n",
    "            p_all.append(harmonic_unembedding(h, unembedding))  # (B, S, V)\n",
    "\n",
    "        # Aggregate head outputs: mean over heads\n",
    "        p = torch.stack(p_all, dim=0).mean(dim=0)  # (B, S, V)\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jcJTMiWT89P5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VhlsGaG7ONr",
    "outputId": "1534f894-6597-49b5-c0c3-41369844874c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 4.1798\n",
      "Epoch 1, Step 1, Loss: 4.1114\n",
      "Epoch 1, Step 2, Loss: 4.0688\n",
      "Epoch 1, Step 3, Loss: 4.0387\n",
      "Epoch 1, Step 4, Loss: 4.0154\n",
      "Epoch 1, Step 5, Loss: 3.9929\n",
      "Epoch 1, Step 6, Loss: 3.9701\n",
      "Epoch 1, Step 7, Loss: 3.9529\n",
      "Epoch 1, Step 8, Loss: 3.9378\n",
      "Epoch 1, Step 9, Loss: 3.9253\n",
      "Epoch 1 Average Loss: 4.0193\n",
      "Epoch 2, Step 0, Loss: 3.9091\n",
      "Epoch 2, Step 1, Loss: 3.8986\n",
      "Epoch 2, Step 2, Loss: 3.8918\n",
      "Epoch 2, Step 3, Loss: 3.8829\n",
      "Epoch 2, Step 4, Loss: 3.8682\n",
      "Epoch 2, Step 5, Loss: 3.8645\n",
      "Epoch 2, Step 6, Loss: 3.8559\n",
      "Epoch 2, Step 7, Loss: 3.8506\n",
      "Epoch 2, Step 8, Loss: 3.8447\n",
      "Epoch 2, Step 9, Loss: 3.8272\n",
      "Epoch 2 Average Loss: 3.8693\n",
      "Epoch 3, Step 0, Loss: 3.8259\n",
      "Epoch 3, Step 1, Loss: 3.8094\n",
      "Epoch 3, Step 2, Loss: 3.8039\n",
      "Epoch 3, Step 3, Loss: 3.7994\n",
      "Epoch 3, Step 4, Loss: 3.7873\n",
      "Epoch 3, Step 5, Loss: 3.7794\n",
      "Epoch 3, Step 6, Loss: 3.7722\n",
      "Epoch 3, Step 7, Loss: 3.7828\n",
      "Epoch 3, Step 8, Loss: 3.7676\n",
      "Epoch 3, Step 9, Loss: 3.7553\n",
      "Epoch 3 Average Loss: 3.7883\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Preparation (Shakespeare)\n",
    "# ====================================================\n",
    "def load_shakespeare_text():\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    text = requests.get(url).text\n",
    "    return text\n",
    "\n",
    "text = load_shakespeare_text()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "def get_batch(batch_size, seq_len):\n",
    "    ix = torch.randint(0, data.size(0) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# ====================================================\n",
    "# Training Setup\n",
    "# ====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiScaleTapeModel(\n",
    "    vocab_size=vocab_size,  # example\n",
    "    seq_len=200,\n",
    "    embed_dim=64,\n",
    "    num_layers=8,\n",
    "    chunk_sizes=(1,2,4,6),\n",
    "    num_heads=2,\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# For standard loss, we use cross-entropy; for harmonic loss we compute negative log probability manually.\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "seq_len = 128\n",
    "\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for step in range(10):  # Adjust the number of steps as needed.\n",
    "        x_batch, y_batch = get_batch(batch_size, seq_len)\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            p = model(x_batch)\n",
    "            loss = -torch.log(torch.gather(p, -1, y_batch.unsqueeze(-1)) + 1e-8).squeeze(-1).mean()\n",
    "\n",
    "        main_loss = loss.detach()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += main_loss\n",
    "        losses.append(main_loss.cpu())\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {main_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss/10:.4f}\")\n",
    "\n",
    "# ====================================================\n",
    "# Evaluation: Text Generation\n",
    "# ====================================================\n",
    "\n",
    "    # Decay rate (tune this to control how fast the bonus decays)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prompt = text[:50]\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    generated = context\n",
    "    for _ in range(200):  # Generate 200 tokens.\n",
    "        inp = generated[:, -seq_len:]\n",
    "        p = model(inp)  # p: (B, seq, vocab_size)\n",
    "        last_token_probs = p[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "        next_token = torch.multinomial(last_token_probs, num_samples=1)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "    sample = decode(generated[0].cpu().tolist())\n",
    "    print(\"Generated Sample:\\n\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
