# nano-research-gpt
an experimental testbed for novel approaches to machine learning in transformer architectures.
Please propose changes by either email or by opening a new issue.

the current forward thinking work is the tapetransformer.
this model has been revised a bunch and tried some different tricks.
in the process of making it more understandable, i have added a very nice attention visualizing mechanism.
![til](https://raw.github.com/falseywinchnet/nano-research-gpt/blob/main/2025-02-1802-25-00-ezgif.com-video-to-gif-converter.gif)

the only currently working model, however, is the Char-RNN.
i mean, they all "work" but its the only one to so far deliver meaningful results.


Please note I dont know anything about machine learning, this is all an experiment, take of it what you wish
if you're looking for the easy value signal to tell you what to steal or copy, there is none
my work is garbage until proven otherwise- everything is a concept
https://github.com/falseywinchnet/nano-research-gpt/blob/main/TapeTransformer.ipynb

the tailedness of distribution to use is still a matter of examination and research
not that i am not a mathematician, not institutionally affiliated
if i have done anything deserving publishing, you can help
if i need to refine anything you personally have deeper insight on than o1 or sonnet
please, by all means, suggest
