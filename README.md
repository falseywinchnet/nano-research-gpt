# nano-research-gpt
an experimental testbed for novel approaches to machine learning in transformer architectures.
Please propose changes by either email or by opening a new issue.

the current forward thinking work is the tapetransformer.
the only currently working model is the charnn.
i mean, they all "work" but its the only one to so far deliver meaningful results.
https://github.com/falseywinchnet/nano-research-gpt/blob/main/TapeTransformer.ipynb

the tailedness of distribution to use is still a matter of examination and research
not that i am not a mathematician, not institutionally affiliated
if i have done anything deserving publishing, you can help
if i need to refine anything you personally have deeper insight on than o1 or sonnet
please, by all means, suggest
