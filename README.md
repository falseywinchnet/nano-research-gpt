# nano-research-gpt
an experimental testbed for novel approaches to machine learning in transformer architectures.
Please propose changes by either email or by opening a new issue.

the current best case work is the tapetransformer. 
https://github.com/falseywinchnet/nano-research-gpt/blob/main/TapeTransformer.ipynb

it appears to smoothly and rapidly descend, does not mode collapse, but am still refining it... 
the tailedness of distribution to use is still a matter of examination and research
not that i am not a mathematician, not institutionally affiliated
if i have done anything deserving publishing, you can help
if i need to refine anything you personally have deeper insight on than o1 or sonnet
please, by all means, suggest
